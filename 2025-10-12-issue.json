[
  {
    "repo": "labring/cri-shim",
    "issues": []
  },
  {
    "repo": "labring/sealos",
    "issues": [
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 291   |\n| ✅ Successful | 192   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18201548304?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-10-03T02:42:50Z",
        "number": 6064,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6064",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nAfter running sealos gen -e defaultVIP=198.19.0.2, I manually changed networking.serviceSubnet to 198.19.0.0/16 in the Clusterfile. However, executing sealos apply -f Clusterfile still results in the error:\n\n\"Applied to cluster error: failed to init masters: generate init config error: ensure IP 198.19.0.2 is not in serviceSubnet range\".\n\n### What is the expected behavior?\n\nThe VIP has been correctly set to 198.19.0.2 without any errors.\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-30T13:06:43Z",
        "number": 6062,
        "state": "CLOSED",
        "title": "BUG: VIP Conflicts with Service Subnet in Sealos",
        "url": "https://github.com/labring/sealos/issues/6062",
        "login": "ancienter",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 291   |\n| ✅ Successful | 192   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in lifecycle/USERS.md\n\n* [TIMEOUT] <https://www.iflytek.com/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18106301832?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-30T13:02:33Z",
        "number": 6061,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6061",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.1.0-rc1\n\n### How to reproduce the bug?\n\nSEALOS_V2_CLOUD_DOMAIN=10.128.250.128.nip.io SEALOS_V2_MASTERS=\"10.128.250.128:22\" ./install-v2.sh\n\n### What is the expected behavior?\n\nInstall\n\n### What do you see instead?\n\n```sh\n2025-09-28T12:45:45 info start to install app in this cluster\n2025-09-28T12:45:45 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing PreProcess Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing pipeline MountRootfs in InstallProcessor.\n2025-09-28T12:45:46 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-09-28T12:45:46 info Executing UpgradeIfNeed Pipeline in InstallProcessor\nError from server (NotFound): configmaps \"sealos-config\" not found\nError from server (NotFound): configmaps \"sealos-config\" not found\nnamespace/sealos-system configured\nnamespace/sealos configured\nRelease \"cert-config\" does not exist. Installing it now.\nError: Unable to continue with install: Certificate \"sealos-cloud\" in namespace \"sealos-system\" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error: missing key \"meta.helm.sh/release-name\": must be set to \"cert-config\"; annotation validation error: missing key \"meta.helm.sh/release-namespace\": must be set to \"sealos-system\"\n2025-09-28T12:45:47 info using v1beta3 kubeadm config\nError: exit status 1\n```\n\n### Operating environment\n\n```markdown\n- Sealos version:5.1.0-rc1\n- Docker version:\n- Kubernetes version:\n- Operating system: Ubuntu 24\n- Runtime environment:physical machine 200G mem\n- Cluster size:1 node\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 6057,
        "state": "OPEN",
        "title": "BUG: install fail",
        "url": "https://github.com/labring/sealos/issues/6057",
        "login": "xiachenrui",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.1.0-beta4\n\n### How to reproduce the bug?\n\n==========================================================================================================\nYou are installing the open source version of Sealos Cloud. Some features are missing. \nFor full functionality, please purchase the commercial edition: https://sealos.io/contact\n==========================================================================================================\n INFO [2025-09-28 17:37:02] >> Using GitHub proxy for downloads. \n INFO [2025-09-28 17:37:02] >> [Step 1] Environment and dependency checks... \n INFO [2025-09-28 17:37:02] >> Environment and dependency checks passed. \n INFO [2025-09-28 17:37:02] >> Sealos CLI is already installed. \n INFO [2025-09-28 17:37:02] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubernetes:v1.28.15 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cilium:v1.17.1 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cert-manager:v1.14.6 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/helm:v3.16.2 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/openebs:v3.10.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/higress:v2.1.3 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubeblocks:v0.8.2 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cockroach:v2.12.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/metrics-server:v0.6.4 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/victoria-metrics-k8s-stack:v1.124.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos-cloud:latest \nError: initializing image from source docker://ghcr.nju.edu.cn/labring/sealos-cloud:latest: invalid character '<' looking for beginning of value\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-29T08:33:22Z",
        "number": 6056,
        "state": "CLOSED",
        "title": "使用官方脚本，安装失败",
        "url": "https://github.com/labring/sealos/issues/6056",
        "login": "ATCThunder",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 291   |\n| ✅ Successful | 192   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in CODE_OF_CONDUCT.md\n\n* [TIMEOUT] <https://www.contributor-covenant.org/version/2/1/code_of_conduct.html> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18045649394?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-27T07:29:36Z",
        "number": 6046,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6046",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Documentation Section\n\n安装文档\n\n### What is the current documentation state?\n\nsealos 集群信息，我在master1节点上创建的集群，如果master1 挂了，就GG。\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### Documentation Section\n\nInstallation documentation\n\n### What is the current documentation state?\n\nsealos cluster information, the cluster I created on the master1 node, if master1 is hung, it will GG.\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change benefit?\n\n_No response_\n\n### Additional information\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-24T15:06:54Z",
        "number": 6036,
        "state": "CLOSED",
        "title": "Docs: brief description of your suggestion || Docs: brief description of your suggestions",
        "url": "https://github.com/labring/sealos/issues/6036",
        "login": "menchao1992",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 301   |\n| ✅ Successful | 201   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17897139865?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-22T02:45:28Z",
        "number": 6021,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6021",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 301   |\n| ✅ Successful | 201   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17883128560?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-21T01:49:33Z",
        "number": 6018,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6018",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 301   |\n| ✅ Successful | 201   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 98    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17866203144?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-20T03:11:56Z",
        "number": 6014,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6014",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-20T08:42:53Z",
        "number": 6007,
        "state": "CLOSED",
        "title": "Feature: admission webhook release tag",
        "url": "https://github.com/labring/sealos/issues/6007",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n$ sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\n\n执行这个安装命令的时候报：\nE0917 10:50:50.919408 39371 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"sealos.hub:5000/pause:3.9\\\": failed to resolve reference \\\"sealos.hub:5000/pause:3.9\\\": failed to do request: Head \\\"https://sealos.hub:5000/v2/pause/manifests/3.9\\\": EOF\" image=\"sealos.hub:5000/pause:3.9\" FATA[0001] pulling image: failed to pull and unpack image \"sealos.hub:5000/pause:3.9\": failed to resolve reference \"sealos.hub:5000/pause:3.9\": failed to do request: Head \"https://sealos.hub:5000/v2/pause/manifests/3.9\": EOF Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service. Error: failed to init masters: master pull image failed, error: exit status 1\n\n改配置，让其通过http访问，但依然还是通过https访问\n\n### What is the expected behavior?\n\n能够正常安装成功\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-18T02:24:34Z",
        "number": 5992,
        "state": "CLOSED",
        "title": "fail to install",
        "url": "https://github.com/labring/sealos/issues/5992",
        "login": "ATCThunder",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n系统： rockylinux8.5\nsealos: v5.0.0 v5.0.1\n异常的k8s版本：v1.31及以上，docker版本container版本都是\n安装命令:\nimages='\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1\n'\nsealos run $images\n1.31 1.32 1.33都进行了测试\n主要报错片段：\nvm.max_map_count = 2147483642 # sealos\n INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10 \nImage is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136\nCreated symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.\n INFO [2025-09-15 23:10:33] >> init kubelet success \n INFO [2025-09-15 23:10:33] >> init rootfs success \n2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.\n2025-09-15T23:10:34 info Copying kubeadm config to master0\n2025-09-15T23:10:34 info start to generate cert and kubeConfig...\n2025-09-15T23:10:34 info start to generate and copy certs to masters...\n2025-09-15T23:10:34 info apiserver altNames : {map[apiserver.cluster.local:apiserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}\n2025-09-15T23:10:34 info Etcd altnames : {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128 ::1:::1]}, commonName : k8s-01\n2025-09-15T23:10:35 info start to copy etc pki files to masters\n2025-09-15T23:10:35 info start to create kubeconfig...\n2025-09-15T23:10:36 info start to copy kubeconfig files to masters\n2025-09-15T23:10:36 info start to copy static files to masters\n2025-09-15T23:10:36 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n\n<img width=\"1920\" height=\"671\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nSystem: rockylinux8.5\nsealos: v5.0.0 v5.0.1\nException k8s version: v1.31 and above, docker version container version are all\nInstallation command:\nimages='\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1\n'\nsealos run $images\n1.31 1.32 1.33 are all tested\nMain error report segments:\nvm.max_map_count = 2147483642 # sealos\n INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10\nImage is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136\nCreated symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.\n INFO [2025-09-15 23:10:33] >> init kubelet success\n INFO [2025-09-15 23:10:33] >> init rootfs success\n2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.\n2025-09-15T23:10:34 info Copying kubeadm config to master0\n2025-09-15T23:10:34 info start to generate cert and kubeConfig...\n2025-09-15T23:10:34 info start to generate and copy certs to masters...\n2025-09-15T23:10:34 info apiserver altNames: {map[apserver.cluster.local:apserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}\n2025-09-15T23:10:34 info Etcd altnames: {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128::1:::1]}, commonName: k8s-01\n2025-09-15T23:10:35 info start to copy etc pki files to masters\n2025-09-15T23:10:35 info start to create kubeconfig...\n2025-09-15T23:10:36 info start to copy kubeconfig files to masters\n2025-09-15T23:10:36 info start to copy static files to masters\n2025-09-15T23:10:36 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n\n<img width=\"1920\" height=\"671\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-16T14:03:24Z",
        "number": 5987,
        "state": "CLOSED",
        "title": "BUG: sealos run 安装1.31及以上版本的k8s时，全部会安装失败！ || BUG: sealos run When installing k8s version 1.31 and above, all installations will fail!",
        "url": "https://github.com/labring/sealos/issues/5987",
        "login": "aiqianbao1314",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n<img width=\"2040\" height=\"204\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3d34da88-fbce-435f-a177-4499adc3ee34\" />\n\n```\nshim: /var/run/image-cri-shim.sock\ncri: /var/run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: false\ntimeout: 15m\nauth: admin:passw0rd\n\nregistries:\n- address: https://hub.192.168.64.4.nip.io\n  auth: admin:3c0e9478196ec93f2e5291d119408ddc\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-14T11:57:18Z",
        "number": 5979,
        "state": "CLOSED",
        "title": "BUG: images shim not sport local  registry",
        "url": "https://github.com/labring/sealos/issues/5979",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n```\n# Copyright © 2022 sealos.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nshim: /var/run/image-cri-shim.sock\ncri: /run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: true\nimage: /var/lib/image-cri-shim\ntimeout: 15m\nauth: admin:passw0rd\n\nregistries:\n- address: http://192.168.64.1:5000\n  auth: admin:passw0rd\n\n```\n\n### If you have solution，please describe it\n\nIt needs to be restarted, and dynamic loading is better.\n\n\n### What alternatives have you considered?\n\n```\n# Copyright © 2022 sealos.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nshim: /var/run/image-cri-shim.sock\ncri: /run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: true\nimage: /var/lib/image-cri-shim\ntimeout: 15m\nauth: admin:passw0rd\n\nregistryConfig: /etc/image-cri-shim.registry.yaml\n```\n\n```\nregistries:\n- address: http://192.168.64.1:5000\n  auth: admin:passw0rd\n```",
        "closed": true,
        "closedAt": "2025-10-10T08:48:46Z",
        "number": 5978,
        "state": "CLOSED",
        "title": "Feature: image shim online regsitry auto load",
        "url": "https://github.com/labring/sealos/issues/5978",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 296   |\n| ✅ Successful | 200   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 95    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/node/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17700194044?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-14T04:54:44Z",
        "number": 5977,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5977",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIt is mainly to solve the problem of mirror offline caching. At present, all of them need to be packaged into tar before they can be offline. So is it more difficult for us to cache all cluster mirror images into registry format? We only need sealos run k8s-all to run all the clusters.\n\n\n### If you have solution，please describe it\n\nSealos starts the temporary registry by default, and then sealos pulls the cluster mirror image directly from this temporary registry, and sealos runs the sealos according to the configuration file.\n\n```\nFROM scratch\nMAINTAINER sealos\nLABEL init=\"...\n      \"apps.sealos.io/type\"=registry \\  \n      \"apps.sealos.io/version\"=v1beta1  \\  \n     \"apps.sealos.io/config\"=config.yaml \n```\n#### apps.sealos.io/type=registry \nDefine the type of cluster mirror as registry type.\n\n#### apps.sealos.io/config\n\nDefining the configuration of registry mainly solves the startup process problem. It will carry out sealos run according to the configuration of config. Of course, there will be no dependency problem here.\n\n```yaml\napiVersion: apps.sealos.io/v1beta1\nkind: Action\nspec:\n   check:\n      role: master,node\n      runCmd:\n      -  xxx\n      -  xxx\n   runApp:\n   - kubernetes:v1.28.15\n   - xxx\n   - xxx\n    \n\n```\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5975,
        "state": "OPEN",
        "title": "Feature: sealos v2 registry design",
        "url": "https://github.com/labring/sealos/issues/5975",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Submit PR according to my requirements\n\n1. Generate weekly community submission record PR and issue\n\n2. Count the number of PRs submitted by each person and the number of consolidations\n\n3. Or what you think is a good open source community weekly newspaper\n\n4. It's better to write a workflow timed trigger. If you can't do it, you can raise the issue regularly and submit my request.",
        "closed": true,
        "closedAt": "2025-09-11T11:01:15Z",
        "number": 5972,
        "state": "CLOSED",
        "title": "pr week report",
        "url": "https://github.com/labring/sealos/issues/5972",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 300   |\n| ✅ Successful | 203   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 96    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/license/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17559881246?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-09T01:14:07Z",
        "number": 5953,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5953",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. Use https://github.com/labring/sealos/blob/main/scripts/cloud/build-offline-tar.sh build offline package.\n2. Use the install.sh in build path.\n3. desktop-frontend can't be runing, it always in Init:CrashLoopBackOff\n4. The log in desktop-frontend-xxx init-database container is:\n```\nPrisma schema loaded from desktop/prisma/global/schema.prisma\nDatasource \"db\": CockroachDB database \"global\", schema \"public\" at \"sealos-cockroachdb-public.sealos.svc.cluster.local:26257\"\n\nError: P1001: Can't reach database server at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`\n\nPlease make sure your database server is running at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`.\nPrisma schema loaded from desktop/prisma/region/schema.prisma\nDatasource \"db\": CockroachDB database, schema \"public\" at \"sealos-cockroachdb-public.sealos.svc.cluster.local:26257\"\n```\n\nI change the db url from:\n```\nglobalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global\"\nregionalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257\n```\nto\n```\nglobalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global?sslmode=require\"\n regionalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require\"\n```\n\nIt always have the same error.\n\n### What is the expected behavior?\n\ndesktop-frontend-xxx be running\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.1\n- Containerd version:v1.7.27\n- Kubernetes version:v1.28.11\n- Operating system:Ubuntu22.04 Linux k8s-master-01 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n- Runtime environment:\n- Cluster size:1master 4node\n```\n\n### Additional information\n\nThe cockroachdb can be connected use psql in k8s by postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require.",
        "closed": true,
        "closedAt": "2025-09-11T10:33:35Z",
        "number": 5952,
        "state": "CLOSED",
        "title": "BUG: desktop-frontend-xxx can't be running",
        "url": "https://github.com/labring/sealos/issues/5952",
        "login": "kevinydsk",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n什么时候支持微软的MSSQL\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### What is the problem this feature will solve?\n\nWhen will Microsoft MSSQL be supported\n\n### If you have solution, please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-08T08:58:43Z",
        "number": 5945,
        "state": "CLOSED",
        "title": "什么时候支持微软的MSSQL || When will Microsoft's MSSQL be supported",
        "url": "https://github.com/labring/sealos/issues/5945",
        "login": "renziyou",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 300   |\n| ✅ Successful | 202   |\n| ⏳ Timeouts   | 2     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 96    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n\n### Errors in controllers/license/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17500925819?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-07T08:52:57Z",
        "number": 5944,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5944",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 300   |\n| ✅ Successful | 203   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 96    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 1     |\n\n## Errors per input\n\n### Errors in controllers/node/README.md\n\n* [ERROR] <https://kubernetes.io/docs/concepts/architecture/controller/> | Network error: error sending request for url (https://kubernetes.io/docs/concepts/architecture/controller/) Maybe a certificate error?\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17472737119?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-05T03:06:01Z",
        "number": 5938,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5938",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 342   |\n| ✅ Successful | 237   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 102   |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 3     |\n\n## Errors per input\n\n### Errors in deploy/base/victoria_metrics_k8s_stack/v1.124.0/charts/victoria-metrics-k8s-stack/charts/grafana/README.md\n\n* [404] <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#envvarsource-v1-core> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [ERROR] <https://yourgerritserver/a/%7Bproject-name%7D/branches/%7Bbranch-id%7D/files/%7Bfile-id%7D/content> | Network error: error sending request for url (https://yourgerritserver/a/%7Bproject-name%7D/branches/%7Bbranch-id%7D/files/%7Bfile-id%7D/content) Maybe a certificate error?\n* [ERROR] <https://yourgerritserver/a/user%2Frepo/branches/master/files/dir1%2Fdir2%2Fdashboard/content> | Network error: error sending request for url (https://yourgerritserver/a/user%2Frepo/branches/master/files/dir1%2Fdir2%2Fdashboard/content) Maybe a certificate error?\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17412049310?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-03T06:32:38Z",
        "number": 5926,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5926",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.1.0\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-03T04:59:00Z",
        "number": 5924,
        "state": "CLOSED",
        "title": "BUG: delete admin and devbox deploy",
        "url": "https://github.com/labring/sealos/issues/5924",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.1.0\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/17283064704/job/49055050941 This action did not upload the mirror image normally, because there is no write permission of the package.\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T06:37:44Z",
        "number": 5906,
        "state": "CLOSED",
        "title": "BUG: release cloud error",
        "url": "https://github.com/labring/sealos/issues/5906",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nLICENSE\n\n### What is the current documentation state?\n\nCONTRIBUTING.md\nREADME.md\nREADME_zh.md\n\nIt is necessary to explain the LICENSE change declaration. According to the contenthttps://github.com/labring/sealos/blob/main/LICENSE.md and  https://github.com/labring/sealos/blob/main/CONTRIBUTOR_LICENSE_AGREEMENT.md\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-29T09:17:13Z",
        "number": 5902,
        "state": "CLOSED",
        "title": "Docs: LICENSE change declaration",
        "url": "https://github.com/labring/sealos/issues/5902",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "主要测试翻译转换\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\nMain test translation conversion\n",
        "closed": true,
        "closedAt": "2025-08-29T03:11:50Z",
        "number": 5901,
        "state": "CLOSED",
        "title": "测试翻译插件 || Test translation plugin",
        "url": "https://github.com/labring/sealos/issues/5901",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n- name: GitHub Translator\n\n  uses: lizheming/github-translate-action@1.1.2\n```",
        "closed": true,
        "closedAt": "2025-08-29T03:04:08Z",
        "number": 5899,
        "state": "CLOSED",
        "title": "GitHub Translator version error",
        "url": "https://github.com/labring/sealos/issues/5899",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "主要是测一下翻译插件",
        "closed": true,
        "closedAt": "2025-08-29T02:31:47Z",
        "number": 5898,
        "state": "CLOSED",
        "title": "测试的问题",
        "url": "https://github.com/labring/sealos/issues/5898",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).\n\n<Onboard this repo>",
        "closed": true,
        "closedAt": "2025-08-29T03:12:13Z",
        "number": 5895,
        "state": "CLOSED",
        "title": "✨ Set up Copilot instructions",
        "url": "https://github.com/labring/sealos/issues/5895",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n月 27 16:26:46 slave1 kubelet[18965]: I0827 16:26:46.352575 18965 reconciler_common.go:258] \"operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321152 18965 scope.go:117] \"RemoveContainer\" containerID=\"301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a\" 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321797 18965 scope.go:117] \"RemoveContainer\" containerID=\"301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a\" 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.323939 18965 remote_runtime.go:385] \"RemoveContainer from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to set removing state for cont 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324006 18965 kuberuntime_container.go:858] failed to remove pod init container \"mount-cgroup\": rpc error: code = Unknown desc = failed to set removing state 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324401 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 10s 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377405 18965 pod_startup_latency_tracker.go:102] \"Observed pod startup duration\" pod=\"kube-system/cilium-operator-6946ccbcc5-kfplj\" podStartSLOduration=3.187 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377726 18965 pod_startup_latency_tracker.go:102] \"Observed pod startup duration\" pod=\"kube-system/kube-sealos-lvscare-slave1\" podStartSLOduration=4.377699505 8月 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.357959 18965 scope.go:117] \"RemoveContainer\" containerID=\"a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f\" 8月 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.358911 18965 scope.go:117] \"RemoveContainer\" containerID=\"a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f\" 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361781 18965 remote_runtime.go:385] \"RemoveContainer from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to set removing state for cont 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361857 18965 kuberuntime_container.go:858] failed to remove pod init container \"mount-cgroup\": rpc error: code = Unknown desc = failed to set removing state 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.362621 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 20s 8月 27 16:27:17 slave1 kubelet[18965]: E0827 16:27:17.212276 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 20s\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-11T10:34:05Z",
        "number": 5878,
        "state": "CLOSED",
        "title": "mount-cgroup",
        "url": "https://github.com/labring/sealos/issues/5878",
        "login": "dzy888",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| 🔍 Total       | 1246  |\n| ✅ Successful  | 364   |\n| ⏳ Timeouts    | 0     |\n| 🔀 Redirected  | 0     |\n| 👻 Excluded    | 864   |\n| ❓ Unknown     | 0     |\n| 🚫 Errors      | 16    |\n| ⛔ Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/4.0/i18n/zh-Hans/self-hosting/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Network error: error sending request for url (https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17192059970?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-25T06:38:02Z",
        "number": 5865,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5865",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| 🔍 Total       | 1246  |\n| ✅ Successful  | 364   |\n| ⏳ Timeouts    | 0     |\n| 🔀 Redirected  | 0     |\n| 👻 Excluded    | 864   |\n| ❓ Unknown     | 0     |\n| 🚫 Errors      | 16    |\n| ⛔ Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/4.0/i18n/zh-Hans/self-hosting/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Network error: error sending request for url (https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17178540205?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-25T02:42:52Z",
        "number": 5864,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5864",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| 🔍 Total       | 1246  |\n| ✅ Successful  | 367   |\n| ⏳ Timeouts    | 0     |\n| 🔀 Redirected  | 0     |\n| 👻 Excluded    | 864   |\n| ❓ Unknown     | 0     |\n| 🚫 Errors      | 13    |\n| ⛔ Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17162608530?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-24T08:56:26Z",
        "number": 5863,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5863",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/17122518546/job/48566545145\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-29T02:30:09Z",
        "number": 5853,
        "state": "CLOSED",
        "title": "BUG:  issues-translator is not work",
        "url": "https://github.com/labring/sealos/issues/5853",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n```\n[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 -h\nAdd nodes into cluster\n\nExamples:\n\nadd to nodes :\n        sealos add --nodes x.x.x.x\n\nadd to default cluster:\n        sealos add --masters x.x.x.x --nodes x.x.x.x\n        sealos add --masters x.x.x.x-x.x.x.y --nodes x.x.x.x-x.x.x.y\n\nadd with different ssh setting:\n        sealos add --masters x.x.x.x --nodes x.x.x.x --passwd your_diff_passwd\nPlease note that the masters and nodes added in one command should have the save password.\n\nOptions:\n    --cluster='default':\n        name of cluster to applied join action\n\n    --masters='':\n        masters to be joined\n\n    --nodes='':\n        nodes to be joined\n\n    -p, --passwd='':\n        use given password to authenticate with\n\n    -i, --pk='/root/.ssh/id_rsa':\n        selects a file from which the identity (private key) for public key authentication is read\n\n    --pk-passwd='':\n        passphrase for decrypting a PEM encoded private key\n\n    --port=22:\n        port to connect to on the remote host\n\n    -u, --user='':\n        username to authenticate as\n\nUsage:\n  sealos add [flags] [options]\n\nUse \"sealos options\" for a list of global command-line options (applies to all commands).\n[root@192-168-100-62 ~]# \n\n[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 --port=65535\n2025-08-21T15:19:05 warn failed to get host arch: failed to create ssh session for 192.168.100.63:22: dial tcp 192.168.100.63:22: connect: connection refused, defaults to amd64\n2025-08-21T15:19:05 info start to scale this cluster\n2025-08-21T15:19:05 info Executing pipeline JoinCheck in ScaleProcessor.\n2025-08-21T15:19:05 info Warning: Using an even number of master nodes is a risky operation and can lead to reduced high availability and potential resource wastage. It is strongly recommended to use an odd number of master nodes for optimal cluster stability. Are you sure you want to proceed?\nDo you want to continue on '192-168-100-62' cluster? Input '192-168-100-62' to continue: 192-168-100-62\n2025-08-21T15:19:15 info checker:containerd [192.168.100.63:22]\n2025-08-21T15:19:15 info Executing pipeline PreProcess in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline PreProcessImage in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline RunConfig in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline MountRootfs in ScaleProcessor.\n2025-08-21T15:19:16 error error occur while sending mount image default-xlop4cm2: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n2025-08-21T15:19:16 error Applied to cluster error: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n2025-08-21T15:19:16 error failed to sync workdir: /root/.sealos/default error, failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\nError: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n[root@192-168-100-62 ~]#\n[root@192-168-100-62 ~]#\n\n\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-21T09:12:38Z",
        "number": 5852,
        "state": "CLOSED",
        "title": "BUG: sealos 添加节点，发现ssh端口被固定成22，无法使用自定义端口",
        "url": "https://github.com/labring/sealos/issues/5852",
        "login": "junlintianxiazhifulinzhongguo",
        "is_bot": false
      },
      {
        "body": "```\n  - name: Renew issue and Sync Images\n    uses: labring/gh-rebot@v0.0.6\n    if: ${{ github.repository_owner == env.DEFAULT_OWNER }}\n    with:\n      version: v0.0.8-rc1\n    env:\n      GH_TOKEN: \"${{ secrets.GITHUB_TOKEN }}\"\n      SEALOS_TYPE: \"issue_renew\"\n      SEALOS_ISSUE_TITLE: \"[DaylyReport] Auto build for sealos\"\n      SEALOS_ISSUE_BODYFILE: \"scripts/ISSUE_RENEW.md\"\n      SEALOS_ISSUE_LABEL: \"dayly-report\"\n      SEALOS_ISSUE_TYPE: \"day\"\n      SEALOS_ISSUE_REPO: \"labring-actions/cluster-image\"\n      SEALOS_COMMENT_BODY: \"/imagesync ghcr.io/${{ github.repository_owner }}/sealos-patch:latest\"\n\n```\n\n- [x]  delete sealos ci sync images\n- [x]  add auto sync image ci in cluster-image ",
        "closed": true,
        "closedAt": "2025-08-22T08:17:09Z",
        "number": 5850,
        "state": "CLOSED",
        "title": "remove sync images task",
        "url": "https://github.com/labring/sealos/issues/5850",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners\nTODO:\n\n- [x] add CODEOWNERS\n- [x] enable 'Require review from CODEOWNERS' in this repository",
        "closed": true,
        "closedAt": "2025-08-21T04:48:07Z",
        "number": 5848,
        "state": "CLOSED",
        "title": "add CODEOWNERS for sealos",
        "url": "https://github.com/labring/sealos/issues/5848",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Instruction:\n\nObjective: Draft a clear and actionable SECURITY.md file for Sealos. This document is intended for security researchers and users who need to report a vulnerability.\n\nPlease structure the output as follows:\n\n    Header: Title: \"Security Policy\"\n\n    Introduction: A brief statement thanking security researchers and users for helping to keep the project and its community safe.\n\n    Supported Versions: Create a table or section clearly stating which versions of the project are currently supported with security updates. Use placeholders for version numbers. For example:\n\n        [ Project Name ] | Version | Supported\n\n        ------------------ | ------- | -------------------------\n\n        [Latest Major Version] (e.g., 2.x) | >= 2.0.0 | ✅ Yes\n\n        [Previous Major Version] (e.g., 1.x) | >= 1.5.0, < 2.0.0 | ⚠️ Best-effort\n\n        [Older Version] | < 1.5.0 | ❌ No\n\n    Reporting a Vulnerability: This is the most critical section. Provide a clear, step-by-step guide on how to report a vulnerability.\n\n        Preferred Method: Instruct reporters to privately disclose vulnerabilities by emailing a dedicated security address (e.g., security@sealos.io) or by using the provider's private feature (e.g., GitHub's \"Privately report a security vulnerability\" feature on the repository's \"Security\" tab). Emphasize the importance of NOT creating a public GitHub Issue for security bugs.\n\n        What to Include: Request that the report includes a clear description, the steps to reproduce the issue, the affected versions, and the potential impact. Mention that providing a fix or suggestion is appreciated, but not required.\n\n    What to Expect After Reporting: Set clear expectations for the reporter.\n\n        Response Time: State that the team will acknowledge the report within a specific timeframe (e.g., \"within 48 hours\").\n\n        Process Outline: Briefly explain the process: the team will triage the report, work on a fix, and keep the reporter updated throughout. Define the goal for a fix timeline (e.g., \"We aim to address critical vulnerabilities within 14 days\").\n\n        Post-Fix Disclosure: Explain the project's disclosure policy. The standard is to give reporters credit and issue a public disclosure (e.g., a GitHub Security Advisory) after a fix is released and users have had time to patch.\n\n    Vulnerability Management Philosophy: Briefly describe the project's approach to security (e.g., \"We treat security vulnerabilities as our highest priority,\" \"We believe in responsible disclosure\").\n\n    Security Updates: Explain how users will be notified of security updates. Typically, this is through:\n\n        GitHub Releases (tagged with a [security] prefix or a security fix version).\n\n        GitHub Security Advisories (GHSA).\n\n        The project's official blog or mailing list.\n\n    Final Instructions for the AI:\n\n        Use clear, concise, and professional language.\n\n        Do not invent specific contact details; Use contact information: security@sealos.io\n\n        The tone should be grateful and collaborative, encouraging security research.\n\n- Additional Reference: https://github.com/electron/electron/blob/main/SECURITY.md",
        "closed": true,
        "closedAt": "2025-09-01T05:08:21Z",
        "number": 5846,
        "state": "CLOSED",
        "title": "add SECURITY.md for sealos",
        "url": "https://github.com/labring/sealos/issues/5846",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Objective: Draft a professional and inclusive Code of Conduct for an open-source software project. The document should be ready to use but will be customized by the project maintainers.\n\nPlease structure the output as follows:\n\n    Header: Title: \"Code of Conduct\"\n\n    Introduction: A brief, welcoming paragraph stating the project's commitment to providing a welcoming and harassment-free experience for everyone, regardless of background or identity. Mention that it applies to all project spaces (e.g., GitHub repos, issue trackers, Discord/Slack, mailing lists, in-person events).\n\n    Core Principles / Our Pledge: Frame this positively. As members and maintainers, we pledge to make participation in our community a harassment-free experience for everyone. We will act and interact in ways that contribute to an open, friendly, diverse, inclusive, and healthy community.\n\n    Expected Behavior: Provide a bulleted list of positive behaviors to encourage. Include examples such as:\n\n        Using welcoming and inclusive language.\n\n        Being respectful of differing viewpoints and experiences.\n\n        Gracefully accepting constructive criticism.\n\n        Focusing on what is best for the community.\n\n        Showing empathy towards other community members.\n\n    Unacceptable Behavior: Provide a bulleted list of behaviors that are considered harassment and are unacceptable. Include examples such as:\n\n        The use of sexualized language or imagery, and unwelcome sexual attention or advances.\n\n        Trolling, insulting/derogatory comments, and personal or political attacks.\n\n        Public or private harassment.\n\n        Publishing others' private information, such as a physical or email address, without their explicit permission.\n\n        Other conduct which could reasonably be considered inappropriate in a professional setting.\n\n    Scope: Clearly state that this Code of Conduct applies within all project spaces and also applies when an individual is officially representing the project in public spaces (e.g., using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an event).\n\n    Enforcement Responsibilities & Guidelines: State that project maintainers are responsible for clarifying and enforcing standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior they deem inappropriate.\n\n        Mention that maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.\n\n    Reporting and Enforcement Procedures:\n\n        Reporting: Instruct community members to contact the project team via a private, designated channel (e.g., a dedicated email address like conduct@project.example.com or a direct message to a specific team member on a platform). Assure them that all reports will be reviewed and investigated promptly and fairly. State that the project team is obligated to maintain confidentiality with regard to the reporter of an incident.\n\n        Enforcement: Outline a general process. For example:\n\n            Correction: A private, written warning from the maintainers, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n            Warning: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period.\n\n            Temporary Ban: A temporary ban from any sort of public interaction or communication with the community.\n\n            Permanent Ban: A permanent ban from the community.\n\n    Attribution: Include an attribution line at the bottom. Use the following text:\n\n        \"This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/), version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\"\n\n    Final Instructions:\n\n        Use clear, professional, and accessible language.\n\n        Use gender-neutral language (e.g., \"they/them\").\n\n        Use contact information: contact@sealos.io.\n\n        Do not invent a specific project name; keep it generic.",
        "closed": true,
        "closedAt": "2025-08-28T06:57:22Z",
        "number": 5844,
        "state": "CLOSED",
        "title": "add CODE_OF_CONDUCT for sealos",
        "url": "https://github.com/labring/sealos/issues/5844",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n进行视频更新替换\n\n### What is the current documentation state?\n\n[https://objectstorageapi.hzh.sealos.run/ecv14ujz-appstore/GitHub-2.mp4](url)\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-22T03:06:40Z",
        "number": 5827,
        "state": "CLOSED",
        "title": "sealos演示视频更新",
        "url": "https://github.com/labring/sealos/issues/5827",
        "login": "43669522",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 281   |\n| ✅ Successful | 197   |\n| ⏳ Timeouts   | 1     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 83    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16917045049?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-12T21:41:14Z",
        "number": 5817,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5817",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 284   |\n| ✅ Successful | 200   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 82    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16864760813?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-11T01:52:57Z",
        "number": 5810,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5810",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 284   |\n| ✅ Successful | 200   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 82    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16852233507?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-09T18:57:45Z",
        "number": 5808,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5808",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 284   |\n| ✅ Successful | 200   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 82    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16837333192?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-11T03:34:25Z",
        "number": 5805,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5805",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nInstall sealos using the [cloud installation script](https://github.com/labring/sealos/blob/main/scripts/cloud/install.sh), after changing the used Kubernetes version to v1.31.10\n\n### What is the expected behavior?\n\nSuccessful installation of sealos\n\n### What do you see instead?\n\nInstalling sealos with Kubernetes v1.13.10 fails:\n```\nAug 08 14:27:22 cloud-m1 kubelet[241103]: E0808 14:27:22.751318  241103 run.go:72] \"command failed\" err=\"failed to validate kubelet configuration, error: invalid configuration: duplicated enforcements \\\"pods\\\" in enforceNodeAllocatable (--enforce-node-allocatable), path: &TypeMeta{Kind:,APIVersion:,}\"\nAug 08 14:27:22 cloud-m1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE\n```\nCheck `/var/lib/kubelet/config.yaml`:\n```\n# grep enforceNodeAllocatable -A 2 /var/lib/kubelet/config.yaml\nenforceNodeAllocatable:\n- pods\n- pods\n```\nRemoving duplicate `pods` results in successful installation.\n\n### Operating environment\n\n```markdown\n- Sealos version: main\n- Docker version:\n- Kubernetes version: v1.31.10\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5804,
        "state": "OPEN",
        "title": "BUG: duplicate `pods` enforcements in `enforceNodeAllocatable` in `/var/lib/kubelet/config.yaml`",
        "url": "https://github.com/labring/sealos/issues/5804",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| 🔍 Total      | 284   |\n| ✅ Successful | 200   |\n| ⏳ Timeouts   | 0     |\n| 🔀 Redirected | 0     |\n| 👻 Excluded   | 82    |\n| ❓ Unknown    | 0     |\n| 🚫 Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16812254845?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-08T23:33:14Z",
        "number": 5803,
        "state": "CLOSED",
        "title": "🤖 Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5803",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n# Sealos Version\n由于 sealos v5.0.1 release 安装 kubernetes 1.31.10 会出现卡在 master0 的 cri 版本错误上，参照 https://github.com/labring/sealos/issues/5644 的修复，直接改用 master 的源码进行编译的\n\n#  安装流程\n```\n sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.10 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1-amd64 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1-amd64 --masters x.x.x.x --nodes x.x.x.x\n```\n\n# 错误截图\n<img width=\"975\" height=\"588\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/60411bdc-ad79-454a-ba6e-b8d61440c6ae\" />\n提示 cilium 的配置中出现参数遗漏问题，导致安装失败\n\n### What is the expected behavior?\n\nkubernetes 可以正常安装\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1(self build from latest master)\n- Docker version: None\n- Kubernetes version: 1.31.10\n- Operating system: OpenEuler\n- Runtime environment: \n- Cluster size: 1 master + 1 worker\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5797,
        "state": "OPEN",
        "title": "BUG: sealos install kubernetes 1.31.10 with cilium 1.16.1 with kubeproxyreplacement error",
        "url": "https://github.com/labring/sealos/issues/5797",
        "login": "benz9527",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nfix default values for Kubelet\n\n```\neventBurst: 100\neventRecordQPS: 50\n...\nkubeAPIBurst: 100\nkubeAPIQPS: 50\n```\n\n### If you have solution，please describe it\n\nhttps://github.com/kubernetes/kubernetes/pull/116121/files\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-28T09:02:57Z",
        "number": 5789,
        "state": "CLOSED",
        "title": "Feature: set Kubelet new default version ",
        "url": "https://github.com/labring/sealos/issues/5789",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-25T07:52:58Z",
        "number": 5787,
        "state": "CLOSED",
        "title": "Feature: auto assign action",
        "url": "https://github.com/labring/sealos/issues/5787",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./lifecycle) | @cuisongliu |",
        "closed": true,
        "closedAt": "2025-08-19T08:43:37Z",
        "number": 5779,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 lifecycle",
        "url": "https://github.com/labring/sealos/issues/5779",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/node) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5778,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/node",
        "url": "https://github.com/labring/sealos/issues/5778",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/account) | @zijiren233 |",
        "closed": false,
        "closedAt": null,
        "number": 5777,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/account",
        "url": "https://github.com/labring/sealos/issues/5777",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/db) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5776,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/db",
        "url": "https://github.com/labring/sealos/issues/5776",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/objectstorage) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5775,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/objectstorage",
        "url": "https://github.com/labring/sealos/issues/5775",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/license) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5774,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/license",
        "url": "https://github.com/labring/sealos/issues/5774",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/app) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5773,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/app",
        "url": "https://github.com/labring/sealos/issues/5773",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5772,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/job/heartbeat",
        "url": "https://github.com/labring/sealos/issues/5772",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/job/init) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5771,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/job/init",
        "url": "https://github.com/labring/sealos/issues/5771",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/terminal) | @lingdie |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5770,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/terminal",
        "url": "https://github.com/labring/sealos/issues/5770",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/user) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5769,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/user",
        "url": "https://github.com/labring/sealos/issues/5769",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/resources) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5768,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/resources",
        "url": "https://github.com/labring/sealos/issues/5768",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/pkg) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5767,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/pkg",
        "url": "https://github.com/labring/sealos/issues/5767",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/devbox) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5766,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/devbox",
        "url": "https://github.com/labring/sealos/issues/5766",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./controllers/db/adminer) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5765,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/db/adminer",
        "url": "https://github.com/labring/sealos/issues/5765",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/launchpad) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5764,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/launchpad",
        "url": "https://github.com/labring/sealos/issues/5764",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/database) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5763,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/database",
        "url": "https://github.com/labring/sealos/issues/5763",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/account) | @bxy4543 |",
        "closed": false,
        "closedAt": null,
        "number": 5762,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0 service/account",
        "url": "https://github.com/labring/sealos/issues/5762",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-30T10:31:57Z",
        "number": 5761,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service",
        "url": "https://github.com/labring/sealos/issues/5761",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/exceptionmonitor) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5760,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/exceptionmonitor",
        "url": "https://github.com/labring/sealos/issues/5760",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/pay) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:13Z",
        "number": 5759,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/pay",
        "url": "https://github.com/labring/sealos/issues/5759",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/vlogs) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-30T10:32:05Z",
        "number": 5758,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/vlogs",
        "url": "https://github.com/labring/sealos/issues/5758",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "## PR: Bump golangci-lint to v2.3.0\n\n### 📋 Task Assignment & Completion Status\n\n#### Services Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./service/devbox) | @bxy4543 |\n",
        "closed": true,
        "closedAt": "2025-08-20T09:49:39Z",
        "number": 5756,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/devbox",
        "url": "https://github.com/labring/sealos/issues/5756",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## PR: Bump golangci-lint to v2.3.1\n\nneed close #5755\n\n### 📋 Task Assignment & Completion Status\n\n#### Services Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ✅ | golangci-lint (./service/devbox) | @zijiren233 |\n| ✅ | golangci-lint (./service/vlogs) | @zijiren233 |\n| ✅ | golangci-lint (./service/pay) | @zijiren233 |\n| ✅ | golangci-lint (./service/exceptionmonitor) | @zijiren233 |\n| ✅ | golangci-lint (./service) | @zijiren233 |\n| ⬜ | golangci-lint (./service/account) | @bxy4543 |\n| ✅ | golangci-lint (./service/database) | @zijiren233 |\n| ✅ | golangci-lint (./service/launchpad) | @zijiren233 |\n\n#### Controllers Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ✅ | golangci-lint (./controllers/db/adminer) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/devbox) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/pkg) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/resources) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/user) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/terminal) | @lingdie |\n| ✅ | golangci-lint (./controllers/job/init) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/app) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/license) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/objectstorage) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/db) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/account) | @zijiren233 |\n| ✅ | golangci-lint (./controllers/node) | @zijiren233 |\n\n#### Kubernetes Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| ⬜ | golangci-lint (./lifecycle) | @cuisongliu  |\n\n*Note: Replace ⬜ with ✅ when completed*\n\n### 🛠️ Local Testing & Quick Fix Guide\n\nTo test and fix lint issues locally:\n\n```bash\n# Navigate to your module directory\ncd controllers/account\n\n# Run linter to check for issues\ngolangci-lint-v2 run --color=always --config=../../.golangci.yml\n\n# Auto-fix issues (only for linters that support --fix)\ngolangci-lint-v2 run --color=always --config=../../.golangci.yml --fix\n```\n\n**Note:** The `--fix` flag only works for certain linters. Manual fixes may still be required for some issues.",
        "closed": false,
        "closedAt": null,
        "number": 5755,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0",
        "url": "https://github.com/labring/sealos/issues/5755",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "刚经历ks删库，不知道敢不敢入手这个架构",
        "closed": true,
        "closedAt": "2025-08-05T08:54:09Z",
        "number": 5747,
        "state": "CLOSED",
        "title": "想问问未来会不会像kubesphere这样删库？",
        "url": "https://github.com/labring/sealos/issues/5747",
        "login": "Conmi-WhiteJoker",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nhttps://github.com/labring/sealos/pull/5719\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5722,
        "state": "OPEN",
        "title": "Feature: change sealos LICENSE task",
        "url": "https://github.com/labring/sealos/issues/5722",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/16489814949/job/46622113359\n\nci error\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-02T09:46:35Z",
        "number": 5721,
        "state": "CLOSED",
        "title": "BUG: ci error sealos container build",
        "url": "https://github.com/labring/sealos/issues/5721",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1、fedora环境，安装5.0.1版本，安装之后，使用自签名证书：IP.nip.ip\n\n正常访问该IP.nip.io,但是使用用户名和密码登录后，点击应用管理，跳转到了新的域名中\n提示：\n网址为 https://applaunchpad.IP.nip.io/ 的网页可能暂时无法连接，或者它已永久性地移动到了新网址。\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5711,
        "state": "OPEN",
        "title": "BUG: 域名配置",
        "url": "https://github.com/labring/sealos/issues/5711",
        "login": "Xiaomajohn",
        "is_bot": false
      },
      {
        "body": "源master 1  sealos  192.168.0.10\n源master 2 192.168.0.11\n源master 3 192.168.0.12\n想通过新增节点剔除旧节点的方式完成节点更换，问题是源master 1怎么剔除，sealos就在这个节点上，最终结果希望是：\n新master 1 sealos  192.168.0.13\n新master 2 192.168.0.14\n新master 3 192.168.0.15",
        "closed": false,
        "closedAt": null,
        "number": 5697,
        "state": "OPEN",
        "title": "sealos怎么将它所在的节点delete",
        "url": "https://github.com/labring/sealos/issues/5697",
        "login": "chenbsun",
        "is_bot": false
      },
      {
        "body": "1.  **核心问题**：\n    *   `kube-apiserver` 告警 `KubeAPIServerLatencyHigh`。\n    *   排查发现，根本原因是 `enable-admission-plugins` 参数被错误地设置为 `NodeRestriction`，导致 `validatingadmissionwebhookconfigurations` 等资源类型不存在。\n2.  **已尝试的、无效的修复手段**：\n    *   **证据一**：直接修改 `/etc/kubernetes/manifests/kube-apiserver.yaml` 文件为正确配置后，强制删除 Pod (`kubectl delete pod ...`)，重建的容器通过 `crictl inspect` 查看，其启动参数**依然是错误的**。\n    *   **证据二**：修改 `kube-system/kubeadm-config` `ConfigMap` 为正确配置后，运行 `kubeadm upgrade apply ...` 命令，虽然命令本身宣告成功，但问题**依旧存在**。\n    *   **证据三**：修改 `containerd` 的配置以解决私有仓库访问问题后，`kubeadm` 依然无法解决根本问题。\n    *   **证据四**：检查了 `/var/lib/sealos/data/default/rootfs/scripts/` 目录下的脚本，没有发现直接覆盖配置的行为。重启 `kubelet` 也**无效**。\n    *   **证据五**：尝试通过 `kubectl edit clusters.apps.sealos.io` 来修改 Sealos 的 CR，但发现该 CRD **不存在**。\n \n## 请问，Sealos 是通过什么机制来保证 `kube-apiserver` 的配置的？这个机制的配置文件在哪里？我应该如何正确地、永久性地修改 `kube-apiserver` 的启动参数？",
        "closed": true,
        "closedAt": "2025-07-18T23:59:04Z",
        "number": 5694,
        "state": "CLOSED",
        "title": "【紧急求助】Sealos 安装的 K8s v1.27.7 集群，kube-apiserver 启动参数被强制固定，无法修改",
        "url": "https://github.com/labring/sealos/issues/5694",
        "login": "zinda2000",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-08-07T06:55:39Z",
        "number": 5689,
        "state": "CLOSED",
        "title": "又见崩溃",
        "url": "https://github.com/labring/sealos/issues/5689",
        "login": "neo515",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n<img width=\"1902\" height=\"78\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b6ce5ee7-3c61-4840-ae71-497d9deb0b76\" />\n\n```shell\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-07-09T10:40:38 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T03:20:06Z",
        "number": 5688,
        "state": "CLOSED",
        "title": "BUG: deploy k8s cluster failed",
        "url": "https://github.com/labring/sealos/issues/5688",
        "login": "bestgopher",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nmacOS arm环境\n\n从 https://github.com/labring/sealos/releases/download/v5.0.1/sealos_5.0.1_linux_arm64.tar.gz 下载的包\n\n执行sealos命令失败：\n\n```\n./sealos \nzsh: exec format error: ./sealos\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-22T15:16:20Z",
        "number": 5687,
        "state": "CLOSED",
        "title": "BUG: MacOS arm环境sealos二进制文件执行失败",
        "url": "https://github.com/labring/sealos/issues/5687",
        "login": "xcbeyond",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\n1. sealos使用的4.3.7版本；kubernetes安装的是1.20.15版本，coredns安装的是1.8.4版本；系统版本：ubuntu20.04；系统内核版本：5.4.0-128-generic\n2. 1个master节点\n3. 出现的问题：在pod中解析svc时  超时\nroot@master-01:~# kubectl exec -it -n rel-apollo kjtsyxzx-msa-apollo-rv-portal-5f94b47d4b-rhw5v -- sh\n/opt/msa # nslookup hd-apollo.hd-apollo.svc.cluster.local\n;; connection timed out; no servers could be reached\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5682,
        "state": "OPEN",
        "title": "coredns解析异常",
        "url": "https://github.com/labring/sealos/issues/5682",
        "login": "xiao1wen2",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nBase env:\n    a. OS version: Centos 7\n    b. Kernel version: 5.4.226-1.el7.elrepo.x86_64\n    c. master * 1\n\nCriVersionInfo:\n  RuntimeApiVersion: v1\n  RuntimeName: containerd\n  RuntimeVersion: v1.7.27\n  Version: 0.1.0\n\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\nHow to reproduct(offline install):\n    a. sealos pull labring/kubernetes:v1.31.0\n    b. sealos save ... && copy by manual\n    c. sealos load\n    d. sealos run labring/kubernetes:v1.31.0 -e cirData=/data/containerd --debug=true --single\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n\n```\n\n### Additional information\n\nSOME LOGS\n```\n2025-07-03T22:23:00 info start to copy static files to masters\n2025-07-03T22:23:00 info start to init master0...\nI0703 22:23:00.622532    6445 interface.go:432] Looking for default routes with IPv4 addresses\nI0703 22:23:00.622588    6445 interface.go:437] Default route transits interface \"ens160\"\nI0703 22:23:00.623008    6445 interface.go:209] Interface ens160 is up\nI0703 22:23:00.623063    6445 interface.go:257] Interface \"ens160\" has 4 addresses :[172.30.0.135/24 fe80::a1ed:122:73a0:f8ed/64 fe80::9d3b:4bf8:7733:7810/64 fe80::f6d0:5eef:587f:b799/64].\nI0703 22:23:00.623081    6445 interface.go:224] Checking addr  172.30.0.135/24.\nI0703 22:23:00.623100    6445 interface.go:231] IP found 172.30.0.135\nI0703 22:23:00.623117    6445 interface.go:263] Found valid IPv4 address 172.30.0.135 for interface \"ens160\".\nI0703 22:23:00.623125    6445 interface.go:443] Found active IP 172.30.0.135 \nI0703 22:23:00.623148    6445 kubelet.go:195] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\nvalidate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nfailed to create new CRI runtime service\nk8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect\n        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1\n        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:392\ngithub.com/spf13/cobra.(*Command).execute\n        github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n        github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n        github.com/spf13/cobra@v1.8.1/command.go:1041\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47\nmain.main\n        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n        runtime/proc.go:271\nruntime.goexit\n        runtime/asm_amd64.s:1695\n2025-07-03T22:23:00 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\n2025-07-03T22:23:00 debug save objects into local: /root/.sealos/default/Clusterfile, objects: apiVersion: apps.sealos.io/v1beta1\nkind: Cluster\nmetadata:\n  creationTimestamp: \"2025-07-03T14:23:00Z\"\n  name: default\nspec:\n  hosts:\n  - ips:\n    - 172.30.0.135:22\n    roles:\n    - master\n    - amd64\n  image:\n  - labring/kubernetes:v1.31.0\n  ssh: {}\nstatus:\n  conditions:\n  - lastHeartbeatTime: \"2025-07-03T14:23:00Z\"\n    message: 'failed to init masters: master pull image failed, error: exit status\n      1'\n    reason: Apply Cluster\n    status: \"False\"\n    type: ApplyClusterError\n  mounts:\n  - env:\n      SEALOS_SYS_CRI_ENDPOINT: /var/run/containerd/containerd.sock\n      SEALOS_SYS_IMAGE_ENDPOINT: /var/run/image-cri-shim.sock\n      cirData: /data/containerd\n      criData: /var/lib/containerd\n      defaultVIP: 10.103.97.2\n      disableApparmor: \"false\"\n      registryConfig: /etc/registry\n      registryData: /var/lib/registry\n      registryDomain: sealos.hub\n      registryPassword: passw0rd\n      registryPort: \"5000\"\n      registryUsername: admin\n      sandboxImage: pause:3.10\n    imageName: labring/kubernetes:v1.31.0\n    labels:\n      check: check.sh $registryData\n      clean: clean.sh && bash clean-cri.sh $criData\n      clean-registry: clean-registry.sh $registryData $registryConfig\n      image: ghcr.io/labring/lvscare:v5.0.1\n      init: init-cri.sh $registryDomain $registryPort && bash init.sh\n      init-registry: init-registry.sh $registryData $registryConfig\n      io.buildah.version: 1.30.0\n      org.opencontainers.image.description: kubernetes-v1.31.0 container image\n      org.opencontainers.image.licenses: MIT\n      org.opencontainers.image.source: https://github.com/labring-actions/cache\n      sealos.io.type: rootfs\n      sealos.io.version: v1beta1\n      version: v1.31.0\n      vip: $defaultVIP\n    mountPoint: /var/lib/containers/storage/overlay/e72dc732ee4dd7e9eea09ec5f3dea9a6d8c1435968e05317c1962bdb8c0748f7/merged\n    name: default-7oh654x7\n    type: rootfs\n  phase: ClusterFailed\n\n2025-07-03T22:23:00 debug sync workdir: /root/.sealos/default\n2025-07-03T22:23:00 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 172.30.0.135:22 locally\nError: failed to init masters: master pull image failed, error: exit status 1\n```",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5677,
        "state": "CLOSED",
        "title": "BUG: failed to init masters: master pull image failed",
        "url": "https://github.com/labring/sealos/issues/5677",
        "login": "Axm11",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.1.3\n\n### How to reproduce the bug?\n\n<img width=\"1496\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/474da4fb-a54c-457f-97ab-32b69d8a862f\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5676,
        "state": "OPEN",
        "title": "BUG: 没有手机号登陆了",
        "url": "https://github.com/labring/sealos/issues/5676",
        "login": "qq547475331",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nAs u know,  u can not access docker.io directly in China. \nso I got an error when I use sealos build\n\n> Storing signatures\n> WARN[0123] Failed, retrying in 1s ... (1/3). Error: initializing source docker://alpine:latest: pinging container registry registry-1.docker.io: Get \"http://registry-1.docker.io/v2/\": dial tcp 75.126.164.178:80: i/o timeout \n\nCan I set a proxy address to connect docker.io in Sealos? How can I do that?\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5668,
        "state": "OPEN",
        "title": "Can Sealos set proxy address to connect docker.io?",
        "url": "https://github.com/labring/sealos/issues/5668",
        "login": "allenonline",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nsealos 如何构建离线kubesphere 部署镜像\n\n### What is the current documentation state?\n\nsealos 如何构建离线kubesphere 部署镜像\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-05T07:48:39Z",
        "number": 5667,
        "state": "CLOSED",
        "title": "sealos 如何构建离线kubesphere 部署镜像",
        "url": "https://github.com/labring/sealos/issues/5667",
        "login": "shunzi115",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n[root@gpustack-server01 ~]# sealos version\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\n### What is the expected behavior?\n\n参考文档进行操作 https://sealos.run/docs/k8s/operations/registry/sealos_sync_backup_solution\nsealos pull labring/kubernetes:v1.24.0 \nsealos create labring/kubernetes:v1.24.0\nsealos registry serve filesystem -p 9090 registry\nsealos login sealos.hub:5000\nsealos registry sync 127.0.0.1:9090 sealos.hub:5000\n\n### What do you see instead?\n\n同步操作失败\n[root@gpustack-server01 ~]# sealos --debug  registry sync 127.0.0.1:9090 http://sealos.hub:5000\n2025-06-25T16:25:34 debug checking if endpoint http://sealos.hub:5000/v2/ is alive\n2025-06-25T16:25:34 debug checking if endpoint http://127.0.0.1:9090/v2/ is alive\n2025-06-25T16:25:34 debug http endpoint http://127.0.0.1:9090/v2/ is alive\nError: registry http status code not 200\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5663,
        "state": "OPEN",
        "title": "BUG: Sealos集群镜像同步失败",
        "url": "https://github.com/labring/sealos/issues/5663",
        "login": "fengjing1009",
        "is_bot": false
      },
      {
        "body": "不能重新注册账号，也不能恢复账号了吗？没有在页面上看到有任何提示呢",
        "closed": true,
        "closedAt": "2025-08-06T02:16:14Z",
        "number": 5661,
        "state": "CLOSED",
        "title": "注销账号后，再次登录一直显示验证码错误，怎么解决？",
        "url": "https://github.com/labring/sealos/issues/5661",
        "login": "yanweiyi11",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n从v1.30.13 升级到v1.31.0出现can not mix '--config' with arguments [certificate-renewal]\nroot@k8s-1:~/.sealos/default# sealos  run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\n2025-06-19T07:37:14 info start to install app in this cluster\n2025-06-19T07:37:14 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-06-19T07:37:14 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-06-19T07:37:14 info are you sure to override these following apps? \nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\nDo you want to continue on 'k8s-1' cluster? Input 'k8s-1' to continue: k8s-1\n2025-06-19T07:37:17 info Executing PreProcess Pipeline in InstallProcessor\n2025-06-19T07:37:17 info Executing pipeline MountRootfs in InstallProcessor.\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.102:22es to 192025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n2025-06-19T07:37:47 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-06-19T07:37:47 info trying default http mode to sync images to hosts [192.168.2.101:22]\n2025-06-19T07:37:49 info Executing UpgradeIfNeed Pipeline in InstallProcessor\n2025-06-19T07:37:49 info Change ClusterConfiguration up to newVersion if need.\n2025-06-19T07:37:49 info start to upgrade master0\n[config/images] Pulled registry.k8s.io/kube-apiserver:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-scheduler:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-proxy:v1.31.0\n[config/images] Pulled registry.k8s.io/coredns/coredns:v1.11.3\n[config/images] Pulled registry.k8s.io/pause:3.9\n[config/images] Pulled registry.k8s.io/etcd:3.5.15-0\ncan not mix '--config' with arguments [certificate-renewal]\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:37:54 error upgrade cluster failed\nError: exit status 1\n\n换成1.31.10 也是一样的\nroot@k8s-1:~/.sealos/default# sealos  run labring/kubernetes:v1.31.10\n2025-06-19T07:41:54 info start to install app in this cluster\n2025-06-19T07:41:54 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-06-19T07:41:54 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-06-19T07:41:54 info Executing PreProcess Pipeline in InstallProcessor\nTrying to pull docker.lishuai.fun/labring/kubernetes:v1.31.10...\nGetting image source signatures\nCopying blob 98e5ac018847 done  \nCopying blob 2587c72eb500 done  \nCopying blob d9dc5de7feeb done  \nCopying blob f4b501a28252 done  \nCopying config 792e378009 done  \nWriting manifest to image destination\nStoring signatures\n2025-06-19T07:42:59 info Executing pipeline MountRootfs in InstallProcessor.\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.103:22es to 192025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.102:22es to 192025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n2025-06-19T07:43:31 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-06-19T07:43:31 info trying default http mode to sync images to hosts [192.168.2.101:22]\n2025-06-19T07:43:32 info Executing UpgradeIfNeed Pipeline in InstallProcessor\n2025-06-19T07:43:32 info Change ClusterConfiguration up to newVersion if need.\n2025-06-19T07:43:32 info start to upgrade master0\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:43:33 warn image pull pre-upgrade failed: master pull image failed, error: exit status 1\ncan not mix '--config' with arguments [certificate-renewal]\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:43:33 error upgrade cluster failed\nError: exit status 1\n感觉v5.0.1 这个版本好久没更新了\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1\n- Docker version:\n- Kubernetes version: 1.30.13\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5657,
        "state": "OPEN",
        "title": "BUG: sealos v5.0.1 从v1.30.x升级到1.31.x失败，can not mix '--config' with arguments [certificate-renewal]",
        "url": "https://github.com/labring/sealos/issues/5657",
        "login": "912988434",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nThe [build-multi-arch-image](https://sealos.run/docs/k8s/operations/build-image/build-multi-arch-image) is designed to build images targeting multi-architecture clusters. However, the images currently loaded into the cluster are single-architecture only. As a result, in mixed-architecture clusters, nodes may pull images that do not match their architecture, leading to unexpected behavior.\n\n### If you have solution，please describe it\n\nTo address this, I propose introducing a flag such as `--multi-arch` or `--save-multi-arch` to allow saving multi-architecture images at build time, it call into sreg and use `copy.CopyAllImages`  to save images.\n\nI’ve done some preliminary work in this area:\n\n- https://github.com/labring/sealos/compare/main...cnfatal:sealos:main\n\n- https://github.com/labring/sreg/compare/main...cnfatal:sreg:main\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5652,
        "state": "OPEN",
        "title": "Feature: Allow save multi-arch images on `sealos build`",
        "url": "https://github.com/labring/sealos/issues/5652",
        "login": "cnfatal",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n拉取的版本是：docker pull labring/kubernetes:v1.33.1-5.0.1 到自己的harbar仓库中\n\n执行\n`sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \\\n            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \\\n            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \\\n            --masters 10.1.21.175 --nodes 10.1.21.176 \\\n            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \\\n            --pk /root/.ssh/id_rsa --user=root --cluster test131  > /root/cluster-file/test131/Clusterfile`\n\n然后执行：\nsealos apply -f Clusterfile --debug\n\n最后报错：\n`Error: failed to init masters: master pull image failed, error: run command `kubeadm config images pull --cri-socket unix:///var/run/image-cri-shim.sock  --kubernetes-version v1.32.5  -v 6` on 10.1.21.175:22, output: I0610 18:39:09.550114  588091 interface.go:432] Looking for default routes with IPv4 addresses\nI0610 18:39:09.550175  588091 interface.go:437] Default route transits interface \"ens192\"\nI0610 18:39:09.550494  588091 interface.go:209] Interface ens192 is up\nI0610 18:39:09.550554  588091 interface.go:257] Interface \"ens192\" has 3 addresses :[10.1.21.175/24 10:1:21::175/64 fe80::250:56ff:fe9f:2775/64].\nI0610 18:39:09.550570  588091 interface.go:224] Checking addr  10.1.21.175/24.\nI0610 18:39:09.550579  588091 interface.go:231] IP found 10.1.21.175\nI0610 18:39:09.550602  588091 interface.go:263] Found valid IPv4 address 10.1.21.175 for interface \"ens192\".\nI0610 18:39:09.550611  588091 interface.go:443] Found active IP 10.1.21.175\nI0610 18:39:09.550630  588091 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\nvalidate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nfailed to create new CRI runtime service\nk8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect\n        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1\n        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:387\ngithub.com/spf13/cobra.(*Command).execute\n        github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n        github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n        github.com/spf13/cobra@v1.8.1/command.go:1041\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47\nmain.main\n        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n        runtime/proc.go:272\nruntime.goexit\n        runtime/asm_amd64.s:1700\n, error: Process exited with status 1,\n`\n\n我用1.29.9不会报错，1.32.5和1.33.1都会报同样的错误\ncontainerd版本是：v1.6.15\n\n错误如何解决呢\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5644,
        "state": "CLOSED",
        "title": "sealos部署k8s v1.32.5集群错误",
        "url": "https://github.com/labring/sealos/issues/5644",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\nliunx环境\n生成Clusterfile文件命令：\n`sealos gen  harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/pause:3.10 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-apiserver:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-controller-manager:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-scheduler:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-proxy:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/coredns:v1.12.0 \\\n            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \\\n            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \\\n            --masters 10.1.21.175 --nodes 10.1.21.176 \\\n            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \\\n            --pk /root/.ssh/id_rsa --user=root --cluster test3  > /root/cluster-file/test3/Clusterfile`\n\n其中kubernetes-sealos:v1.33.1-5.0.1是拉取了docker pull labring/kubernetes:v1.33.1-5.0.1 这个镜像传到我自己的harbor仓库中的地址\n\n执行后报错：\n\n<img width=\"1499\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/172d263c-c36d-462c-a25c-5efc815c843f\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.1\n- Docker version:无\n- Kubernetes version:1.33.1\n- Operating system:centos7\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T06:08:46Z",
        "number": 5642,
        "state": "CLOSED",
        "title": "BUG: sealos自定义安装，k8s版本v1.33.1部署集群失败",
        "url": "https://github.com/labring/sealos/issues/5642",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nnone\n\n### If you have solution，please describe it\n\nnone\n\n### What alternatives have you considered?\n\nnone",
        "closed": false,
        "closedAt": null,
        "number": 5640,
        "state": "OPEN",
        "title": "Feature: 什么时候支持Flutter",
        "url": "https://github.com/labring/sealos/issues/5640",
        "login": "JocelynFloresz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1.sealos version\nv5.0.1\n\n![Image](https://github.com/user-attachments/assets/f5f5358c-6158-476a-b9df-ed182d165e16)\n2.linux Operating system kernel version:\n\n![Image](https://github.com/user-attachments/assets/6f6c93c5-955d-4bc9-a4bc-c7e8a7f8aff8)\n\n3.sealos run --debug registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8\n\n4.error\n\n![Image](https://github.com/user-attachments/assets/0c859efa-f0c9-43d3-b603-04e411040f14)\n\n5.两周前是可以部署的，最近一直报上面的错误，我理解这个应该是个bug，也不知道如何配置能够让sealos 拉取本地镜像，使用sealos pull 可以正常拉取kube相关镜像\n\n![Image](https://github.com/user-attachments/assets/0f03d4c8-d09b-49f1-b36e-586f18cde874)\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T06:12:02Z",
        "number": 5638,
        "state": "CLOSED",
        "title": "The latest version ( v5.0.1 ) of the sealos deployment k8s will fail due to a public image pull",
        "url": "https://github.com/labring/sealos/issues/5638",
        "login": "yifeng-x",
        "is_bot": false
      },
      {
        "body": "按照官方文档进行离线部署，发现sealos run执行中，仍然会从远端拉取kube相关的镜像，请问该如何处理\n\n![Image](https://github.com/user-attachments/assets/1a8ceffb-530c-4042-9299-0056b457d06f)",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5637,
        "state": "CLOSED",
        "title": "离线部署sealos，镜像拉取失败导致部署失败",
        "url": "https://github.com/labring/sealos/issues/5637",
        "login": "yifeng-x",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nPress the button to stop all projects at once to ensure no financial expenses.\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5634,
        "state": "OPEN",
        "title": "One-click to start or stop billing",
        "url": "https://github.com/labring/sealos/issues/5634",
        "login": "maqun02",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\nreboot集群后 ，k8s无法自动起来，并且找到不到 k8s ,reset 之后重新 run 报错 warn failed to copy image 127.0.0.1:40945/coredns:1.7.0: copying system image from manifest list: trying to reuse blob sha256:c6568d217a0023041ef9f729e8836b19f863bcdb612bb3a329ebc165539f5a80 at destination: pinging container registry 192.31.16.161:5050: received unexpected HTTP status: 503 Service Unavailable\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5616,
        "state": "OPEN",
        "title": "BUG:sealos 重启机器后 集群无法自动重启",
        "url": "https://github.com/labring/sealos/issues/5616",
        "login": "notionev",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nI attempted to test how Sealos recovers after a failure of the master0 node. During this, I discovered an issue: on control plane nodes other than master0, the host file on the host machine and the host file inside the pods are inconsistent. For example, in the controller-manager pod, when master0 becomes unavailable, controller-manager fails. The same behavior is observed in other control plane components as well.\n### Reproduce\nThe cluster is configured with three control plane nodes: master1, master2, and master3, and one worker node node1. The cluster was started with the following command:\n```sh\nroot@master1:~# sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 \\\n     --masters 192.168.64.15,192.168.64.16,192.168.64.17 \\\n     --nodes 192.168.64.18 \\\n     -u root --pk='/root/.ssh/multipass_key' \\\n     --output Clusterfile\nsealos apply -f Clusterfile\n```\nThen shutdown the master1. Below is the output from controller-manager on master2 after master1 (master0) went offline. As you can see, the DNS resolution of apiserver.cluster.local points to master1 (master0).\n```sh\nroot@master2:~# kubectl logs -n kube-system kube-controller-manager-master2 | tail -n10\nE0526 13:58:46.446458       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:58:52.590254       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:58:58.736356       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:01.811818       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:04.879391       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:11.023681       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:14.104059       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:17.166370       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:20.240788       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:26.395983       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\n```\nBy directly inspecting the hosts file under /var/lib/kubelet/pods, I confirmed this was indeed the case.\n```sh\nroot@master2:~# grep -i \"apiserver.cluster.local\" -r /var/lib/kubelet/pods/\n/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/volumes/kubernetes.io~configmap/kube-proxy/..2025_05_26_02_51_19.923862820/kubeconfig.conf:    server: https://apiserver.cluster.local:6443\n/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/etc-hosts:192.168.64.16 apiserver.cluster.local\nroot@master2:~# ls /var/lib/kubelet/pods/*/containers\n/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/containers:\nkube-scheduler\n\n/var/lib/kubelet/pods/362d9138-58c5-4961-b8b9-39d9aa57f8d7/containers:\ncoredns\n\n/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/containers:\nkube-controller-manager\n\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/containers:\nkube-proxy\n\n/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/containers:\netcd\n\n/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/containers:\napply-sysctl-overwrites  cilium-agent  clean-cilium-state  config  install-cni-binaries  mount-bpf-fs  mount-cgroup\n\n/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/containers:\nkube-apiserver\n\n/var/lib/kubelet/pods/e9afef92-76ae-461a-9fde-f105f5521e07/containers:\ncoredns\n```\nWe can see that pod 56a1a6061487d03b440de1b2e6d4cba5 is kube-controller-manager with host record\n192.168.64.15 apiserver.cluster.local.\nHowever, the host machine’s /etc/hosts file looks like this:\n```sh\nroot@master2:~# cat /etc/hosts\n# Your system has configured 'manage_etc_hosts' as True.\n# As a result, if you wish for changes to this file to persist\n# then you will need to either\n# a.) make changes to the master file in /etc/cloud/templates/hosts.debian.tmpl\n# b.) change or remove the value of 'manage_etc_hosts' in\n#     /etc/cloud/cloud.cfg or cloud-config from user-data\n#\n127.0.1.1 master2 master2\n127.0.0.1 localhost\n# The following lines are desirable for IPv6 capable hosts\n::1 localhost ip6-localhost ip6-loopback\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n192.168.64.15 sealos.hub\n192.168.64.16 apiserver.cluster.local\n```\nAnd not all control plane component pods have their apiserver.cluster.local entry pointing to master1 (master0) — some point to the host machine instead.\nAfter reading parts of Sealos’ source code, I suspect this inconsistency is caused by a data race. During the initialization of control plane nodes, Sealos modifies the host machine’s /etc/hosts file twice: the first time during the init phase, it checks whether the node is a master, and if so, it points to master0. This makes sense because during join master, the node locates master0 through apiserver.cluster.local. The second modification happens after the kubeadm join command completes, at which point it only waits for the API server to become ready, but not all control plane components.\nThe first host modification in init stage\n```go\ndefaultInitializers = append(defaultInitializers, &registryHostApplier{}, &registryApplier{}, &defaultCRIInitializer{}, &apiServerHostApplier{}, &lvscareHostApplier{}, &defaultInitializer{})\n\nfunc (a *apiServerHostApplier) Apply(ctx Context, host string) error {\n    if slices.Contains(ctx.GetCluster().GetMasterIPAndPortList(), host) {\n    \tif err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetMaster0IP(), constants.DefaultAPIServerDomain); err != nil {\n    \t\treturn fmt.Errorf(\"failed to add hosts: %v\", err)\n    \t}\n    \treturn nil\n    }\n    if err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetVIP(), constants.DefaultAPIServerDomain); err != nil {\n    \treturn fmt.Errorf(\"failed to add hosts: %v\", err)\n    }\n    \n    return nil\n}\n```\nThe second host modification after kubeadm join\n```go\nfunc (k *KubeadmRuntime) joinMasters(masters []string) error {\n    // ...\n    err = k.sshCmdAsync(master, joinCmd)\n    if err != nil {\n    \treturn fmt.Errorf(\"exec kubeadm join in %s failed %v\", master, err)\n    }\n    \n    err = k.execHostsAppend(master, master, k.getAPIServerDomain())\n    if err != nil {\n    \treturn fmt.Errorf(\"add master0 apiserver domain hosts in %s failed %v\", master, err)\n    }\n    // ...\n}\n```\nThe possible cases\n```\ncase1 apiserver.cluster.local points to master0: first host modification --> pod start --> second host modification\ncase2 apiserver.cluster.local points to host machine: first host modification --> second host modification --> pod start \n```\nHere is the doc about behavior of [kubeadm join](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#feature-gates)\n```\nWithout the feature gate enabled, kubeadm will only wait for the kube-apiserver on a control plane node to become ready. \nThe wait process starts right after the kubelet on the host is started by kubeadm. \nYou are advised to enable this feature gate in case you wish to observe a ready state from all control plane components \nduring the kubeadm init or kubeadm join command execution.\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5614,
        "state": "OPEN",
        "title": "BUG: data race between control plane components and host modification",
        "url": "https://github.com/labring/sealos/issues/5614",
        "login": "A2ureStone",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\ninstall\n\n### What is the current documentation state?\n\npod内访问一个外部服务地址，该地址解析的ip也为100.64开头，结果无法正常访问，修改podIP段后才正常\n\n\n### What is your suggestion?\n\n建议默认podIP端设置为内网常见地址段，如192.168、10、172等。\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-23T06:43:05Z",
        "number": 5608,
        "state": "CLOSED",
        "title": "podIP段默认使用了100地址，导致某些场景下网段冲突不通",
        "url": "https://github.com/labring/sealos/issues/5608",
        "login": "gitfxx",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\n对于使用sealos部署的已经运行的集群去修改配置，是不生效的。\n\n官方文档中有关于自定义配置更新的描述“集群运行成功后会把 Clusterfile 保存到 .sealos/default/Clusterfile 文件中，可以修改其中字段来重新 apply 对集群进行变更。” 不知道这个是我理解错误，还是bug没有生效。\nhttps://sealos.run/docs/k8s/operations/run-cluster/gen-apply-cluster \n\n我集群部署运行起来后，发现calico网络有些异常，需要指定一下interface，然后我到Clusterfile中添加了相关的calico配置，重新sealos apply -f Clusterfile,发现没有生效。我指定配置的方法也是参考官方文档\n---\napiVersion: apps.sealos.io/v1beta1\nkind: Config\nmetadata:\n  name: calico\nspec:\n  path: charts/calico/values.yaml\n  strategy: merge\n  data: |\n    installation:\n      enabled: true\n      kubernetesProvider: \"\"\n      calicoNetwork:\n        ipPools:\n        - blockSize: 26\n          cidr: 10.160.0.0/12\n          encapsulation: IPIP\n          natOutgoing: Enabled\n          nodeSelector: all()\n        nodeAddressAutodetectionV4:\n          interface: \"eth0\"\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-23T06:42:43Z",
        "number": 5604,
        "state": "CLOSED",
        "title": "BUG: sealos自定义配置更新不生效",
        "url": "https://github.com/labring/sealos/issues/5604",
        "login": "maolchen",
        "is_bot": false
      },
      {
        "body": "I'm entering the system through wsl -d Ubuntu-22.04, and this system is newly installed. I want to install a single-node version using Sealos on it. However, during the installation, it keeps saying that port 6433 is occupied.\n\nBut when I check with sudo lsof -i :6433, it shows nothing is using it. Why is that?\n\nMy installation command is:\nbash ./install.sh --cloud-version=v5.0.0 --zh --single\n\n![Image](https://github.com/user-attachments/assets/966da201-9a7b-403a-bb06-1b079670316c)",
        "closed": true,
        "closedAt": "2025-08-30T10:28:23Z",
        "number": 5596,
        "state": "CLOSED",
        "title": "When installing, I get an error saying that port 6433 is already in use.",
        "url": "https://github.com/labring/sealos/issues/5596",
        "login": "dwtyxugy",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. In this env:\n```\nLinux k8s-master-0 6.8.0-58-generic #60-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:09:50 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/arm64\n```\n2. With this config (cluster.yaml):\n```\napiVersion: apps.sealos.io/v1beta1\nkind: Cluster\nmetadata:\n  name: default\nspec:\n  hosts:\n    - ips:\n      - \"192.168.64.15\"\n      roles:\n        - master\n        - arm64\n    - ips:\n      - \"192.168.64.21\"\n      - \"192.168.64.22\"\n      - \"192.168.64.23\"\n      roles:\n        - node\n        - arm64 \n  image:\n    - registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\n    - registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1\n    - registry.cn-shanghai.aliyuncs.com/labring/cilium:1.16.3\n  ssh:\n    passwd: xxxx\n    pk: /root/.ssh/id_rsa\n    port: 22\n    user: root\n```\n3. Run:\n```\nsealos apply -f cluster.yaml\n```\n4. See Error:\n```\n2025-05-05T00:06:40 info start to copy kubeconfig files to masters\n2025-05-05T00:06:40 info start to copy static files to masters\n2025-05-05T00:06:40 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-05-05T00:06:40 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5577,
        "state": "CLOSED",
        "title": "BUG: image pull failed",
        "url": "https://github.com/labring/sealos/issues/5577",
        "login": "monshunter",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. ubuntu 24.04  server 3台机器，使用的如下命令在一台机器上安装：\n（域名+内网ip的方式安装的）\n$ curl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\n  --cloud-version=v5.0.1 \\\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\n  --proxy-prefix=$PROXY_PREFIX \\\n  --cloud-domain=<your_domain>\n\n2. 安装过程出现如下错误，但不影响安装执行。\nError from server (NotFound): configmaps \"desktop-frontend-config\" not found\n![Image](https://github.com/user-attachments/assets/5cb2025e-e428-4335-b934-e2a9b118e770)\n\n4. 安装完成之后，k8s查看pod日志如下：\n\n![Image](https://github.com/user-attachments/assets/faa81804-729f-484a-b46e-62cf9590ab65)\n\n\n1、安装过程出现问题：Error from server (NotFound): configmaps \"desktop-frontend-config\" not found。\n是否需要解决？\n\n2、使用域名+内网ip的访问不到，这个怎么解决？\n\n![Image](https://github.com/user-attachments/assets/3d4db61e-75bb-4270-8cc2-f0125e8b50e4)\n\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5571,
        "state": "OPEN",
        "title": "BUG: install success but doesn't work",
        "url": "https://github.com/labring/sealos/issues/5571",
        "login": "shadowlaser",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5570,
        "state": "OPEN",
        "title": "BUG: jaevor/go-nanoid deleted, replace to github.com/matoous/go-nanoid/v2 v2.1.0",
        "url": "https://github.com/labring/sealos/issues/5570",
        "login": "lingdie",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n安装完没有devbox，应用商店也没有\n![Image](https://github.com/user-attachments/assets/269830d7-0faf-4777-a45d-6d2bf9edf326)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T10:29:40Z",
        "number": 5558,
        "state": "CLOSED",
        "title": "Feature: install cloud miss devbox runtime",
        "url": "https://github.com/labring/sealos/issues/5558",
        "login": "cx2c",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n网页版最新\n\n### How to reproduce the bug?\n\n请问我把DevBox发布成应用之后，pod一直拉镜像失败是因为啥？\n\n<img width=\"2289\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5a1061c6-8bd1-4927-97c8-b5a3729e8b7b\" />\n\n<img width=\"2558\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/31fdd36e-3e7c-463a-a30a-46f9e8e65e7a\" />\n\n<img width=\"2558\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/43ec5d0d-c8d5-4c28-a83b-ac98ea5be3fd\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-04-17T06:35:04Z",
        "number": 5531,
        "state": "CLOSED",
        "title": "正式应用无法拉取镜像",
        "url": "https://github.com/labring/sealos/issues/5531",
        "login": "Zoey-Zhang922",
        "is_bot": false
      },
      {
        "body": "According to [this post](https://securitylab.github.com/resources/github-actions-preventing-pwn-requests/), the workflow files are susceptible to leaking sensitive data like secrets of repositories.\nAs a result, we should make the following changes to the workflow files:\n1. Use [GITHUB_TOKEN](https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication) everywhere.\n2. The actions should be triggered by `pull_request` or similarly less permissive event type.\n\nThis is the issue for tracking the above change. All pull requests should reference this issue.",
        "closed": true,
        "closedAt": "2025-08-25T07:48:45Z",
        "number": 5516,
        "state": "CLOSED",
        "title": "Tracking issue for Github Actions workflow files reworking",
        "url": "https://github.com/labring/sealos/issues/5516",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n```\n[root@haisen1 ~]# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4                   registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8           --masters $MASTER_NODE_1                                                 -p Hxxxxxxen123\n2025-03-23T12:46:49 info start to install app in this cluster\n2025-03-23T12:46:49 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-03-23T12:46:49 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-03-23T12:46:49 info are you sure to override these following apps?\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4\nregistry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8\n✗ Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue: █\n```\n\n我想在脚本里执行，但是它会阻塞住\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-26T03:51:18Z",
        "number": 5498,
        "state": "CLOSED",
        "title": "How to automatically default and avoid interactive input Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue",
        "url": "https://github.com/labring/sealos/issues/5498",
        "login": "lhsaq2009",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n```\nwget https://****-public.oss-cn-beijing.aliyuncs.com/sealos_5.0.1_linux_amd64.rpm \\\n    && rpm -ivh sealos_5.0.1_linux_amd64.rpm                                           \\\n    && sudo yum install -y chrony iproute socat iproute-tc jq\n\nMASTER_NODE_1=$(hostname -I | awk '{print $1}')\nWORKER_NODE_1=\"${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 1 ))\"\nWORKER_NODE_2=\"${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 2 ))\"\n\n# 仅需第一个 Master 节点上运行 sealos run 命令即可\n# 使用内外地址大约 4 分钟 集群构建完成\n\n# https://hub.docker.com/r/labring/kubernetes/tags\n# https://hub.docker.com/r/labring/helm/tags\n\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.8 \\\n           registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4       \\\n           registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.5     \\\n     --masters $MASTER_NODE_1                                           \\\n     --nodes   $WORKER_NODE_1,$WORKER_NODE_2                            \\\n     -p Haisen123\n\n# 安装失败重试前，先执行清理：sealos reset\n\nkubectl get nodes\n    \n    NAME      STATUS   ROLES           AGE   VERSION\n    haisen1   Ready    control-plane   67s   v1.28.8\n    haisen2   Ready    <none>          43s   v1.28.8\n    haisen3   Ready    <none>          42s   v1.28.8\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-17T12:30:32Z",
        "number": 5488,
        "state": "CLOSED",
        "title": "After installation, there is no master?",
        "url": "https://github.com/labring/sealos/issues/5488",
        "login": "lhsaq2009",
        "is_bot": false
      },
      {
        "body": "是否支持在已有k8s集群上 run app镜像",
        "closed": true,
        "closedAt": "2025-03-30T05:14:59Z",
        "number": 5469,
        "state": "CLOSED",
        "title": "Is it supported run app image on existing k8s clusters?",
        "url": "https://github.com/labring/sealos/issues/5469",
        "login": "BussanQ",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1-beta2-22d9a138f\n\n### How to reproduce the bug?\n\n```\ncurl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\n  --cloud-version=v5.0.1 \\\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\n  --proxy-prefix=$PROXY_PREFIX\n```\n![Image](https://github.com/user-attachments/assets/60b154ff-0ca5-4c2d-a910-5f21a2ab9b1a)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system: Ubuntu 24.04.2 LTS\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-08T13:20:31Z",
        "number": 5455,
        "state": "CLOSED",
        "title": "BUG: Error: run chmod to rootfs failed: exit status 1",
        "url": "https://github.com/labring/sealos/issues/5455",
        "login": "hubyao",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n1\n\n### What is the current documentation state?\n\nhttps://github.com/user-attachments/assets/b0a7937f-ca81-4021-a743-9d5d605fa61d\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-06T12:47:24Z",
        "number": 5442,
        "state": "CLOSED",
        "title": "update readme",
        "url": "https://github.com/labring/sealos/issues/5442",
        "login": "zuoFeng59556",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n[root@k8s-host218 cluster-file]# sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.24.2 \\\n>   --masters [10:1:12::122] --nodes [10:1:12::236] \\\n>   --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123 \\\n>   --pk /root/.ssh/id_rsa --user=root --cluster ipv6test  > /root/cluster-file/Clusterfile-ipv6\nError: invalid ip: [10:1:12::122]\n\n无法用ipv6地址生成 Clusterfile文件\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-15T17:30:10Z",
        "number": 5420,
        "state": "CLOSED",
        "title": "Supports sealos gen to generate ipv6 Clusterfile, and can be successfully deployed",
        "url": "https://github.com/labring/sealos/issues/5420",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nroot@ubuntu:/home/ubuntu# sealos version\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\nroot@ubuntu:/home/ubuntu# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\nFlag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node\n2025-02-25T00:38:42 info Start to create a new cluster: master [192.168.110.132], worker [], registry 192.168.110.132\n2025-02-25T00:38:42 info Executing pipeline Check in CreateProcessor.\n2025-02-25T00:38:42 info checker:hostname [192.168.110.132:22]\n2025-02-25T00:38:42 info checker:timeSync [192.168.110.132:22]\n2025-02-25T00:38:42 info checker:containerd [192.168.110.132:22]\nError: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-25T08:51:01Z",
        "number": 5407,
        "state": "CLOSED",
        "title": "BUG: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first",
        "url": "https://github.com/labring/sealos/issues/5407",
        "login": "maybe-why-not",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n 5.0.1\n\n### How to reproduce the bug?\n\nsealos load -i xxx.tar\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n[root@localhost ~]# sealos load -i 11111.tar\nGetting image source signatures\nCopying blob 5879b58524ff done  \nCopying blob 6e76332bdae4 done  \nCopying blob ce8d20826ef0 done  \nCopying blob 8aa9e05fa73d done  \nGetting image source signatures\nCopying blob 8aa9e05fa73d done  \nCopying blob 6e76332bdae4 done  \nCopying blob 5879b58524ff done  \nCopying blob ce8d20826ef0 done  \nError: payload does not match any of the supported image formats:\n * oci: initializing source oci:11111.tar:: open 11111.tar/index.json: not a directory\n * oci-archive: writing blob: adding layer with blob \"sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552\": processing tar fil\ne(open /bin/kubeadm: permission denied): exit status 1\n * docker-archive: writing blob: adding layer with blob \"sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552\": processing tar \nfile(open /bin/kubeadm: permission denied): exit status 1\n * dir: open 11111.tar/manifest.json: not a directory\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-22T01:17:00Z",
        "number": 5397,
        "state": "CLOSED",
        "title": "BUG: Error: payload does not match any of the supported image formats",
        "url": "https://github.com/labring/sealos/issues/5397",
        "login": "zhaoyangpp",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0、5.0.1\n\n### How to reproduce the bug?\n\n1、我将离线包load到本地后，然后生成一个配置文件\n[root@k8s-1 ~]# sealos images\nREPOSITORY                         TAG        IMAGE ID       CREATED         SIZE\nlocalhost/labring/kubernetes       v1.28.14   4ab085e9b7cb   4 months ago    630 MB\nlocalhost/labring/metrics-server   v0.6.4     79507e56a004   17 months ago   30.1 MB\nlocalhost/labring/calico           v3.24.6    3d5490e2bcb4   21 months ago   355 MB\nlocalhost/labring/flannel          v0.20.2    af12a008a763   2 years ago     90 MB\n\n#生成配置文件\nsealos gen localhost/labring/kubernetes:v1.28.14 localhost/labring/calico:v3.24.6 --masters 192.168.80.141 -o clusterfile.yaml\n\n2、使用生成的配置文件安装k8s，遇到如下解析image name错误\n[root@k8s-1 ~]# sealos run -f clusterfile.yaml \n2025-02-21T01:04:16 info Start to create a new cluster: master [192.168.80.141], worker [], registry 192.168.80.141\n2025-02-21T01:04:16 info Executing pipeline Check in CreateProcessor.\n2025-02-21T01:04:16 info checker:hostname [192.168.80.141:22]\n2025-02-21T01:04:16 info checker:timeSync [192.168.80.141:22]\n2025-02-21T01:04:16 info checker:containerd [192.168.80.141:22]\n2025-02-21T01:04:16 info Executing pipeline PreProcess in CreateProcessor.\nError: error parsing image name \"//clusterfile.yaml\": pinging container registry registry-1.docker.io: Get \"http://registry-1.docker.io/v2/\": dial tcp 31.13.96.194:80: connect: connection refused\n\n### What is the expected behavior?\n\n正常情况下应该是不用走外网拉镜像了，这里不知道为什么会这样\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-21T11:17:48Z",
        "number": 5396,
        "state": "CLOSED",
        "title": "BUG: Offline mode deployment encounters problem of still pulling the network image",
        "url": "https://github.com/labring/sealos/issues/5396",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1、sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.21.0 \\\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.12.0 \\\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1 \\\nregistry.cn-shanghai.aliyuncs.com/labring/cert-manager:v1.8.0 \\\n --masters 192.168.73.201  --nodes 192.168.73.202 -p centos -o  Clusterfile\n\n2、sealos apply -f Clusterfile\n\n### What is the expected behavior?\n\nk8s安装成功\n\n### What do you see instead?\n\nk8s安装失败，以下为使用sealos v5.0.1安装k8s1.21.0的报错日志，但是使用同样的命令安装k8s1.27.7就可以成功安装k8s\n报错日志如下：\nW0219 23:19:27.942539   24132 strict.go:47] unknown configuration schema.GroupVersionKind{Group:\"kubeadm.k8s.io\", Version:\"v1beta3\", Kind:\"InitConfiguration\"} for scheme definitions in \"k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31\" and \"k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28\"\nno kind \"InitConfiguration\" is registered for version \"kubeadm.k8s.io/v1beta3\" in scheme \"k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31\"\nTo see the stack trace of this error execute with --v=5 or higher\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1\n- Docker version:\n- Kubernetes version:v1.21.0\n- Operating system: centos7\n- Runtime environment: vm\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-24T01:51:39Z",
        "number": 5388,
        "state": "CLOSED",
        "title": "sealos 5.0.1 An error was reported when installing k8s 1.21 and below, and the installation of 1.27.7 was successful.",
        "url": "https://github.com/labring/sealos/issues/5388",
        "login": "houbaionee",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n用github action执行ci-patch-image.yml的工作流生成的sealos-amd64文件会报错：\n`sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.38' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.33' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.34' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.32' not found (required by sealos2)`\n\n### If you have solution，please describe it\n\n我先用你们提供的方式试试\nBXY4543:\nMakefile里边的 make\n\nBXY4543:\nOptions:\n  DEBUG            Whether or not to generate debug symbols. Default is 0.\n\n  BINS             Binaries to build. Default is all binaries under cmd.\n                   This option is available when using: make {build}(.multiarch)\n                   Example: make build BINS=\"sealos sealctl\"\n\n  PLATFORMS        Platform to build for. Default is linux_arm64 and linux_amd64.\n                   This option is available when using: make {build}.multiarch\n                   Example: make build.multiarch PLATFORMS=\"linux_arm64 linux_amd64\"\n\n  V                Set to 1 enable verbose build. Default is 0.\n\nBXY4543:\nmake build BINS=\"sealos\" PLATFORMS=\"linux_arm64 linux_amd64\"\n\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-19T16:30:49Z",
        "number": 5384,
        "state": "CLOSED",
        "title": "The system provides a non-root-run version of sealos",
        "url": "https://github.com/labring/sealos/issues/5384",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. k8s==1.28.12\n2. 当我禁止anonymous-auth时，master与node节点notready。   查看node上的lvscare服务，显示连不上api-server\nvim etc/kubernetes/manifests/kube-apiserver.yaml\n - --anonymous-auth=false\n\n3. curl api-server.local:6443\n返回401 unauthorized\n\n### What is the expected behavior?\n\n希望能够正常禁止匿名登录\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.1\n- Docker version: 26.1.4\n- Kubernetes version: 1.28.12\n- Operating system: ubuntu 24.04\n- Runtime environment: ubuntu 24.04\n- Cluster size: 3 master 3 node\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-16T20:50:58Z",
        "number": 5377,
        "state": "CLOSED",
        "title": "BUG: 禁止匿名登录，anonymous-auth=false，节点notready",
        "url": "https://github.com/labring/sealos/issues/5377",
        "login": "wscjxky",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n# 概述\n- 感觉主要是安装参数` --master-ips=172.19.117.70,172.19.117.71,172.19.117.72 `导致的，使用单节点方式，没有该问题，可以正常手动添加\n- 在ubuntu 24.04上应该是必现问题，在阿里云和本地都能复现\n\n\n# 安装命令\n```\nbash install.sh  --zh   --cloud-version=v5.0.1  --image-registry=registry.cn-shanghai.aliyuncs.com  --proxy-prefix=https://mirror.ghproxy.com  --pod-cidr=100.64.0.0/10  --service-cidr=10.96.0.0/22  --cloud-port=443  --ssh-private-key=/root/.ssh/id_rsa  --kubernetes-version=1.28.11  --master-ips=172.19.117.70,172.19.117.71,172.19.117.72  --cloud-domain=sealos.mydomain.com\n```\n\n# 操作系统\nroot@iZuf6cb62qby5xh37bhzv6Z:~/sealos_5.0.1# cat /etc/os-release \nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n\n\n\n![Image](https://github.com/user-attachments/assets/2c23128d-f072-4e89-b39a-9d1c6bb59735)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-16T04:07:14Z",
        "number": 5376,
        "state": "CLOSED",
        "title": "BUG: Error: client rate limiter Wait returned an error: context deadline exceeded",
        "url": "https://github.com/labring/sealos/issues/5376",
        "login": "steinvenic",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n脚本不太好用，能做到1panel面板那样的安装就好了\n\n### What is the current documentation state?\n\nsealos 集群部署 这一块看不懂\n\n### What is your suggestion?\n\n加入哔哩哔哩的安装视频\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T10:30:06Z",
        "number": 5375,
        "state": "CLOSED",
        "title": "= Is it OK to deploy sealos for a video version? Now these look a little dizzy😵‍💫",
        "url": "https://github.com/labring/sealos/issues/5375",
        "login": "docker-master",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nplugin download\n\n### What is the current documentation state?\n\nhttps://hzh.sealos.run/?bd_vid=&k=&s=\nI can't open the online project from this location using curser, it tells me. \nCursor's Devbox is often not the latest. If there are any issues, please manually install the plugin referenced this URI.\n\nMicrosoft's Plugin Store does not have a download option for devbox. \nWhere can I download the plugin devbox?\n\n<img width=\"1136\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8f2bbda7-aad7-4cbf-8e42-1f216c182769\" />\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-26T00:19:42Z",
        "number": 5372,
        "state": "CLOSED",
        "title": "Docs: devbox plugin file , how  to download it?",
        "url": "https://github.com/labring/sealos/issues/5372",
        "login": "jemerci",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-05-24T20:04:09Z",
        "number": 5358,
        "state": "CLOSED",
        "title": "the ` tailscale-options` envoy filter dosn't pass headscale's `Upgrade: tailscale-control-protocol` to backend",
        "url": "https://github.com/labring/sealos/issues/5358",
        "login": "lin-calvin",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. 先通过sealos run 安装的master1及node1, 正常运行。\n```\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.30.5 \\\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 \\\nregistry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \\\n--masters 10.100.0.1 \\\n--nodes 10.100.0.11 \\\n--user root \\\n--passwd abcd1234\n```\n2. 在node1节点上一个pod里面也可正常访问网络。\n```\nroot@node1:~# kubectl exec -it 678a3ac94db887506c4d9fa0  -n xd -- /bin/sh\nDefaulted container \"app\" out of: app, lifecycle-sidecar\n/app # ping www.baidu.com\nPING www.baidu.com (110.242.68.3): 56 data bytes\n64 bytes from 110.242.68.3: seq=0 ttl=43 time=13.281 ms\n64 bytes from 110.242.68.3: seq=1 ttl=43 time=12.461 ms\n64 bytes from 110.242.68.3: seq=2 ttl=43 time=12.353 ms\n64 bytes from 110.242.68.3: seq=3 ttl=43 time=12.365 ms\n^Z[1]+  Stopped                    ping www.baidu.com\n/app # exit\nYou have stopped jobs.\n/app # exit\nroot@node1:~# ping www.baidu.com\nPING www.a.shifen.com (110.242.69.21) 56(84) bytes of data.\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=1 ttl=45 time=13.0 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=2 ttl=45 time=12.7 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=3 ttl=45 time=12.5 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=4 ttl=45 time=12.5 ms\n^Z\n[1]+  Stopped                 ping www.baidu.com\nroot@node1:~# \n```\n\n问题：然后通过`sealos add --nodes 10.100.0.13` 添加新的节点，发现里面创建的pod不能访问网络, 但节点本身可访问网络。\n```\nroot@node3:~# kubectl exec -it 6788b47bd5b570fe47ef8b0e  -n xd -- /bin/sh\nDefaulted container \"app\" out of: app, lifecycle-sidecar\n/app # ping www.baidu.com\n^Z[1]+  Stopped                    ping www.baidu.com\n/app # exit\nYou have stopped jobs.\n/app # exit\nroot@node3:~# \nroot@node3:~# ping www.baidu.com\nPING www.a.shifen.com (180.101.50.242) 56(84) bytes of data.\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=1 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=2 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=3 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=4 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=5 ttl=47 time=11.3 ms\n^Z\n[5]+  Stopped                 ping www.baidu.com\nroot@node3:~# \n```\n\n以下是pod信息：\n```\nroot@master1:~# kubectl get pods -o wide -A\nNAMESPACE     NAME                               READY   STATUS    RESTARTS          AGE     IP            NODE      NOMINATED NODE   READINESS GATES\nkube-system   cilium-2s2h4                       1/1     Running   4 (5d10h ago)     5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   cilium-6g7lm                       1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   cilium-djjgr                       1/1     Running   0                 16h     10.100.0.12   node2     <none>           <none>\nkube-system   cilium-dtcmn                       1/1     Running   0                 16h     10.100.0.13   node3     <none>           <none>\nkube-system   cilium-operator-5dccd84bff-f7jhc   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   coredns-55cb58b774-4x4xz           1/1     Running   0                 5d10h   10.0.0.215    node1     <none>           <none>\nkube-system   coredns-55cb58b774-f558l           1/1     Running   0                 5d10h   10.0.0.106    node1     <none>           <none>\nkube-system   etcd-master1                       1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-apiserver-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-controller-manager-master1    1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-proxy-284zd                   1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>\nkube-system   kube-proxy-gmtkj                   1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-proxy-n688p                   1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>\nkube-system   kube-proxy-zkcl7                   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   kube-scheduler-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-sealos-lvscare-node1          1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   kube-sealos-lvscare-node2          1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>\nkube-system   kube-sealos-lvscare-node3          1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>\nxd          6788a6ead5b570fe47ef8b0d           2/2     Running   10 (2m35s ago)    18h     10.0.0.11     node1     <none>           <none>\nxd          6788b47bd5b570fe47ef8b0e           2/2     Running   2 (8h ago)        16h     10.0.3.78     node3     <none>           <none>\nroot@master1:~# \n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-21T21:00:21Z",
        "number": 5355,
        "state": "CLOSED",
        "title": "sealos add can not visit network",
        "url": "https://github.com/labring/sealos/issues/5355",
        "login": "allran",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-10T03:17:01Z",
        "number": 5332,
        "state": "CLOSED",
        "title": "BUG: frontend devbox ingress host is not correct",
        "url": "https://github.com/labring/sealos/issues/5332",
        "login": "liseri",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nSealos Version: 5.0.1\r\n\r\n### How to reproduce the bug?\r\n\r\n1. In this environment:\r\n   - Sealos 5.0.1 version installed.\r\n   - Cluster consists of 5 nodes:\r\n     - Master nodes: 192.168.37.10, 192.168.37.11, 192.168.37.12\r\n     - Worker nodes: 192.168.37.13, 192.168.37.14\r\n   - Using default Sealos configurations for cluster setup.\r\n\r\n2. With this config:\r\n   - Default configuration with Kubernetes setup and basic network settings.\r\n   - No custom modifications made to the Sealos configuration.\r\n\r\n3. Run:\r\n   Run `sealos reset --nodes 192.168.37.13` to reset only the node `192.168.37.13`.\r\n\r\n4. See error:\r\n   Instead of resetting just the node `192.168.37.13`, the command resets the entire cluster, including master and other worker nodes. This behavior is not expected, as I only intended to reset the specified node.\r\n\r\n### What is the expected behavior?\r\n\r\nI expect that when running `sealos reset --nodes 192.168.37.13`, only the node `192.168.37.13` should be reset, leaving the rest of the cluster (master and other worker nodes) intact.\r\n\r\n### What do you see instead?\r\n\r\nInstead of resetting just the specified node `192.168.37.13`, the entire cluster gets reset, including the master and other worker nodes, which is not the intended behavior.\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: 5.0.1\r\n- Docker version:\r\n- Kubernetes version: 1.28.10\r\n- Operating system: Ubuntu 22.04\r\n- Runtime environment: Physical machine (16G memory, 8 core CPU, 200GB storage)\r\n- Cluster size: 3 master, 2 node\r\n- Additional information: No additional services like Istio or Dashboard enabled.\r\n```\r\n\r\n\r\n### Additional information\r\n- The issue occurred when running the `sealos reset --nodes` command, where the reset command affected the entire cluster instead of the specified node.\r\n- Cluster is using default Sealos configuration with Kubernetes setup.\r\n- No custom configurations were applied to Sealos or Kubernetes.",
        "closed": true,
        "closedAt": "2025-04-23T16:06:14Z",
        "number": 5303,
        "state": "CLOSED",
        "title": "BUG: sealos reset Command Resets Entire Cluster Instead of Specified Node",
        "url": "https://github.com/labring/sealos/issues/5303",
        "login": "villain6666",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nError when upgrade kuberenetes from v1.29.7 to v1.30.5\r\ncan not mix '--config' with arguments [certificate-renewal]\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.0\r\n- Containerd version: 1.6.28\r\n- Kubernetes version: v1.29.7\r\n- Operating system: ubuntu 22.04\r\n- Runtime environment: containerd 1.6.28\r\n- Cluster size: 3 master\r\n- Additional information: none\n```\n\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5283,
        "state": "OPEN",
        "title": "BUG: error when upgrade kuberenetes from v1.29.7 to v1.30.5, can not mix '--config' with arguments [certificate-renewal]",
        "url": "https://github.com/labring/sealos/issues/5283",
        "login": "Ironeie",
        "is_bot": false
      },
      {
        "body": "\n## Description\n\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\n\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n\n### What To Do?\n\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n\n**For further information, suggestions, and guidance:**\n- 📖 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- 💬 Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\n\nWe sincerely apologize for any inconvenience this may cause.\n\n**Thank you for your cooperation and understanding!** :pray:\n",
        "closed": false,
        "closedAt": null,
        "number": 5250,
        "state": "OPEN",
        "title": ":warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy",
        "url": "https://github.com/labring/sealos/issues/5250",
        "login": "camilamacedo86",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nsealos run labring/kubernetes:v1.31.0 labring/helm:v3.12.0 labring/calico:v3.24.6 --masters xxx, return error \r\n2024-11-23T17:33:21 info start to init master0...\r\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\r\nTo see the stack trace of this error execute with --v=5 or higher\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-26T18:31:15Z",
        "number": 5231,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/5231",
        "login": "qikaih",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. install sealos tool v5.0.1\r\n2. install k8s use:\r\n```\r\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \\\r\n     --masters 172.17.132.44 \\\r\n     --nodes 172.17.132.43 -p xxxxxxx\r\n\r\n```\r\n3. install sealos use:\r\n```\r\n$ curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\r\n  --cloud-version=v5.0.1 \\\r\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\r\n  --proxy-prefix=https://mirror.ghproxy.com\r\n```\r\n4. installation failed, stuck at the “run launchpad monitoring” step. log is:\r\n[root@hkserver-44~.sealoslogs.txt](https://github.com/user-attachments/files/17797950/root%40hkserver-44.sealoslogs.txt)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.1\r\n- Docker version: no\r\n- Kubernetes version:1.28.11\r\n- Operating system: centos8\r\n- Runtime environment: glibc 2.28; kernel 5.4\r\n- Cluster size: 2(Time synched)\r\n- Additional information:\r\n\r\n[root@hkserver-44 logs]# kubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS              RESTARTS      AGE\r\naccount-system              account-controller-manager-6fd48875b8-627mg                       2/2     Running             1 (14m ago)   15m\r\naccount-system              account-service-5f94b5f779-7ww8r                                  1/1     Running             0             15m\r\naccount-system              init-job-5kqgb                                                    0/1     Completed           1             15m\r\naccount-system              license-controller-manager-54fdd5d6f7-ckg9j                       2/2     Running             0             15m\r\napp-system                  app-controller-manager-fb7b86fb8-b7cbr                            2/2     Running             0             16m\r\napplaunchpad-frontend       applaunchpad-frontend-55b94f448f-rn6kf                            1/1     Running             0             14m\r\ncert-manager                cert-manager-656b9496d4-qjm7b                                     1/1     Running             0             24m\r\ncert-manager                cert-manager-cainjector-766bf6f44c-6vbf2                          1/1     Running             0             24m\r\ncert-manager                cert-manager-webhook-66b5cd7597-4rjwf                             1/1     Running             0             24m\r\ncockroach-operator-system   cockroach-operator-manager-77cdd79458-lxshg                       1/1     Running             0             23m\r\ncostcenter-frontend         costcenter-frontend-6fd7c894f-cwdn4                               1/1     Running             0             13m\r\ncronjob-frontend            cronjob-frontend-d9cbb67b4-stpjc                                  1/1     Running             0             12m\r\ndbprovider-frontend         dbprovider-frontend-b4fd55fb7-f68cg                               1/1     Running             0             13m\r\nhigress-system              higress-controller-6766c9486f-kj47d                               0/2     Pending             0             22m\r\nhigress-system              higress-gateway-8cr6w                                             0/1     ContainerCreating   0             22m\r\nhigress-system              higress-gateway-lznr7                                             0/1     ContainerCreating   0             22m\r\nkb-system                   csi-attacher-s3-0                                                 1/1     Running             0             19m\r\nkb-system                   csi-provisioner-s3-0                                              2/2     Running             0             19m\r\nkb-system                   csi-s3-jm7sg                                                      2/2     Running             0             19m\r\nkb-system                   csi-s3-r7r2p                                                      2/2     Running             0             19m\r\nkb-system                   kb-addon-migration-dt-platform-548c895976-xltfd                   0/1     ImagePullBackOff    0             19m\r\nkb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-npgvv                     1/1     Running             0             19m\r\nkb-system                   kubeblocks-85ddddddd4-rblm9                                       1/1     Running             0             21m\r\nkb-system                   kubeblocks-dataprotection-7f9c76fb8f-79nxp                        1/1     Running             0             21m\r\nkube-system                 cilium-dtfkj                                                      1/1     Running             0             142m\r\nkube-system                 cilium-operator-5448c894cc-xcwp9                                  1/1     Running             0             142m\r\nkube-system                 cilium-zrrtj                                                      1/1     Running             0             142m\r\nkube-system                 coredns-5dd5756b68-p5lh2                                          1/1     Running             0             143m\r\nkube-system                 coredns-5dd5756b68-rtpnp                                          1/1     Running             0             143m\r\nkube-system                 etcd-hkserver-44                                                  1/1     Running             4             143m\r\nkube-system                 kube-apiserver-hkserver-44                                        1/1     Running             4             143m\r\nkube-system                 kube-controller-manager-hkserver-44                               1/1     Running             11            143m\r\nkube-system                 kube-proxy-8mxl7                                                  1/1     Running             0             143m\r\nkube-system                 kube-proxy-lw465                                                  1/1     Running             0             142m\r\nkube-system                 kube-scheduler-hkserver-44                                        1/1     Running             12            143m\r\nkube-system                 kube-sealos-lvscare-hkserver-43                                   1/1     Running             3             142m\r\nkube-system                 metrics-server-68f967f7dc-j76nc                                   1/1     Running             0             23m\r\nkuboard                     kuboard-v3-hkserver-44                                            1/1     Running             0             43m\r\nlicense-frontend            license-frontend-597c57bfc9-nzxb5                                 1/1     Running             0             13m\r\nopenebs                     openebs-localpv-provisioner-56d6489bbc-zdgpb                      1/1     Running             0             23m\r\nresources-system            resources-controller-manager-77fdd6fdd4-r4vn4                     2/2     Running             0             15m\r\nsealos                      database-monitor-deployment-6b964bd695-wm42h                      1/1     Running             0             12m\r\nsealos                      desktop-frontend-59958dd9df-gktp4                                 1/1     Running             0             16m\r\nsealos                      launchpad-monitor-deployment-84fdd6884f-q9mpr                     1/1     Running             0             12m\r\nsealos                      sealos-cockroachdb-0                                              1/1     Running             0             19m\r\nsealos                      sealos-cockroachdb-1                                              1/1     Running             0             19m\r\nsealos                      sealos-cockroachdb-2                                              1/1     Running             0             19m\r\nsealos                      sealos-mongodb-mongodb-0                                          3/3     Running             0             19m\r\ntemplate-frontend           template-frontend-df96f944d-97llk                                 1/1     Running             0             13m\r\nterminal-frontend           terminal-frontend-7969b49c99-5drs5                                1/1     Running             0             14m\r\nterminal-system             terminal-controller-manager-79fc658d89-qzm4k                      2/2     Running             0             16m\r\nuser-system                 user-controller-manager-658d4445bb-dqxcn                          2/2     Running             0             16m\r\nvm                          victoria-metrics-k8s-stack-grafana-76cfcccd7f-7tsgg               3/3     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-tbbmj    1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-p6794         1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-rzd6l         1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bch94f   1/1     Running             0             22m\r\nvm                          vmagent-victoria-metrics-k8s-stack-69857ff7c7-458hw               2/2     Running             0             19m\r\nvm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-s6sx7               2/2     Running             0             22m\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running             0             22m\r\nvm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-dxrls               1/1     Running             0             22m\r\n```\n```\n\n\n### Additional information\n\nhow to debug/log/resolve it;",
        "closed": true,
        "closedAt": "2025-01-07T10:41:48Z",
        "number": 5213,
        "state": "CLOSED",
        "title": "Sealos v5.0.1 self-hosted installation failed, stuck at the “run launchpad monitoring” step.",
        "url": "https://github.com/labring/sealos/issues/5213",
        "login": "liseri",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n怎么把本地已存在的代码项目关联到远程刚创建的Devboox项目下面，这部分的流程文档说明\n\n### What is the current documentation state?\n\n去到文档链接部分未看到这部分的文档输出\n\n### What is your suggestion?\n\n补充一下怎么把本地已存在的代码项目关联到远程刚创建的Devboox项目下面，这部分的流程文档说明\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-16T16:10:07Z",
        "number": 5211,
        "state": "CLOSED",
        "title": "About the local existing code associated with the remotely created Devbox project",
        "url": "https://github.com/labring/sealos/issues/5211",
        "login": "cfy3133",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n网页版的\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\nvue应用连接不上cursor\r\n\n\n### What do you see instead?\n\n一直显示 ‘’设置 SSH 主机 gzg.sealos.run-ns-d1l8go8n-devboxreact-scxvqiz699: 正在下载 VS Code 服务器‘’  一直转圈圈连不上\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nnode.js是可以连接上",
        "closed": true,
        "closedAt": "2025-03-10T04:46:03Z",
        "number": 5201,
        "state": "CLOSED",
        "title": "vue application cannot connect to cursor",
        "url": "https://github.com/labring/sealos/issues/5201",
        "login": "wenbo-quan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n当前版本\n\n### How to reproduce the bug?\n\n新加坡区云开发id：pxyyxos06z\r\n无法启动，无法停止，导致无法删除。\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-10T04:46:02Z",
        "number": 5200,
        "state": "CLOSED",
        "title": "Singapore cloud development ID: pxyyxos06z cannot be started, cannot be stopped, and cannot be deleted.",
        "url": "https://github.com/labring/sealos/issues/5200",
        "login": "imococo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0\n\n### How to reproduce the bug?\n\nProblems encountered during make build\n\n### What is the expected behavior?\n\n===========> Building binary lvscare v5.0.1 for linux_amd64\r\n===========> Building binary sealctl v5.0.1 for linux_amd64\r\n# github.com/labring/sealos/cmd/sealctl\r\n/usr/local/go/pkg/tool/linux_amd64/link: running x86_64-linux-gnu-gcc failed: exit status 1\r\n/usr/bin/x86_64-linux-gnu-gcc -m64 -s -Wl,-z,now -Wl,-z,nocopyreloc -o $WORK/b001/exe/a.out -rdynamic /tmp/go-link-3363692486/go.o /tmp/go-link-3363692486/000000.o /tmp/go-link-3363692486/000001.o /tmp/go-link-3363692486/000002.o /tmp/go-link-3363692486/000003.o /tmp/go-link-3363692486/000004.o /tmp/go-link-3363692486/000005.o /tmp/go-link-3363692486/000006.o /tmp/go-link-3363692486/000007.o /tmp/go-link-3363692486/000008.o /tmp/go-link-3363692486/000009.o /tmp/go-link-3363692486/000010.o /tmp/go-link-3363692486/000011.o /tmp/go-link-3363692486/000012.o /tmp/go-link-3363692486/000013.o /tmp/go-link-3363692486/000014.o /tmp/go-link-3363692486/000015.o /tmp/go-link-3363692486/000016.o /tmp/go-link-3363692486/000017.o /tmp/go-link-3363692486/000018.o /tmp/go-link-3363692486/000019.o /tmp/go-link-3363692486/000020.o /tmp/go-link-3363692486/000021.o /tmp/go-link-3363692486/000022.o /tmp/go-link-3363692486/000023.o /tmp/go-link-3363692486/000024.o /tmp/go-link-3363692486/000025.o /tmp/go-link-3363692486/000026.o /tmp/go-link-3363692486/000027.o -O2 -g -O2 -g -lpthread -O2 -g -ldl -O2 -g -O2 -g -O2 -g -ldl\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crt1.o: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crti.o: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -lpthread: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -lc: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crtn.o: No such file or directory\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[1]: *** [scripts/make-rules/golang.mk:60: go.build.linux_amd64.sealctl] Error 1\r\nmake: *** [Makefile:58: build] Error 2\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-01T03:51:35Z",
        "number": 5185,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/5185",
        "login": "AutuSnow",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1     \r\n\r\ncilium  1.16.0 or 1.16.1  \r\n\r\n```\r\nℹ️  Using Cilium version 1.16.0\r\n Auto-detected cluster name: kubernetes\r\n Auto-detected kube-proxy has been installed\r\n\r\nError: Unable to install Cilium: execution error at (cilium/templates/cilium-configmap.yaml:71:5): kubeProxyReplacement must be explicitly set to a valid value (true or false) to continue.\r\n\r\n```\r\n\n\n### What is the expected behavior?\n\nWhere do I need to specify kubeProxyReplacement\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-03T05:08:48Z",
        "number": 5184,
        "state": "CLOSED",
        "title": "install Cilium version 1.16.0  error",
        "url": "https://github.com/labring/sealos/issues/5184",
        "login": "dxygit1",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-01T01:51:34Z",
        "number": 5183,
        "state": "CLOSED",
        "title": "I want to independently deploy etcd to a host outside the cluster. Does sealos support offline deployment of etcd?",
        "url": "https://github.com/labring/sealos/issues/5183",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n2024-10-29T22:13:45 debug sync workdir: /root/.sealos/default\r\n2024-10-29T22:13:45 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 10.4.0.95:22 locally\r\nError: could not get local addresses: route ip+net: no such network interface\r\n\r\n```\r\nfunc New(inner ssh.Interface) (Interface, error) {\r\n\taddr, err := net.InterfaceAddrs()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"could not get local addresses: %v\", err)\r\n\t}\r\n\treturn &wrap{\r\n\t\tinner:          inner,\r\n\t\tlocalAddresses: sets.Set[string](netutil.AddressSet(isValid, addr)),\r\n\t}, nil\r\n}\r\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:56Z",
        "number": 5180,
        "state": "CLOSED",
        "title": "BUG: can't get local addresses in single node",
        "url": "https://github.com/labring/sealos/issues/5180",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nsealos run  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17   ----\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\r\n[kubelet-check] Initial timeout of 40s passed.\r\n\r\n        Unfortunately, an error has occurred:\r\n                timed out waiting for the condition\r\n\r\n        This error is likely caused by:\r\n                - The kubelet is not running\r\n                - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\r\n\r\n        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\r\n                - 'systemctl status kubelet'\r\n                - 'journalctl -xeu kubelet'\r\n\r\n        Additionally, a control plane component may have crashed or exited when started by the container runtime.\r\n        To troubleshoot, list all containers using your preferred container runtimes CLI.\r\n\r\n        Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl:\r\n                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'\r\n                Once you have found the failing container, you can inspect its logs with:\r\n                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'\r\n\r\nerror execution phase wait-control-plane: couldn't initialize a Kubernetes cluster\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-10-29T12:21:13 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\nError: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:56Z",
        "number": 5179,
        "state": "CLOSED",
        "title": "registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17  install error",
        "url": "https://github.com/labring/sealos/issues/5179",
        "login": "biao-lvwan",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n当前应用商店模板过少，很多地方不能够自定义，部分在实际使用时还会有bug，并且应用管理当中创建应用的功能太不灵活了。\n\n### If you have solution，please describe it\n\n读取网络文件，或者以yaml编辑器的形式传入写好的文件，和正常应用商店里面的模板一样被部署，如果有安全考虑的话，可以不支持用户看到其他用户的自定义模板，不然的话，我觉得在用户本人允许的前提下，可以开放让其他用户获取到第三方模板。这样子对很多sealos的潜在用户而言，可以解决很多痛点问题，同时对于很多有定制化需求的小团队也可以更灵活的使用。\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:57Z",
        "number": 5176,
        "state": "CLOSED",
        "title": "Does the official intend to develop third-party or custom application template deployment?",
        "url": "https://github.com/labring/sealos/issues/5176",
        "login": "VocabVictor",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nNone\n\n### How to reproduce the bug?\n\nI tried to add a custom domain name to my project today, but I found that it could not be added successfully, even though I had parsed it 6 hours later, as shown in the picture:\r\n\r\n![pic](https://github.com/user-attachments/assets/f32289f8-6272-49b6-9342-684d166e22a2)\r\n\r\nI have also seen many users reporting this problem. Is it possible to try without verifying the CNAME? My parsing is on CloudFlare.\r\n\r\n![image](https://github.com/user-attachments/assets/25e9ebee-f426-4d6c-a64c-9efcf6e8e98d)\n\n### What is the expected behavior?\n\ni hope to improve\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-18T13:08:46Z",
        "number": 5155,
        "state": "CLOSED",
        "title": "BUG: Problem adding custom domain name to sealos.io",
        "url": "https://github.com/labring/sealos/issues/5155",
        "login": "alltobebetter",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nDisable the verification of domain name resolution when adding custom domains\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-26T03:33:15Z",
        "number": 5152,
        "state": "CLOSED",
        "title": "How to disable verification for adding custom domain names",
        "url": "https://github.com/labring/sealos/issues/5152",
        "login": "clstech",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nrootfs：\r\n![QQ_1727251867263](https://github.com/user-attachments/assets/3cf27fbd-30d1-4e72-bdeb-3a4847168e08)\r\n\n\n### What is the expected behavior?\n\nsealos run 自己打包的镜像，将cni ingress-nginx 通过helm的方式进行install安装，sealos run 镜像:v1.0.1 --masters 192.10.100.1 能够正常运行整个集群，网络组件 ingress 正常就绪，但是我通过sealos run 镜像:v1.0.1 --masters  192.10.100.1  --nodes 192.10.100.2 就会报错 Error: master not allow empty\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-27T05:14:44Z",
        "number": 5107,
        "state": "CLOSED",
        "title": "sealos add node Error: master not allow empty",
        "url": "https://github.com/labring/sealos/issues/5107",
        "login": "sharkat",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n![QQ_1727251467658](https://github.com/user-attachments/assets/2812456b-11fd-4af8-b4d8-8cc4a7694723)\r\n![QQ_1727251490677](https://github.com/user-attachments/assets/82097b4b-280f-4ce5-a903-670af50825fd)\r\n![QQ_1727251530883](https://github.com/user-attachments/assets/55699506-b733-4760-993b-7c38a632e726)\r\n![QQ_1727251563595](https://github.com/user-attachments/assets/b1921db2-9517-4dbe-b8fa-b28185ae4a09)\r\n![QQ_1727251595720](https://github.com/user-attachments/assets/e69c02af-7b8a-430d-8832-8bbcd2efb3af)\r\n\n\n### What is the expected behavior?\n\nsealos run images:v0.0.1 执行相关脚本通过helm install中间件 调用检查脚本 等待相关pod就绪，手动执行脚本没问题，通过run执行到中途Error: signal: killed 就报错了。\n\n### What do you see instead?\n\nError: signal: killed \n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.0\r\n- Docker version:\r\n- Kubernetes version:1.27.7\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T09:52:41Z",
        "number": 5106,
        "state": "CLOSED",
        "title": "sealos run Error: signal: killed",
        "url": "https://github.com/labring/sealos/issues/5106",
        "login": "sharkat",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0\r\n\r\n### How to reproduce the bug?\r\n\r\n```\r\nsealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\n```\r\n\r\n### What is the expected behavior?\r\n\r\nno warn message\r\n\r\n### What do you see instead?\r\n\r\nget warn message\r\n\r\n```\r\n2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip\r\n```\r\n```\r\nroot@ubuntu:/data/cluster-image/kubernetes-nmstate/latest# sealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\n2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip\r\nGetting image source signatures\r\nCopying blob 52b6e7f862c8 done  \r\nCopying blob 68f4756818ef done  \r\nCopying config 6a28855840 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\nGetting image source signatures\r\nCopying blob 5a8fd2560a4c done  \r\nCopying blob eac1b95df832 done  \r\nCopying blob e5f565e86761 done  \r\nCopying blob d382f544b01f done  \r\nCopying blob 47aa3ed2034c done  \r\nCopying blob 7c282516a5e0 done  \r\nCopying config 19fe8540f8 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\nGetting image source signatures\r\nCopying blob e500b42a2bb5 skipped: already exists  \r\nCopying blob 2895d6faeea8 skipped: already exists  \r\nCopying config 4268cf1312 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\n2024-09-14T17:56:03 info saving images quay.io/nmstate/kubernetes-nmstate-handler:v0.82.0, quay.io/openshift/origin-kube-rbac-proxy:4.10.0, quay.io/nmstate/kubernetes-nmstate-operator:v0.82.0\r\nSTEP 1/3: FROM scratch\r\nSTEP 2/3: COPY . .\r\nSTEP 3/3: CMD [\"bash entrypoint.sh\"]\r\nCOMMIT registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\nGetting image source signatures\r\nCopying blob 46100663e36e done  \r\nCopying config b90302488f done  \r\nWriting manifest to image destination\r\nStoring signatures\r\n--> b90302488f6a\r\nSuccessfully tagged registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\nb90302488f6abf1e33910c9b8638e56260e3418a1c166ebe1a8f85324022b232\r\n```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2025-01-14T03:58:24Z",
        "number": 5083,
        "state": "CLOSED",
        "title": "BUG:  warn image tar config is not exists,skip",
        "url": "https://github.com/labring/sealos/issues/5083",
        "login": "weironz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n2. ip change\r\n3. sealos reset --force\r\n4. rm -rf /usr/bin/sealos\r\n5. rm -rf /root/.sealos\r\n6. rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd /root/.kube\r\n7. sudo apt-get remove --purge docker-ce docker-ce-cli containerd.io \r\n8. sudo rm -rf /var/lib/docker /var/lib/containerd\r\n9. rm find / -name \"*sealos*\" 2>/dev/null\r\n10. sudo reboot\r\n11. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n12. ERROR: No such file or directoryscripts/init.sh:line 18:etc/sealos/.env \n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nI thought etc/sealos/.env was created by the script, but during execution it showed that there was no such file.",
        "closed": true,
        "closedAt": "2024-09-18T08:54:09Z",
        "number": 5065,
        "state": "CLOSED",
        "title": "etc/sealos/.env: No such file or directory",
        "url": "https://github.com/labring/sealos/issues/5065",
        "login": "Qiuqi777",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1. sealos reset\n\n### What is the expected behavior?\n\n[root@master0 deployment]# sealos reset --force --masters=xx.xx.xx.xx\r\n2024-09-09T14:08:17 info start to delete Cluster: master [10.105.5.233], node []\r\n2024-09-09T14:08:17 info start to reset nodes: []\r\n2024-09-09T14:08:17 info start to reset masters: [10.105.5.233:22]\r\n2024-09-09T14:08:17 info start to reset node: 10.105.5.233:22\r\n/usr/bin/kubeadm\r\n[preflight] Running pre-flight checks\r\nW0909 14:08:17.401140   12652 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\r\n[reset] Deleted contents of the etcd data directory: /var/lib/etcd\r\n[reset] Stopping the kubelet service\r\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\r\n[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\r\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\r\n\r\nThe reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\r\n\r\nThe reset process does not reset or clean up iptables rules or IPVS tables.\r\nIf you wish to reset iptables, you must do so manually by using the \"iptables\" command.\r\n\r\nIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\r\nto reset your system's IPVS tables.\r\n\r\nThe reset process does not clean your kubeconfig files and you must remove them manually.\r\nPlease, check the contents of the $HOME/.kube/config file.\r\n2024-09-09T14:08:17 info Executing pipeline Bootstrap in DeleteProcessor\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x359c0c6]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/labring/sealos/pkg/bootstrap.NewContextFrom(0xc000614240)\r\n\tgithub.com/labring/sealos/pkg/bootstrap/context.go:71 +0x2c6\r\ngithub.com/labring/sealos/pkg/bootstrap.New(0x0?)\r\n\tgithub.com/labring/sealos/pkg/bootstrap/bootstrap.go:49 +0x1d\r\ngithub.com/labring/sealos/pkg/apply/processor.(*DeleteProcessor).UndoBootstrap(0xc00059f3e0, 0xc000614240)\r\n\tgithub.com/labring/sealos/pkg/apply/processor/delete.go:83 +0x28c\r\ngithub.com/labring/sealos/pkg/apply/processor.DeleteProcessor.Execute({{0x44c6e10?, 0xc000b47350?}, {0x44bae80?, 0xc000627ba0?}}, 0xc000627ba0?)\r\n\tgithub.com/labring/sealos/pkg/apply/processor/delete.go:50 +0x35c\r\ngithub.com/labring/sealos/pkg/apply/applydrivers.(*Applier).deleteCluster(0xc000496820)\r\n\tgithub.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:265 +0x53\r\ngithub.com/labring/sealos/pkg/apply/applydrivers.(*Applier).Delete(0xc000496820)\r\n\tgithub.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:256 +0xe7\r\ngithub.com/labring/sealos/cmd/sealos/cmd.newResetCmd.func1(0xc000522c00?, {0xc00059e300?, 0x2?, 0x2?})\r\n\tgithub.com/labring/sealos/cmd/sealos/cmd/reset.go:54 +0xa3\r\ngithub.com/spf13/cobra.(*Command).execute(0xc000522c00, {0xc00059e2e0, 0x2, 0x2})\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:940 +0x862\r\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x5c6d560)\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:1068 +0x3bd\r\ngithub.com/spf13/cobra.(*Command).Execute(...)\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:992\r\ngithub.com/labring/sealos/cmd/sealos/cmd.Execute()\r\n\tgithub.com/labring/sealos/cmd/sealos/cmd/root.go:49 +0x25\r\nmain.main()\r\n\tgithub.com/labring/sealos/cmd/sealos/main.go:27 +0x29\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:v5.0.0\r\n- Docker version:\r\n- Kubernetes version:1.27.7\r\n- Operating system:openeuler 22.03\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-01-09T18:58:19Z",
        "number": 5054,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug panic: runtime error: invalid memory address or nil pointer dereference",
        "url": "https://github.com/labring/sealos/issues/5054",
        "login": "gitnehc",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5\n\n### How to reproduce the bug?\n\n![image](https://github.com/user-attachments/assets/b2616a1e-a7d8-48bf-ac17-9f011cf3f322)\r\nFollow the helm installation steps on the official website: https://sealos.run/docs/self-hosting/lifecycle-management/operations/build-image/build-image-helm_charts, an error occurred during run\n\n### What is the expected behavior?\n\nHope nginx can be pulled up\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-11T06:00:21Z",
        "number": 5033,
        "state": "CLOSED",
        "title": "BUG: deploy nginx Error: can't apply application type images only since RootFS type image is not applied yet",
        "url": "https://github.com/labring/sealos/issues/5033",
        "login": "biao-lvwan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0\r\n\r\n### How to reproduce the bug?\r\n\r\nsealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.12            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1            registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8            --masters 10.10.16.5,10.10.16.7,10.10.16.8            --nodes 10.10.16.11,10.10.16.12,10.10.16.14            -i '/root/.ssh/id_rsa' --output ~/Clusterfile\r\n\r\n### What is the expected behavior?\r\n```\r\napiServer:\r\n  certSANs:\r\n  - 127.0.0.1\r\n  - apiserver.cluster.local\r\n  - 10.10.16.5\r\n  - 10.10.16.7\r\n  - 10.10.16.8\r\n ```\r\n\r\n### What do you see instead?\r\n\r\n```\r\napiServer:\r\n  certSANs:\r\n  - 127.0.0.1\r\n  - apiserver.cluster.local\r\n  - 10.103.97.2\r\n  - 10.10.16.5\r\n  - 10.10.16.7\r\n  - 10.10.16.8\r\n ```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v5.0.0\r\n- Docker version: v20.10.7\r\n- Kubernetes version: v1.28.12\r\n- Operating system: Rocky 9.4\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-09-11T06:01:59Z",
        "number": 5029,
        "state": "CLOSED",
        "title": "BUG: There is a fake ip:10.103.97.2 in the Clusterfile with sealos v5.0.0 gen",
        "url": "https://github.com/labring/sealos/issues/5029",
        "login": "snakeliwei",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nsealos reset\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n![img_v3_02e7_f295c4c1-12ee-405a-aecf-79ce047fe78g](https://github.com/user-attachments/assets/a8292b3a-7bbc-4081-accd-b783b657d627)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0\r\n- Kubernetes version: v1.27.11\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-22T17:19:02Z",
        "number": 5007,
        "state": "CLOSED",
        "title": "BUG: sealos reset nil pointer",
        "url": "https://github.com/labring/sealos/issues/5007",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIs it possible to support modifying kubelet root-dir during the installation of k8s cluster. In the file /usr/lib/systemd/system/kubelet.service. d/10-kubeadm. conf, add KUBELET_EXTRA_ARGS=\"-- root dir=/data/dock/kubelet\"\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T01:27:33Z",
        "number": 4997,
        "state": "CLOSED",
        "title": "Support modifying kubelet rootdir when installing k8s cluster",
        "url": "https://github.com/labring/sealos/issues/4997",
        "login": "lllicg",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nroot@malone:~# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\r\nFlag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node\r\n2024-08-23T11:47:20 info Start to create a new cluster: master [192.168.163.23], worker [], registry 192.168.163.23\r\n2024-08-23T11:47:20 info Executing pipeline Check in CreateProcessor.\r\n2024-08-23T11:47:20 info checker:hostname [192.168.163.23:22]\r\n2024-08-23T11:47:20 info checker:timeSync [192.168.163.23:22]\r\n2024-08-23T11:47:20 info checker:containerd [192.168.163.23:22]\r\n2024-08-23T11:47:20 info Executing pipeline PreProcess in CreateProcessor.\r\n2024-08-23T11:47:20 info Executing pipeline RunConfig in CreateProcessor.\r\n2024-08-23T11:47:20 info Executing pipeline MountRootfs in CreateProcessor.\r\n2024-08-23T11:47:26 info Executing pipeline MirrorRegistry in CreateProcessor.\r\n2024-08-23T11:47:26 info trying default http mode to sync images to hosts [192.168.163.23:22]\r\n2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:22829/coredns/coredns:v1.10.1: trying to reuse blob sha256:25b7032c281a433b92d09930f3a03c0f7382c27eb69ae7f35addf2e3853dbba7 at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway\r\n2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:11009/cilium/certgen:v0.1.8: copying image 1/2 from manifest list: trying to reuse blob sha256:0342af6c6dfc37396624a2296e5e6a737e1a61a8ca5d48ba9259a86270d4bc7e at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-08-23T12:37:59Z",
        "number": 4995,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4995",
        "login": "malone231214",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIs it possible to support k8s cluster deployments with mixed architectures (mixed x86 and arm).  I tried it and now it doesn't work. If it's not feasible, or if there's not much demand for it, forget it\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-01-14T03:58:23Z",
        "number": 4994,
        "state": "CLOSED",
        "title": "support install k8s cluster  with mixed architectures",
        "url": "https://github.com/labring/sealos/issues/4994",
        "login": "JACKCHEN99",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.1.3\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n账户登录问题。如何找回我的账户？",
        "closed": true,
        "closedAt": "2024-12-23T21:37:33Z",
        "number": 4985,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4985",
        "login": "TingqiaoXu",
        "is_bot": false
      },
      {
        "body": "centos7 sealos4.2.0 4.3.0都试过，部署单节点k8s，看起来是kubeadm config images pull 从本地仓库拉取kube-proxy的时候卡住很长时间，最后失败，但是其他的组件kube-apiserver，kube-controller-manager等可以正常下载。\r\n![1723733625084](https://github.com/user-attachments/assets/263f3c46-409d-4db3-8c74-f925a82003f0)\r\ndocker pull手动拉取kube-proxy也是在划线的地方卡了很久，但最后能下载成功。是否能设置超时时间，或者我手动拉取kube-proxy之后，还需要执行哪些脚本，才能部署完k8s？\r\n![1723734061116](https://github.com/user-attachments/assets/5dbc7c82-980b-4b4d-b33d-ac05311e0170)\r\n",
        "closed": true,
        "closedAt": "2024-12-18T05:27:34Z",
        "number": 4978,
        "state": "CLOSED",
        "title": "Failed to download image during offline deployment",
        "url": "https://github.com/labring/sealos/issues/4978",
        "login": "ttsdmmn",
        "is_bot": false
      },
      {
        "body": "centos7 sealos4.2.0 4.3.0都试过，看起来是kubeadm config images pull 从本地仓库拉取kube-proxy的时候卡住很长时间，最后失败，但是其他的组件kube-apiserver，kube-controller-manager等可以正常下载\r\n![1723733335960](https://github.com/user-attachments/assets/85163d56-d6d5-40a5-9960-2bce4e0ab333)\r\n\r\n",
        "closed": true,
        "closedAt": "2024-08-15T15:03:15Z",
        "number": 4977,
        "state": "CLOSED",
        "title": "Mirror pull failed during offline deployment",
        "url": "https://github.com/labring/sealos/issues/4977",
        "login": "ttsdmmn",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n1. centos 7 with kernel 5.6.14\r\n![8d813db3-2262-4c41-a36b-f9d213ee5764](https://github.com/user-attachments/assets/a54aabde-dea0-4d5b-bbf1-bcb24f5f4844)\r\n2. curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n![5e5797fa-cf0d-4967-af8b-257530a32b7f](https://github.com/user-attachments/assets/254f1aa3-c34d-445c-82ea-5f2cb9abc8c7)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-08-16T08:31:52Z",
        "number": 4966,
        "state": "CLOSED",
        "title": "centos7 with kernel 5.6.14  install and  failed to pull image \"k8s.gcr.io/pause:3.2\"",
        "url": "https://github.com/labring/sealos/issues/4966",
        "login": "zlwzlwzlw",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv4.3.5\r\n\r\n### How to reproduce the bug?\r\n\r\n1. create a sealos app for kubevirt's virtual machine\r\n2.  locally import the sealos image \r\n3.  execute 'sealos run ' command line for multiple times to create multiple VMs with different names\r\n\r\n### What is the expected behavior?\r\n\r\nall 'sealos run' command lines completed and all VMs should be created successfully\r\n\r\n### What do you see instead?\r\n\r\none of the sealos pods failed to complete, the details as below: \r\n[root@cluster-master-10 samples]# k logs -f sealosapp-vm-jg-209-predelete-26gfz -n vm-system\r\n+ /tmp/ssh-init.sh\r\n++ cat /tmp/sealosapp-ssh/hosts\r\n+ export SEALOS_HOST=10.20.97.10\r\n+ SEALOS_HOST=10.20.97.10\r\n+ ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa root@10.20.97.10 'bash /opt/sealosapp/vm-jg-209/uninstall.sh'\r\nWarning: Permanently added '10.20.97.10' (ED25519) to the list of known hosts.\r\n+ export COMMON_NAMESPACE=vm-system\r\n+ COMMON_NAMESPACE=vm-system\r\n+ export REPOSITORY=harbor.cloud.net\r\n+ REPOSITORY=harbor.cloud.net\r\n+ export TAG=v1.2.2\r\n+ TAG=v1.2.2\r\n+ export SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-20\r\n+ SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-209\r\n+ service_name=vm-jg-209\r\n+ component=vm-app\r\n+ '[' -z vm-jg-209 ']'\r\n+ '[' -z vm-app ']'\r\n+ sealos run harbor.cloud/vm-app:v1.2.2 -e NAMESPACE=vm-system -e UNINSTALL=true -e chart=vm-app -e release=vm-jg-209 -e CPU_CORES=4 -e DATA_DISK_1_PVC_VOLUME_MODE=Block -e DATA_DISK_1_SIZE=10Gi -e DATA_DISK_1_STORAGE_CLASS=sc-jnds-data -e DATA_DISK_1_TYPE=pvc -e DISK_SIZE=20Gi -e MEMORY=48Gi -e POWER_ON=true -e STORAGE_CLASS=sc-jnds-yw -e 'TCP_PORTS=22 5432 18198 18196 18089' --cmd=./opt/uninstall.sh -f\r\n2024-08-14T09:23:51 info sync new version copy pki config: /var/lib/sealos/data/default/pki /root/.sealos/default/pki\r\n2024-08-14T09:23:51 info sync new version copy etc config: /var/lib/sealos/data/default/etc /root/.sealos/default/etc\r\n2024-08-14T09:23:51 info start to install app in this cluster\r\n2024-08-14T09:23:51 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-14T09:23:51 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-14T09:23:51 info Executing PreProcess Pipeline in InstallProcessor\r\nError: failed to mount: mounting \"default-qbfp1r97\" container \"default-qbfp1r97\": saving updated state for build container \"0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b\": saving builder state to \"/var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json\": rename /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/.tmp-buildah.json1423736918 /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json: no such file or directory\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v4.3.5\r\n- Docker version: 24.0.9\r\n- Kubernetes version: 1.26\r\n- Operating system:  openeuler 22.03\r\n- Runtime environment:\r\n- Cluster size:  standalone k3s cluster with only one node maintained\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-12-18T05:27:34Z",
        "number": 4965,
        "state": "CLOSED",
        "title": "BUG: failed to mount: mounting \"default-qbfp1r97\" container \"default-qbfp1r97\": saving updated state for build container",
        "url": "https://github.com/labring/sealos/issues/4965",
        "login": "jeven2016",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos-cloud-5.0.0\n\n### How to reproduce the bug?\n\n正在安装 Sealos Cloud.\r\n2024-08-12T22:11:42 info start to install app in this cluster\r\n2024-08-12T22:11:42 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-12T22:11:42 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-12T22:11:42 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-08-12T22:11:43 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env.tmpl completed\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml.tmpl completed\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml.tmpl completed\r\n2024-08-12T22:12:16 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-08-12T22:12:16 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\netc/sealos/.env: line 1: syntax error near unexpected token `newline'\r\nError: exit status 2\r\n\n\n### What is the expected behavior?\n\n正常安装\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n应该是模板没有替换参数，找不到替换的值，显示<no value>",
        "closed": true,
        "closedAt": "2024-10-17T04:34:11Z",
        "number": 4962,
        "state": "CLOSED",
        "title": "sealos-cloud-5.0.0 install failed",
        "url": "https://github.com/labring/sealos/issues/4962",
        "login": "kinbod",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0\n\n### How to reproduce the bug?\n\nwhy still stay this step ? When executing curl, I confirmed that I pressed Enter all the way, except for entering the domain name.\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n![240810_12h34m56s_screenshot](https://github.com/user-attachments/assets/a400982a-044a-4ec1-a25c-166cf8672698)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0\r\n- Docker version: \r\n- Kubernetes version: 1.29.7\r\n- Operating system: Debian 12\r\n- Runtime environment: maybe containerd ? I use sealos cli \r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:55Z",
        "number": 4956,
        "state": "CLOSED",
        "title": "sealos 5.0,  self domain",
        "url": "https://github.com/labring/sealos/issues/4956",
        "login": "mkshift",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nsealos run时和sealos apply -f Clusterfile时docker默认的存储路径是/var/lib/docker，docker网络地址段也是默认的，这两个默认值在sealos初始化时如何修改？\n\n### If you have solution，please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:55Z",
        "number": 4955,
        "state": "CLOSED",
        "title": "How does sealos specify docker's data storage directory and modify bip during initialization?",
        "url": "https://github.com/labring/sealos/issues/4955",
        "login": "baodelu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nVersion: 4.3.0-7ee53f1d\n\n### How to reproduce the bug?\n\n为集群添加node节点时报错：error Applied to cluster error: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?\r\nError: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?   \r\n\r\n运行sealos add 之前也执行过export DO_NOT_CHECKSUM=true 无效果仍然报错，预添加的是第7个节点，之前一次性安装6节点（3master+3node）全部正常运行，但升级过kubernetes 版本到v1.28.0,操作系统均为ubuntu 22.04,网络为同网段，手动拷贝文件没问题，但是无法通过手动拷贝解决，add失败后需要delete掉节点上已经拷贝的其他文件会被清理掉\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:53Z",
        "number": 4951,
        "state": "CLOSED",
        "title": "为集群添加node节点时报错 Kubefile  sha256 sum not match",
        "url": "https://github.com/labring/sealos/issues/4951",
        "login": "suntiexuan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n```\r\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\\r\n  --cloud-version=v5.0.0 \\\r\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\r\n  --proxy-prefix=https://mirror.ghproxy.com\r\n```  \r\nprogram aborted Error: exit status 2\r\ninfracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1 pull image 403 Forbidden\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n```\r\naddon.extensions.kubeblocks.io/snapshot-controller enabled\r\nsecret/additional-scrape-configs created\r\nvmagent.operator.victoriametrics.com/victoria-metrics-k8s-stack patched\r\ndeployment.apps/vmagent-victoria-metrics-k8s-stack restarted\r\n正在修改 Ingress-nginx-controller 的容忍度, 以允许它在主节点上运行.\r\nconfigmap/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\n正在安装 Sealos Cloud.\r\n2024-08-03T23:03:59 info start to install app in this cluster\r\n2024-08-03T23:03:59 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env.tmpl completed\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml.tmpl completed\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml.tmpl completed\r\n2024-08-03T23:04:03 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-08-03T23:04:03 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\netc/sealos/.env: line 1: syntax error near unexpected token `newline'\r\nError: exit status 2\r\n```\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0.\r\n- Docker version:\r\n- Kubernetes version: v1.29.7\r\n- Operating system: Rocky Linux 9.2\r\n- Runtime environment: virtual machine (vmware, 20G memory, 12 core cpu, 500G storage)\r\n- Cluster size: 1master,1node\r\n- Additional information:\n```\n\n\n### Additional information\n\n```\r\n[root@sealos-master ~]# kubectl get nodes -o wide\r\nNAME            STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                      KERNEL-VERSION                 CONTAINER-RUNTIME\r\nsealos-master   Ready    control-plane   46m   v1.29.7   192.168.50.68    <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20\r\nsealos-node     Ready    <none>          46m   v1.29.7   192.168.50.226   <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20\r\n```\r\nkb-addon-snapshot-controller \r\n```\r\n  Normal   Scheduled  49s                default-scheduler  Successfully assigned kb-system/kb-addon-snapshot-controller-7bc7cf9dbf-vgcqg to sealos-node                                                                             │\r\n│   Normal   BackOff    18s (x2 over 48s)  kubelet            Back-off pulling image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\"                                                         │\r\n│   Warning  Failed     18s (x2 over 48s)  kubelet            Error: ImagePullBackOff                                                                                                                                                  │\r\n│   Normal   Pulling    5s (x3 over 49s)   kubelet            Pulling image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\"                                                                  │\r\n│   Warning  Failed     4s (x3 over 49s)   kubelet            Failed to pull image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\": failed to pull and unpack image \"infracreate-registry.cn │\r\n│ -zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\": failed to copy: httpReadSeeker: failed open: unexpected status code https://infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/v2/apecloud/snapshot-controller/b │\r\n│ lobs/sha256:1ef6c138bd5f2ac45f7b4ee54db0e513efad8576909ae9829ba649fb4b067388: 403 Forbidden                                                                                                                                          │\r\n│   Warning  Failed     4s (x3 over 49s)   kubelet            Error: ErrImagePull   \r\n```",
        "closed": true,
        "closedAt": "2024-12-10T01:38:54Z",
        "number": 4942,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4942",
        "login": "einnse",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nsealos run as shown in doc\r\n\r\n sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.0 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4  registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 registry.k8s.io/etcd:3.5.6-0 --single\r\n\r\n\r\n<img width=\"768\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c3984e90-48b3-4320-b6ae-bf0b138b6eed\">\r\n\n\n### What is the expected behavior?\n\n<img width=\"768\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ffe0c822-1ba2-46d1-8beb-72cdc854a52f\">\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-29T06:37:59Z",
        "number": 4933,
        "state": "CLOSED",
        "title": " failed to pull and unpack image \\\"registry.k8s.io/etcd:3.5.6-0",
        "url": "https://github.com/labring/sealos/issues/4933",
        "login": "mingmingshiliyu",
        "is_bot": false
      },
      {
        "body": "新买的服务器是arm架构的，但是既有集群是x86架构。如何将其加入集群。\n目前翻阅了文档和issue，说是支持sealos join命令，但是下载了5.0版本的 sealos没有此命令，请求帮助",
        "closed": true,
        "closedAt": "2024-07-24T13:56:34Z",
        "number": 4923,
        "state": "CLOSED",
        "title": "ARM nodes join x86 clusters to form heterogeneous clusters.",
        "url": "https://github.com/labring/sealos/issues/4923",
        "login": "Forfires",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n1\n\n### What is the current documentation state?\n\n\r\nhttps://github.com/user-attachments/assets/a7b7ed5c-0e31-4158-8a76-3b161ed70a70\r\n\r\n\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-22T04:27:27Z",
        "number": 4914,
        "state": "CLOSED",
        "title": "Docs: demo video",
        "url": "https://github.com/labring/sealos/issues/4914",
        "login": "zuoFeng59556",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1、deploy self-hosted sealos-cloud\r\n2、register a non-admin user\r\n3、deploy any app on sealos desktop\n\n### What is the expected behavior?\n\ndeploy success\n\n### What do you see instead?\n\n![image](https://github.com/user-attachments/assets/4bba48f9-7060-446c-b702-d71b85a89ebb)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nI just used a node to deploy sealos, when using the admin account can deploy the application normally, using a non-admin account not, if you need to go through some configuration to make the non-admin user can use it normally, how to modify it? ",
        "closed": true,
        "closedAt": "2024-08-01T16:54:37Z",
        "number": 4913,
        "state": "CLOSED",
        "title": "BUG: non-admin user can not deploy app on v5.0.0",
        "url": "https://github.com/labring/sealos/issues/4913",
        "login": "zaunist",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1.  centos 7.8 \r\n2.  sealos-v5.0.0 deploy k8s in private cloud environment\r\n3.  SHELL:  curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0-beta5   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n\r\n4.  ERROR INFO:  \r\nW0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)\r\n[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11\r\nfailed to pull image \"k8s.gcr.io/pause:3.5\": output: E0712 03:49:52.907061   67905 remote_image.go:167] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\" image=\"k8s.gcr.io/pause:3.5\"\r\ntime=\"2024-07-12T03:49:52-04:00\" level=fatal msg=\"pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\"\r\n, error: exit status 1\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-07-12T03:49:52 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\r\nError: failed to init masters: master pull image failed, error: exit status  1\n\n### What is the expected behavior?\n\ni think this is a bug ,  if user use this scripts to deploy k8s by sealos , this will try to get images from docker.io/xxx ;\r\n\r\nin the sealo project shoud considering  the internet is diffrent , this project  code shoud make user skip to get images form docker.io/xxx .\r\n\r\nThe code can avoid such an error, like \r\n---\r\n W0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)\r\n[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11\r\nfailed to pull image \"k8s.gcr.io/pause:3.5\": output: E0712 03:49:52.907061   67905 remote_image.go:167] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\" image=\"k8s.gcr.io/pause:3.5\"\r\ntime=\"2024-07-12T03:49:52-04:00\" level=fatal msg=\"pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\"\r\n, error: exit status 1\r\nTo see the stack trace of this error execute with --v=5 or higher\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0(latest)\r\n- Docker version: null\r\n- Kubernetes version: v1.27.11\r\n- Operating system: centos-7.8\r\n- Runtime environment: containerd\r\n- Cluster size: 1\r\n- Additional information: mabey code shoud fix this bug , otherwise user will not success deploy k8s . \r\nThis is a sad story！！！\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-17T03:07:09Z",
        "number": 4892,
        "state": "CLOSED",
        "title": "BUG:  The latest version ( v5.0.0 ) of the sealos privatization deployment will fail due to a public image pull issue",
        "url": "https://github.com/labring/sealos/issues/4892",
        "login": "ai-liuys",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-15T03:23:47Z",
        "number": 4891,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4891",
        "login": "ai-liuys",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\nVirtualBox:v7.0.18\r\nubuntu-20.04.5-live-server-amd64.iso\r\nhttps://old-releases.ubuntu.com/releases/20.04.5/ubuntu-20.04.5-live-server-amd64.iso\r\nUbuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)\r\nCPU：4核\r\n内存：8G\r\nIP：192.168.1.11\r\n\r\n安装Sealos命令行工具v4.3.7并安装K8S集群\r\n```\r\necho \"deb [trusted=yes] https://apt.fury.io/labring/ /\" | tee /etc/apt/sources.list.d/labring.list\r\napt update\r\napt install -y sealos\r\n\r\nsealos run \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.14 \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/calico:3.27.3 \\\r\n  --masters 192.168.1.11\r\n```\r\n查看CPU占用，发现systemd-udevd异常，进而发现calico异常\r\n```\r\n# htop\r\n\r\n# /lib/systemd/systemd-udevd -D\r\n\r\ncalico_tmp_B: /usr/lib/udev/rules.d/80-net-setup-link.rules:5 Failed to run builtin 'path_id': No such file or directory\r\n```\n\n### What is the expected behavior?\n\n希望空集群节点CPU持续占用小于0.5核\n\n### What do you see instead?\n\n空集群节点CPU持续占用1.5核\n\n### Operating environment\n\n```markdown\n- Sealos version:v4.3.7\r\n- Docker version:containerd:v1.7.18\r\n- Kubernetes version:v1.27.14\r\n- Operating system:Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)\r\n- Runtime environment:virtual machine (virbox, 8G memory, 4 core cpu, 100G storage)\r\n- Cluster size:1 master\r\n- Additional information:helm:v3.14.1, calico:3.27.3\n```\n\n\n### Additional information\n\n在线升级Ubuntu 22.04.4 LTS (GNU/Linux 5.15.0-116-generic x86_64)后，CPU持续占用小于0.5核，/lib/systemd/systemd-udevd -D正常",
        "closed": true,
        "closedAt": "2024-07-17T03:08:13Z",
        "number": 4881,
        "state": "CLOSED",
        "title": "The Sealos command line is used to build a K8Sv1.27.14 cluster node on Ubuntu20.04.5. The CPU continues to occupy 1.5 cores.",
        "url": "https://github.com/labring/sealos/issues/4881",
        "login": "MuJianning",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5\n\n### How to reproduce the bug?\n\n执行sealos apply -f Clusterfile  ，Clusterfile如下： xxxx均为内网IP\r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  hosts:\r\n  - ips:\r\n    - x.x.x.x:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n  - ips:\r\n    - x.x.x.x:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n  image:\r\n  - x.x.x.x:8080/labring/kubernetes:v1.29.6\r\n  - x.x.x.x:8080/labring/helm:v3.9.4\r\n  - x.x.x.x:8080/labring/cilium:v1.14.9\r\n  ssh:\r\n    passwd: \"PASSWORD\"\r\n    pk: /root/.ssh/id_rsa\r\n    port: 22\r\nstatus: {}\r\n执行完成后，会报错，报错内容为：\r\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\r\n[kubelet-check] Initial timeout of 40s passed.\r\n\r\nUnfortunately, an error has occurred:\r\n        timed out waiting for the condition\r\n\r\nThis error is likely caused by:\r\n        - The kubelet is not running\r\n        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\r\n\r\nIf you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\r\n        - 'systemctl status kubelet'\r\n        - 'journalctl -xeu kubelet'\r\n\r\nAdditionally, a control plane component may have crashed or exited when started by the container runtime.\r\nTo troubleshoot, list all containers using your preferred container runtimes CLI.\r\nHere is one example how you may list all running Kubernetes containers by using crictl:\r\n        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'\r\n        Once you have found the failing container, you can inspect its logs with:\r\n        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'\r\nerror execution phase wait-control-plane: couldn't initialize a Kubernetes cluster\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-07-10T11:29:16 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\nError: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\n检查kubelet状态，错误提示：\r\nJul 10 12:08:00 k8s-master-01 kubelet[24373]: E0710 12:08:00.064905   24373 kuberuntime_manager.go:1172] \"CreatePodSandbox for pod failed\" err=\"rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/a035ab8223d9d9db07cc92d89fa259d6708dc59ba4ab523e1216cd7458025bdd/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown\" pod=\"kube-system/kube-scheduler-k8s-master-01\"\r\n检查containerd状态，错误提示：\r\nJul 10 12:09:21 k8s-master-01 containerd[24036]: time=\"2024-07-10T12:09:21.072501176+08:00\" level=error msg=\"RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-k8s-master-01,Uid:0a9e7e740d78dd02274543ddd3c28942,Namespace:kube-system,Attempt:0,} failed, error\" error=\"failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/580589d578e6fa9d791ba8b2cffd221b4c1c24159015fbf34ee0891ebd427d16/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown\"\r\n最终通过file /usr/bin/runc发现/usr/bin/runc不是可执行程序，\r\n![微信图片_20240710121357](https://github.com/labring/sealos/assets/146060041/98a44917-bb12-45c5-97c1-b91e03185064)\r\n\r\n在另外一个节点执行 yum install runc -y，安装完成再查看file /usr/bin/runc为可执行程序。\r\n![a244443b81728790faf0f6cc606a8c8](https://github.com/labring/sealos/assets/146060041/7eaf45b3-1c37-4443-954d-555dc736f6d3)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.0-beta5\r\n- Docker version:无\r\n- Kubernetes version:1.29.6\r\n- Operating system:CentOS 7.9\r\n- Runtime environment:虚拟机 4核16G内存64G存储\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-10T07:47:19Z",
        "number": 4877,
        "state": "CLOSED",
        "title": "/usr/bin/runc is not an executable program",
        "url": "https://github.com/labring/sealos/issues/4877",
        "login": "linuxflow-hqukz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos_4.3.7_linux_arm64.tar.gz\n\n### How to reproduce the bug?\n\n操作系统：麒麟V10（SP3）/(Lance)-aarch64-Build23/20230324\r\n内核版本：4.19.90-52.22.v2207.ky10.aarch64\r\nsealos版本：4.3.7_linux_arm64、其他版本的也都试过了\r\nk8s版本：1.23.17、1.24.2、1.25.5、1.25.6、1.25.16、1.29.6都试过了\n\n### What is the expected behavior?\n\n卡在Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\r\n静态pod如kube-apiserver、kube-controller-manager、kube-scheduler这些都起不来；kubelet状态是running，报错静态pod容器捡不起来，getting node xxx not found；containerd状态running，报错failed to get sandbox container task: no running task found\r\n\r\n然后比较奇怪的是我之前用麒麟V10(SP2)的系统利用sealos4.3.7是能够正常拉起k8s集群的，只不过有个cgroup的问题导致coredns起不来，需要改containerd的cgropu配置，改的和kubelet的cgroup不一致才行，这样我觉得有隐患，所以听取网上意见更换成麒麟SP3系统，其他的都不变，结果连集群都拉不起来了！！！！！！ 来个大神救我，卡2天了，难受\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:4.3.7\r\n- Docker version:\r\n- Kubernetes version:1.25.16\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:3个master\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-17T03:08:30Z",
        "number": 4871,
        "state": "CLOSED",
        "title": "God help me! ! ! Kirin sp3 system arm architecture machine, deploying k8s is stuck here in kubelet startup/etc/kubernetes/manifest static pod, kubelet reports error getting node xxx not found",
        "url": "https://github.com/labring/sealos/issues/4871",
        "login": "SupRenekton",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\n5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\n1、run cmd sealos apply -f Clusterfile to create k8s.\r\nClusterfile:\r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  env:\r\n  - criData=/data/cri/lib\r\n  hosts:\r\n  - ips:\r\n    - 10.19.193.224:22\r\n    - 10.19.193.167:22\r\n    - 10.19.193.161:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx1\r\n      user: root\r\n      port: 22\r\n  - ips:\r\n    - 10.19.193.153:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxxx2\r\n      user: root\r\n      port: 22\r\n  image:\r\n  - 10.21.10.10:5000/labring/kubernetes:v1.27.11\r\n  - 10.21.10.10:5000/labring/helm:v3.10.3\r\n  - 10.21.10.10:5000/labring/calico:v3.26.1\r\n \r\n2、modify the Clusterfile and remove ip address 10.19.193.161, to delete node \r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  env:\r\n  - criData=/data/cri/lib\r\n  hosts:\r\n  - ips:\r\n    - 10.19.193.224:22\r\n    - 10.19.193.167:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx1\r\n      user: root\r\n      port: 22\r\n  - ips:\r\n    - 10.19.193.153:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx2\r\n      user: root\r\n      port: 22\r\n  image:\r\n  - 10.21.10.10:5000/labring/kubernetes:v1.27.11\r\n  - 10.21.10.10:5000/labring/helm:v3.10.3\r\n  - 10.21.10.10:5000/labring/calico:v3.26.1\r\n\r\n3、run cmd sealos apply -f Clusterfile\r\n2024-07-09T11:01:12 warn delete master 10.19.193.161:22 failed cannot get node with ip address 10.19.193.161:22: cannot find host with internal ip 10.19.193.161\r\n2024-07-09T11:01:12 error failed to clean node, exec command if which kubeadm;then kubeadm reset -f  -v 0;fi && \\\r\nrm -rf /etc/kubernetes/ && \\\r\nrm -rf /etc/cni && rm -rf /opt/cni && \\\r\nrm -rf /var/lib/etcd && (ip link delete kube-ipvs0 >/dev/null 2>&1 || true)\r\n failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n2024-07-09T11:01:12 error failed to clean node, exec command rm -rf $HOME/.kube failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n2024-07-09T11:01:12 info succeeded in deleting master 10.19.193.161:22\r\n2024-07-09T11:01:12 info start to sync lvscare static pod to node: 10.19.193.153:22 master: [10.19.193.224:6443 10.19.193.167:6443]\r\n10.19.193.153:22        2024-07-09T11:01:12 info generator lvscare static pod is success\r\n2024-07-09T11:01:12 info Executing pipeline UndoBootstrap in ScaleProcessor\r\n2024-07-09T11:01:12 error Applied to cluster error: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\nError: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n\r\n### What is the expected behavior?\r\n\r\n_No response_\r\n\r\n### What do you see instead?\r\n\r\n_No response_\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:v5.0.1\r\n- containerd version:v1.7.14\r\n- Kubernetes version:1.27.11\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-09-20T12:06:40Z",
        "number": 4869,
        "state": "CLOSED",
        "title": "failed to delete node",
        "url": "https://github.com/labring/sealos/issues/4869",
        "login": "lllicg",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\ncurl -sfL https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\  \r\n--cloud-domain=<your_domain> \\   \r\n--cert-path=<your_crt> \\  \r\n--key-path=<your_key>\r\n\r\n### What is the expected behavior?\r\n\r\nFull install\r\n\r\n### What do you see instead?\r\n\r\n```\r\n...\r\n2024-07-05T19:59:06 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-07-05T19:59:06 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\nnamespace/sealos-system created\r\nnamespace/sealos created\r\ncustomresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created\r\nno mongodb uri found, create mongodb and gen mongodb uri\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...mongodb secret has been generated successfully.\r\nno cockroachdb uri found, create cockroachdb and gen cockroachdb uri\r\ncockroachdb statefulset is created.\r\nwaiting for cockroachdb ready    Error: signal: killed\r\nwaiting for cockroachdb ready ...\r\n```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v5.0.0-beta5\r\n- Docker version: v20.10.7\r\n- Kubernetes version: v1.25.0\r\n- Operating system: Ubuntu 24.04\r\n- Runtime environment: 3 x 4gb / 2 cpu / 40GB server (Hetzner - CX22's)\r\n- Cluster size: 1 master, 2 nodes\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nInstall just stops with trying to start CockroachDB in the cluster. Eventually signal gets killed. Maybe not enough resources? There was an older issue like this (#4704). Just wondering if they were able to solve it. Cheers, Dave",
        "closed": true,
        "closedAt": "2024-07-06T00:37:58Z",
        "number": 4861,
        "state": "CLOSED",
        "title": "BUG: CockroachDB stops install process",
        "url": "https://github.com/labring/sealos/issues/4861",
        "login": "saashqdev",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nboth v4.3.7 and v5.0.0_beta\n\n### How to reproduce the bug?\n\n1. use sealos init install k8s (version such  v1.27.1);\r\n2. upgrade version to v1.27.5;\r\n3. add master or node\n\n### What is the expected behavior?\n\nthe newly added master or node install k8s v1.27.5\n\n### What do you see instead?\n\nthe newly added master or node install k8s v1.27.1\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\nNo matter what version of k8s you install, as long as the cluster is upgraded, when adding new masters or nodes, the new masters or nodes will be installed with the previous version.",
        "closed": true,
        "closedAt": "2024-07-09T01:12:29Z",
        "number": 4856,
        "state": "CLOSED",
        "title": "BUG: add nodes after upgrading, the version installed on the newly added nodes is incorrect",
        "url": "https://github.com/labring/sealos/issues/4856",
        "login": "yangxggo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nboth v4.3.7 and v5.0.0_beta\n\n### How to reproduce the bug?\n\nupgrade k8s from v1.26.x to v1.27.y\n\n### What is the expected behavior?\n\nall pods restart and work fine\n\n### What do you see instead?\n\nsome pods (include kube-scheduler) are pending or terminating for a long time\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-09T01:13:42Z",
        "number": 4848,
        "state": "CLOSED",
        "title": "BUG: upgrade from k8s v1.26.x to v1.27.y , kubelet restart failed",
        "url": "https://github.com/labring/sealos/issues/4848",
        "login": "yangxggo",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nI've noticed that https://github.com/labring/sealos/blob/main/controllers/account/controllers/utils/email.go is using https://github.com/go-gomail/gomail for sending SMTP mails. go-mail/gomail is not actively maintained anymore and hasn't seen an update/response to an issue by the maintainer/merged PR in more than 6-7 years.\n\n### If you have solution，please describe it\n\nI am the maintainer of https://github.com/wneessen/go-mail which is pretty similar to go-mail/gomail but actively maintained and following modern standards and idiomatic Go. If there is any interest, I am willing to replace go-gomail/gomail in `email.go` to wneessen/go-mail.\r\n\r\nLet me know if a PR is of interest.\n\n### What alternatives have you considered?\n\nN/A",
        "closed": true,
        "closedAt": "2024-11-26T07:01:35Z",
        "number": 4840,
        "state": "CLOSED",
        "title": "Chore: Replace go-gomail/gomail with wneessen/go-mail",
        "url": "https://github.com/labring/sealos/issues/4840",
        "login": "wneessen",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nAPI Documentation\n\n### What is the current documentation state?\n\nNot Found\n\n### What is your suggestion?\n\nAdd separate section to describe Sealos APIs in detail.\n\n### Why is this change beneficial?\n\nProgrammatically control multitenant apps deployment staging and production.\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-07T00:09:41Z",
        "number": 4839,
        "state": "CLOSED",
        "title": "Request of Adding API documentation for developers",
        "url": "https://github.com/labring/sealos/issues/4839",
        "login": "mudevai",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos_4.3.7_linux_arm64.tar.gz\n\n### How to reproduce the bug?\n\n国产linux系统：4.19.90-24.4.v2101.ky10.aarch64 #1 SMP Mon May 24 14:45:37 CST 2021 aarch64 aarch64 aarch64 GNU/Linux\r\nsealos gen labring/kubernetes:v1.25.16 \\\r\n  labring/helm:v3.13.2 \\\r\n  labring/calico:v3.24.6 \\\r\n  --masters x.x.x.x,x.x.x.x,x.x.x.x --output Clusterfile\r\nClusterfile配置没有修改，直接apply：sealos apply -f Clusterfile\r\n\r\nk8s集群建起来后，只有coredns显示pod没起来，CrashLoopBackOff状态\r\n报错信息：\r\nfailed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n2个coredns pod CrashLoopBackOff，其他pod状态正常\r\nfailed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"\n\n### Operating environment\n\n```markdown\n- Sealos version: sealos_4.3.7_linux_arm64.tar.gz\r\n- Docker version: 无\r\n- Kubernetes version: kubernetes:v1.25.16\r\n- Operating system: 4.19.90-24.4.v2101.ky10.aarch64\r\n- Runtime environment: \r\n- Cluster size: 3个Master节点\r\n- Additional information:\r\n  labring/helm:v3.13.2\r\n  labring/calico:v3.24.6\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-10T10:08:52Z",
        "number": 4836,
        "state": "CLOSED",
        "title": "BUG: brief descricontainer_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"ption of the bug",
        "url": "https://github.com/labring/sealos/issues/4836",
        "login": "SupRenekton",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\n5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\nsealos add --nodes 10.0.12.12 --debug\r\n\r\n### What is the expected behavior?\r\n\r\n添加节点成功\r\n\r\n### What do you see instead?\r\n\r\n<img width=\"1438\" alt=\"image\" src=\"https://github.com/labring/sealos/assets/27618687/277697e1-244f-44b7-9c55-486dbf3b0475\">\r\n完整的debug日志在下面链接中\r\n[node.log](https://github.com/user-attachments/files/16043343/node.log)\r\n\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-07-02T07:12:01Z",
        "number": 4829,
        "state": "CLOSED",
        "title": "BUG: Failed to add node node",
        "url": "https://github.com/labring/sealos/issues/4829",
        "login": "gclm",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solution，please describe it\n\n![微信图片_20240629132646](https://github.com/labring/sealos/assets/22442450/7cf69ac3-211c-436e-899a-11d54a4ab05c)\r\n\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T01:28:18Z",
        "number": 4826,
        "state": "CLOSED",
        "title": "How to install through sealos when Dockerhub cannot be accessed",
        "url": "https://github.com/labring/sealos/issues/4826",
        "login": "liuchengwen",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7-f39b2339\n\n### How to reproduce the bug?\n\n1. https://rockylinux.org/zh_CN （2024-06）最新版本\r\n2. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4\r\n3. Job for containerd.service failed because the control process exited with error code.\r\n\n\n### What is the expected behavior?\n\n前置条件是干净的系统，但是这么安装为何不行？\n\n### What do you see instead?\n\n[root@k8s-master01 ~]# systemctl status containerd.service                                                                                   ● containerd.service - containerd container runtime\r\n     Loaded: loaded (/etc/systemd/system/containerd.service; enabled; preset: disabled)\r\n     Active: activating (auto-restart) (Result: exit-code) since Sat 2024-06-29 10:25:57 CST; 3s ago\r\n       Docs: https://containerd.io\r\n    Process: 4282 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)\r\n    Process: 4283 ExecStart=/usr/bin/containerd (code=exited, status=203/EXEC)\r\n   Main PID: 4283 (code=exited, status=203/EXEC)\r\n        CPU: 4ms\r\n[root@k8s-master01 ~]# journalctl -xeu containerd.service\r\n░░ Subject: Process /usr/bin/containerd could not be executed\r\n░░ Defined-By: systemd\r\n░░ Support: https://wiki.rockylinux.org/rocky/support\r\n░░\r\n░░ The process /usr/bin/containerd could not be executed and failed.\r\n░░\r\n░░ The error number returned by this process is ERRNO.\r\nJun 29 10:26:18 k8s-master01 systemd[4415]: containerd.service: Failed at step EXEC spawning /usr/bin/containerd: No such file or>\r\n░░ Subject: Process /usr/bin/containerd could not be executed\r\n░░ Defined-By: systemd\r\n░░ Support: https://wiki.rockylinux.org/rocky/support\r\n░░\r\n░░ The process /usr/bin/containerd could not be executed and failed.\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-29T04:35:28Z",
        "number": 4825,
        "state": "CLOSED",
        "title": "rockylinux",
        "url": "https://github.com/labring/sealos/issues/4825",
        "login": "zeje",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5-a0b3363d9\n\n### How to reproduce the bug?\n\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh --zh  \\\r\n --cloud-version=v5.0.0-beta5 \\\r\n --image-registry=registry.cn-shanghai.aliyuncs.com \\\r\n --proxy-prefix=https://mirror.ghproxy.com \\\r\n --master-ips=192.168.0.112,192.168.0.113,192.168.0.120 \\\r\n --node-ips=192.168.0.123,192.168.0.121 \\\r\n --pod-cidr=100.64.0.0/10 \\\r\n --service-cidr=10.96.0.0/22 \\\r\n --cloud-domain=192.168.0.112.nip.io \\\r\n --cloud-port=443 \\\r\n --ssh-password=123456 \\\r\n --kubernetes-version=1.25.6\r\n\r\n![image](https://github.com/labring/sealos/assets/174031613/65457c16-089a-42b1-b862-7afd94e34ba9)\r\n\r\n![image](https://github.com/labring/sealos/assets/174031613/d63b1cb2-8eb6-4e83-ae98-5ee882cf93a0)\r\n\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\nEverything went well up until now\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0-beta5\r\n- Docker version: \r\n- Kubernetes version:  1.25.6\r\n- Operating system: ubuntu 22.04\r\n- Runtime environment: virtual machine (8G memory, 4 core cpu, 80G storage) *5\r\n- Cluster size: 3 master, 2 node\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:45Z",
        "number": 4814,
        "state": "CLOSED",
        "title": "BUG: Error: run chmod to rootfs failed: exit status 1",
        "url": "https://github.com/labring/sealos/issues/4814",
        "login": "nlqiong-me",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0\n\n### How to reproduce the bug?\n\nroot@VM-32-39-ubuntu:~# kubectl get no -owide\r\nNAME              STATUS   ROLES           AGE   VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION       CONTAINER-RUNTIME\r\nvm-32-39-ubuntu   Ready    control-plane   66m   v1.26.15   172.16.32.39   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15\r\nvm-32-48-ubuntu   Ready    <none>          66m   v1.26.15   172.16.32.48   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15\r\nroot@VM-32-39-ubuntu:~# ./sealos delete --nodes 172.16.32.48\r\n2024-06-27T15:28:49 info are you sure to delete these nodes?\r\n✔ Yes [y/yes], No [n/no]: y█\r\n2024-06-27T15:28:50 info start to scale this cluster\r\n2024-06-27T15:28:50 info Executing pipeline DeleteCheck in ScaleProcessor.\r\n2024-06-27T15:28:50 info checker:hostname [172.16.32.39:22]\r\n2024-06-27T15:28:50 info checker:containerd [172.16.32.39:22]\r\nError: failed to run checker: containerd is installed on 172.16.32.39:22 please uninstall it first\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n#4738 \r\nhttps://github.com/labring/sealos/blob/main/pkg/apply/processor/scale.go#L161\r\n\r\nA more precise `Phase` is needed to tell the checker whether it needs to be checked.",
        "closed": true,
        "closedAt": "2024-07-02T09:28:09Z",
        "number": 4809,
        "state": "CLOSED",
        "title": "BUG: delete node error: failed to run checker: containerd is installed on xxx please uninstall it first",
        "url": "https://github.com/labring/sealos/issues/4809",
        "login": "bxy4543",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv4.3.7\r\n\r\n### How to reproduce the bug?\r\n执行命令：\r\n[root@master sealos4.4.0]# sealos  registry copy reg.ide.local:5000/titanide/busybox:latest sealos.hub:5000\r\nGetting image source signatures\r\nError: trying to reuse blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa at destination: checking whether a blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa exists in sealos.hub:5000/titanide/busybox: authentication required\r\n\r\n### What is the expected behavior?\r\n\r\n_No response_\r\n\r\n### What do you see instead?\r\n\r\n_No response_\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:44Z",
        "number": 4802,
        "state": "CLOSED",
        "title": "BUG: sealos registry copy无法鉴权通过，即便执行过sealos login命令，依然会提示：authentication required",
        "url": "https://github.com/labring/sealos/issues/4802",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos ：4.1.3\n\n### How to reproduce the bug?\n\n1、sealos pull  labring/kubernetes:v1.27.7,打包成tar包：kubernetes-v1.27.7.tar\r\n2、mount至本地后修改images/shim/下DefaultImageList  将其修改为私有仓库镜像如：\r\n192.182.xxx.xx/registry.k8s.io/coredns:v1.9.3\r\n192.182.xxx.xx/registry.k8s.io/etcd:3.5.10-0\r\n192.182.xxx.xx/registry.k8s.io/kube-controller-manager:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/kube-scheduler:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/kube-proxy:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/pause:3.9\r\n192.182.xxx.xx/registry.k8s.io/kube-apiserver:v1.26.15\r\n3、删除registry目录下文件\r\n4、修改Kubefile文件，添加label\r\nFROM scratch\r\nMAINTAINER sealos\r\nLABEL init=\"init-cri.sh \\$registryDomain \\$registryPort && bash init.sh\" \\\r\n      clean=\"clean.sh && bash clean-cri.sh \\$criData\" \\\r\n      check=\"check.sh \\$registryData\" \\\r\n      init-registry=\"init-registry.sh \\$registryData \\$registryConfig\" \\\r\n      clean-registry=\"clean-registry.sh \\$registryData \\$registryConfig\" \\\r\n      sealos.io.type=\"rootfs\" sealos.io.version=\"v1beta1\" version=\"v1.26.15\"\r\nENV criData=/var/lib/containerd \\\r\n    registryData=/var/lib/registry \\\r\n    registryConfig=/etc/registry \\\r\n    registryDomain=192.168.xxx.xx \\\r\n    registryPort=5000 \\\r\n    registryUsername=admin \\\r\n    registryPassword=passw0rd \\\r\n    disableApparmor=false \\\r\n    SEALOS_SYS_CRI_ENDPOINT=/var/run/containerd/containerd.sock \\\r\n    SEALOS_SYS_IMAGE_ENDPOINT=/var/run/image-cri-shim.sock \\\r\nCOPY . .\r\n5、sealos build\r\n6、sealos run 出现部分报错：\r\n6.1:\r\n\"PullImage from image service failed\" err=\"rpc error: code = Unavailable desc = error reading from server: EOF\" image=\"192.168.xxx.xx:5000/\"\r\nFATA[0000] pulling image: rpc error: code = Unavailable desc = error reading from server: EOF\r\n6.2\r\n[preflight] Pulling images required for setting up a Kubernetes cluster\r\n[preflight] This might take a minute or two, depending on the speed of your internet connection\r\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r\nI0624 15:59:50.512988 2316864 checks.go:832] using image pull policy: IfNotPresent\r\nI0624 15:59:50.531237 2316864 checks.go:849] pulling: registry.k8s.io/kube-apiserver:v1.26.15\r\nI0624 15:59:50.625223 2316864 checks.go:849] pulling: registry.k8s.io/kube-controller-manager:v1.26.15\r\nI0624 15:59:50.713096 2316864 checks.go:849] pulling: registry.k8s.io/kube-scheduler:v1.26.15\r\nI0624 15:59:50.800815 2316864 checks.go:849] pulling: registry.k8s.io/kube-proxy:v1.26.15\r\nI0624 15:59:50.883503 2316864 checks.go:849] pulling: registry.k8s.io/pause:3.9\r\nI0624 15:59:50.970530 2316864 checks.go:849] pulling: registry.k8s.io/etcd:3.5.10-0\r\nI0624 15:59:51.059066 2316864 checks.go:849] pulling: registry.k8s.io/coredns/coredns:v1.9.3\r\n[preflight] Some fatal errors occurred:\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.10-0: output: time=\"2024-06-24T15:59:51+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n\r\n疑问点：为何从远端拉取镜像而不是本地内置registry，查看containerd服务发现未设置point\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-07T00:09:42Z",
        "number": 4797,
        "state": "CLOSED",
        "title": "BUG: clusterimages build error",
        "url": "https://github.com/labring/sealos/issues/4797",
        "login": "ricardolrl",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0\n\n### How to reproduce the bug?\n\nInstall Sealos with the template frontend enabled\n\n### What is the expected behavior?\n\nNo zombies\n\n### What do you see instead?\n\nFind out one of the pids of the zombie processes:\r\n```\r\n# ps aux | grep Z\r\n1001     63841  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>\r\n1001     64043  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>\r\n1001     64183  0.0  0.0      0     0 ?        Z    Jun22   0:00 [git] <defunct>\r\n1001     64495  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>\r\n1001     64661  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>\r\n```\r\nFind out which container holds this pid:\r\n```\r\n# pstree -s -p 64661\r\nsystemd(1)───containerd-shim(43915)───node(46306)───git(64661)\r\n# pstree 46306\r\nnode─┬─108*[git]\r\n     └─14*[{node}]\r\n# crictl ps -q | xargs crictl inspect --output go-template --template '{{.info.pid}}, {{.status.metadata.name}}' | grep 46306\r\n46306, template-frontend\r\n```\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\nThese lines might be problematic:\r\n\r\nhttps://github.com/labring/sealos/blob/78cefc7715b6c0952f079cc6bbc1bd32e96adbcb/frontend/providers/template/src/pages/api/updateRepo.ts#L83-L87",
        "closed": true,
        "closedAt": "2024-06-25T06:18:03Z",
        "number": 4796,
        "state": "CLOSED",
        "title": "BUG: template frontend leaves zombie `git` processes",
        "url": "https://github.com/labring/sealos/issues/4796",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.0.0\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n维护集群添加多个节点\n\n### What do you see instead?\n\n2024-06-23T17:31:21 debug cmd for bash in host:  /var/lib/sealos/data/default/rootfs/opt/sealctl token\r\n\r\n\r\n\r\n\r\n\r\n2024-06-23T17:33:57 debug failed to exec remote 10.20.0.3:22 shell: /var/lib/sealos/data/default/rootfs/opt/sealctl token output: &{default} error: exit status 1\r\n2024-06-23T17:33:57 error Applied to cluster error: exit status 1\r\n2024-06-23T17:33:57 debug write cluster file to local storage: /root/.sealos/default/Clusterfile\r\nError: exit status 1\n\n### Operating environment\n\n```markdown\n- Sealos version: 4.0.0\r\n- Docker version: containerd 1.6.2\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\ncentos 7.6\n```\n\n\n### Additional information\n\n你们这个写的太乱了, 之前的版本刚开始安装是好的,添加节点都没问题, 结果运行一段时间, 添加节点就是问题了",
        "closed": true,
        "closedAt": "2024-11-07T00:09:43Z",
        "number": 4794,
        "state": "CLOSED",
        "title": "Failed to add node",
        "url": "https://github.com/labring/sealos/issues/4794",
        "login": "he-aook",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n离线安装 K8s\n\n### What is the current documentation state?\n\n_No response_\n\n### What is your suggestion?\n\n老师，您好。\r\n首先在有网络的环境中导出集群镜像： 这里面能否增加\r\n具体k8s和各种网络插件 给一个最佳的推荐  \r\n在执行命令 sealos run kubernetes.tar \r\n遇到以下错误 \r\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r\n^[2^NW0621 15:16:42.324677   16367 checks.go:835] detected that the sandbox image \"sealos.hub:5000/pause:3.9\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\r\nerror execution phase preflight: [preflight] Some fatal errors occurred:\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.11: output: E0621 15:08:59.182331   16827 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\\\": dial tcp 74.125.23.82:443: i/o timeout\" image=\"registry.k8s.io/kube-apiserver:v1.28.11\"\r\ntime=\"2024-06-21T15:08:59+08:00\" level=fatal msg=\"pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\\\": dial tcp 74.125.23.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.11: output: E0621 15:11:33.659002   16943 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\" image=\"registry.k8s.io/kube-controller-manager:v1.28.11\"\r\ntime=\"2024-06-21T15:11:33+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.11: output: E0621 15:14:07.264143   17021 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\" image=\"registry.k8s.io/kube-scheduler:v1.28.11\"\r\ntime=\"2024-06-21T15:14:07+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.11: output: E0621 15:16:42.295317   17286 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\\\": dial tcp 108.177.125.82:443: i/o timeout\" image=\"registry.k8s.io/kube-proxy:v1.28.11\"\r\ntime=\"2024-06-21T15:16:42+08:00\" level=fatal msg=\"pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\\\": dial tcp 108.177.125.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: E0621 15:19:05.165352   17615 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unavailable desc = error reading from server: EOF\" image=\"registry.k8s.io/pause:3.9\"\r\ntime=\"2024-06-21T15:19:05+08:00\" level=fatal msg=\"pulling image: rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.12-0: output: time=\"2024-06-21T15:19:05+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///run/containerd/containerd.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\\\"\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: E0621 15:21:39.262383   18253 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to resolve reference \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\\\": dial tcp 108.177.125.82:443: i/o timeout\" image=\"registry.k8s.io/coredns/coredns:v1.10.1\"\r\ntime=\"2024-06-21T15:21:39+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to resolve reference \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\\\": dial tcp 108.177.125.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\n\n### Why is this change beneficial?\n\n可能因为特殊原因 很多人会遇到这种情况 \n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:46Z",
        "number": 4793,
        "state": "CLOSED",
        "title": "Can you add more details about the offline installation of k8s?",
        "url": "https://github.com/labring/sealos/issues/4793",
        "login": "camuselliot",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0-beta5\n\n### How to reproduce the bug?\n\nError during installation\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...Error: signal: killed\r\n\r\nInstallation steps\r\n\r\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\  \r\n--cloud-version=v5.0.0-beta5 \\  \r\n--image-registry=registry.cn-shanghai.aliyuncs.com --zh \\  \r\n--proxy-prefix=https://mirror.ghproxy.com\r\n```\r\n\r\n\r\nMaster IP：192.168.1.80\r\nNode IP：192.168.1.81,192.168.1.83\r\nDomain name：192.168.1.80.nip.io\r\n\r\nMongoDB 5.0版本依赖支持 AVX 指令集的 CPU, 当前环境不支持 AVX, 已切换为 MongoDB 4.0版本, 更多信息查看: https://www.mongodb.com/docs/v5.0/administration/production-notes/\r\n2024-06-20T18:56:19 info start to install app in this cluster\r\n2024-06-20T18:56:19 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-06-20T18:56:19 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-06-20T18:56:19 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-06-20T18:56:20 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-06-20T18:58:01 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-06-20T18:58:01 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\nnamespace/sealos-system created\r\nnamespace/sealos created\r\ncustomresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created\r\nno mongodb uri found, create mongodb and gen mongodb uri\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...Error: signal: killed\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\nkubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS    RESTARTS       AGE\r\ncert-manager                cert-manager-5f8646db6b-5nhxh                                     1/1     Running   0              14h\r\ncert-manager                cert-manager-cainjector-5cf5f57dd7-fdmp6                          1/1     Running   0              14h\r\ncert-manager                cert-manager-webhook-687b7f8b97-l5txj                             1/1     Running   0              14h\r\ncockroach-operator-system   cockroach-operator-manager-848fd7d88d-zpnk4                       1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-26dpm                                    1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-6n9mw                                    1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-wb5d9                                    1/1     Running   0              14h\r\nkb-system                   kb-addon-snapshot-controller-64b4dcb6bc-lzfdm                     1/1     Running   2 (14h ago)    14h\r\nkb-system                   kubeblocks-6954875d7d-rtcnh                                       1/1     Running   3 (14h ago)    14h\r\nkb-system                   kubeblocks-dataprotection-5cfdf54fcc-zh7wg                        1/1     Running   4 (14h ago)    14h\r\nkube-system                 cilium-gpgmh                                                      1/1     Running   0              14h\r\nkube-system                 cilium-gplkj                                                      1/1     Running   0              14h\r\nkube-system                 cilium-jtf27                                                      1/1     Running   0              14h\r\nkube-system                 cilium-operator-97f465f54-lrktg                                   1/1     Running   4 (14h ago)    14h\r\nkube-system                 coredns-5d78c9869d-8mxzp                                          1/1     Running   0              14h\r\nkube-system                 coredns-5d78c9869d-kppdj                                          1/1     Running   0              14h\r\nkube-system                 etcd-rofine30                                                     1/1     Running   2              14h\r\nkube-system                 kube-apiserver-rofine30                                           1/1     Running   2              14h\r\nkube-system                 kube-controller-manager-rofine30                                  1/1     Running   10 (14h ago)   14h\r\nkube-system                 kube-proxy-5mkgv                                                  1/1     Running   0              14h\r\nkube-system                 kube-proxy-82k5f                                                  1/1     Running   0              14h\r\nkube-system                 kube-proxy-kvnxl                                                  1/1     Running   0              14h\r\nkube-system                 kube-scheduler-rofine30                                           1/1     Running   10 (14h ago)   14h\r\nkube-system                 kube-sealos-lvscare-rofine31                                      1/1     Running   1              14h\r\nkube-system                 kube-sealos-lvscare-rofine32                                      1/1     Running   1              14h\r\nkube-system                 metrics-server-84c9bd846f-hb8sh                                   1/1     Running   0              14h\r\nopenebs                     openebs-localpv-provisioner-6b456b4c69-wxxr8                      1/1     Running   4 (14h ago)    14h\r\nvm                          victoria-metrics-k8s-stack-grafana-55b778cbf9-jnbsb               3/3     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-658d9b4dd5-tglth    1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-scbzm         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-sd7tw         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-vmglj         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-6b774mjx9v   1/1     Running   3 (14h ago)    14h\r\nvm                          vmagent-victoria-metrics-k8s-stack-c694d459d-vsxxm                2/2     Running   0              14h\r\nvm                          vmalert-victoria-metrics-k8s-stack-695868d7fc-89vr8               2/2     Running   0              14h\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running   0              14h\r\nvm                          vmsingle-victoria-metrics-k8s-stack-687d8cc7b4-r6c6d              1/1     Running   0              14h\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0-beta5\r\n- Docker version: containerd://1.7.15\r\n- Kubernetes version: v1.27.11\r\n- Operating system: Anolis OS 8.9   5.10.134-16.2.an8.x86_64\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:47Z",
        "number": 4790,
        "state": "CLOSED",
        "title": "Error during installation: waiting for MongoDB to be ready… Error: signal: killed.",
        "url": "https://github.com/labring/sealos/issues/4790",
        "login": "zhubao315",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\n1. sealos reset\r\n2. rm ~/.sealos/ -rf \r\n3. sealos gen docker.io/labring/kubernetes:v1.27.14 --masters 192.168.x.x --pk=$HOME/.ssh/id_rsa -o ./Clusterfile\r\n4. sed -i \"s|100.64.0.0/10|100.77.0.0/10|g\" ./Clusterfile\r\n5. sed -i \"s|10.96.0.0/22|10.99.0.0/22|g\" ./Clusterfile\r\n6. sealos apply -f ./Clusterfile\r\n7. sealos run labring/helm:v3.14.1\r\n8. sealos run labring/cilium:v1.14.11 --env ExtraValues=\"ipam.mode=kubernetes\"\r\n9. kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o wide\n\n### What is the expected behavior?\n\nThe IP address assigned to the expected pod is 100.77.x.x\n\n### What do you see instead?\n\nIn fact, the IP assigned to the pod is 100.64.x.x:\r\n![image](https://github.com/labring/sealos/assets/21333000/c1030583-ce99-4f23-bb91-a71860943261)\r\nThe running configuration of the kube-controller-manager pod is as follows：\r\n![image](https://github.com/labring/sealos/assets/21333000/0b2205bf-4f4b-40ec-a2f4-e5ab6f49d7b7)\r\n\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: 4.3.7\r\n- Docker version: none\r\n- Kubernetes version: 1.27.14\r\n- Operating system: centos7\r\n- Runtime environment: 32core 128G\r\n- Cluster size: 1\r\n- Additional information: none\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-20T02:27:58Z",
        "number": 4779,
        "state": "CLOSED",
        "title": "BUG: Modifying pod_cidr via pod_subnet does not take effect",
        "url": "https://github.com/labring/sealos/issues/4779",
        "login": "cnmac",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.1.4\n\n### How to reproduce the bug?\n\n1.netstat -anpl|grep 5000\r\n<img width=\"1080\" alt=\"CleanShot 2024-06-13 at 13 57 36@2x\" src=\"https://github.com/labring/sealos/assets/16560252/4ad3b82c-eada-46f0-ae3c-158f9590d1a3\">\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v4.1.4\r\n- Docker version: v23.0.0\r\n- Kubernetes version: v1.25.6\r\n- Operating system: ubuntu20.04\r\n- Runtime environment: cloud(tencent, 8G memory, 2core cpu, 50G storage)\r\n- Cluster size: 1 master,1 node\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-17T07:13:00Z",
        "number": 4774,
        "state": "CLOSED",
        "title": "BUG: The local registry has a large number of network connections that will not be released",
        "url": "https://github.com/labring/sealos/issues/4774",
        "login": "xmwilldo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7 v5.0.0-beta\n\n### How to reproduce the bug?\n\nOS openeuler 24.03 LTS\r\n\r\n253 作为外部控制机，其他主机作为真实master，如101\r\n```\r\nsealos gen labring/kubernetes:v1.29.5-4.3.7 labring/helm:v3.14.0 labring/metrics-server:v0.6.4 --masters 192.168.3.101,192.168.3.102,192.168.3.103  ....\r\nsealos apply -f  ...\r\nwarn failed to copy image 127.0.0.1:46879/kube-apiserver:v1.29.5: trying to reuse blob sha256:b2ce0e0660777651a7a188ae1128acc61d01aca10a035a8b1faa2cdd8bbf0785 at destination: pinging container registry 192.168.3.101:5050: received unexpected HTTP status: 502 Bad Gateway\r\n```\r\nsealos 同步镜像至101（master0)的5050 registry，但是registry启动 http 5000/ debug 5001 ，所以始终提示  502 Bad Gateway\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:  v4.3.7\r\n- Docker version: no\r\n- Kubernetes version:  labring/kubernetes:v1.29.5-4.3.7\r\n- Operating system: openeuler 2403\r\n- Runtime environment: no\r\n- Cluster size: no \r\n- Additional information: no\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-10T14:59:56Z",
        "number": 4770,
        "state": "CLOSED",
        "title": "Failed to transfer image during installation",
        "url": "https://github.com/labring/sealos/issues/4770",
        "login": "yulinor",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\n1.使用yum 安装\r\n2.执行创建单机集群\r\n3. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\r\n4.centos7.9\n\n### What is the expected behavior?\n\n正常运行、\n\n### What do you see instead?\n\n[root@k8smaster ~]# kubectl get po -A\r\nNAMESPACE     NAME                                READY   STATUS                  RESTARTS       AGE\r\nkube-system   cilium-22szr                        0/1     Init:CrashLoopBackOff   20 (24s ago)   78m\r\nkube-system   cilium-operator-86666d88cb-dnfhx    1/1     Running                 0              78m\r\nkube-system   coredns-5d78c9869d-ggk9p            0/1     Pending                 0              78m\r\nkube-system   coredns-5d78c9869d-qgd2c            0/1     Pending                 0              78m\r\nkube-system   etcd-k8smaster                      1/1     Running                 12             78m\r\nkube-system   kube-apiserver-k8smaster            1/1     Running                 12             78m\r\nkube-system   kube-controller-manager-k8smaster   1/1     Running                 12             78m\r\nkube-system   kube-proxy-pfngx                    1/1     Running                 0              78m\r\nkube-system   kube-scheduler-k8smaster            1/1     Running                 12             78m\r\n\r\n\r\n查看日志：\r\n0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..\r\n\r\n\r\nkubelet：\r\n\r\nI0605 20:23:00.421613    5697 server.go:415] \"Kubelet version\" kubeletVersion=\"v1.27.7\"\r\nI0605 20:23:00.421677    5697 server.go:417] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nI0605 20:23:00.421895    5697 server.go:578] \"Standalone mode, no API client\"\r\nI0605 20:23:00.422011    5697 container_manager_linux.go:802] \"CPUAccounting not enabled for process\" pid=5697\r\nI0605 20:23:00.422020    5697 container_manager_linux.go:805] \"MemoryAccounting not enabled for process\" pid=5697\r\nI0605 20:23:00.426873    5697 server.go:466] \"No api server defined - no events will be sent to API server\"\r\nI0605 20:23:00.426888    5697 server.go:662] \"--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\"\r\nI0605 20:23:00.427421    5697 container_manager_linux.go:265] \"Container manager verified user specified cgroup-root exists\" cgroupRoot=[]\r\nI0605 20:23:00.427488    5697 container_manager_linux.go:270] \"Creating Container Manager object based on Node Config\" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}\r\nI0605 20:23:00.427515    5697 topology_manager.go:136] \"Creating topology manager with policy per scope\" topologyPolicyName=\"none\" topologyScopeName=\"container\"\r\nI0605 20:23:00.427529    5697 container_manager_linux.go:301] \"Creating device plugin manager\"\r\nI0605 20:23:00.427572    5697 state_mem.go:36] \"Initialized new in-memory state store\"\r\nI0605 20:23:00.431001    5697 kubelet.go:411] \"Kubelet is running in standalone mode, will skip API server sync\"\r\nI0605 20:23:00.431434    5697 kuberuntime_manager.go:257] \"Container runtime initialized\" containerRuntime=\"containerd\" version=\"v1.7.15\" apiVersion=\"v1\"\r\nI0605 20:23:00.431651    5697 volume_host.go:75] \"KubeClient is nil. Skip initialization of CSIDriverLister\"\r\nW0605 20:23:00.432240    5697 csi_plugin.go:189] kubernetes.io/csi: kubeclient not set, assuming standalone kubelet\r\nW0605 20:23:00.432259    5697 csi_plugin.go:266] Skipping CSINode initialization, kubelet running in standalone mode\r\nE0605 20:23:00.432366    5697 safe_sysctls.go:62] \"Kernel version is too old, dropping net.ipv4.ip_local_reserved_ports from safe sysctl list\" kernelVersion=\"3.10.0\"\r\nI0605 20:23:00.432500    5697 server.go:1168] \"Started kubelet\"\r\nI0605 20:23:00.432828    5697 kubelet.go:1548] \"No API server defined - no node status update will be sent\"\r\nI0605 20:23:00.432922    5697 server.go:194] \"Starting to listen read-only\" address=\"0.0.0.0\" port=10255\r\nI0605 20:23:00.434664    5697 server.go:162] \"Starting to listen\" address=\"0.0.0.0\" port=10250\r\nI0605 20:23:00.434729    5697 ratelimit.go:65] \"Setting rate limiting for podresources endpoint\" qps=100 burstTokens=10\r\nI0605 20:23:00.434909    5697 fs_resource_analyzer.go:67] \"Starting FS ResourceAnalyzer\"\r\nE0605 20:23:00.435607    5697 server.go:794] \"Failed to start healthz server\" err=\"listen tcp 127.0.0.1:10248: bind: address already in use\"\r\nE0605 20:23:00.435846    5697 cri_stats_provider.go:455] \"Failed to get the info of the filesystem with mountpoint\" err=\"unable to find data in memory cache\" mountpoint=\"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\"\r\nI0605 20:23:00.435859    5697 server.go:461] \"Adding debug handlers to kubelet server\"\r\nE0605 20:23:00.435879    5697 kubelet.go:1400] \"Image garbage collection failed once. Stats initialization may not have completed yet\" err=\"invalid capacity 0 on image filesystem\"\r\nI0605 20:23:00.436149    5697 volume_manager.go:284] \"Starting Kubelet Volume Manager\"\r\nI0605 20:23:00.436339    5697 desired_state_of_world_populator.go:145] \"Desired state populator starts to run\"\r\nE0605 20:23:00.436476    5697 server.go:179] \"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"\r\n\r\n\r\ncrictl：\r\n[root@k8smaster ~]# crictl ps\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\r\nbde21b3a0d030       9dd45d0a36c9e       About an hour ago   Running             cilium-operator           0                   b302f86dd95a2       cilium-operator-86666d88cb-dnfhx\r\na6df0160e0dde       21dd3d6f9c60d       About an hour ago   Running             kube-proxy                0                   93053c30c7029       kube-proxy-pfngx\r\nbee6716da8735       58cbecfde1998       About an hour ago   Running             kube-controller-manager   12                  e1a8c7a48c7d8       kube-controller-manager-k8smaster\r\n3a1adb4be83ec       44e520c7a8226       About an hour ago   Running             kube-apiserver            12                  40c7e5974affe       kube-apiserver-k8smaster\r\n23124a7bc7e7f       c8c40891e65bd       About an hour ago   Running             kube-scheduler            12                  d6c6d6a8e48cf       kube-scheduler-k8smaster\r\ncd68f2af5bed6       73deb9a3f7025       About an hour ago   Running             etcd                      12                  f57813da9d5f0       etcd-k8smaster\r\n\r\n\r\ncrictl images：\r\n[root@k8smaster ~]# crictl images\r\nIMAGE                                     TAG                 IMAGE ID            SIZE\r\nsealos.hub:5000/cilium/cilium             v1.13.4             d00a7abfa71a6       174MB\r\nsealos.hub:5000/cilium/operator           v1.13.4             9dd45d0a36c9e       30.8MB\r\nsealos.hub:5000/coredns/coredns           v1.10.1             ead0a4a53df89       16.2MB\r\nsealos.hub:5000/etcd                      3.5.9-0             73deb9a3f7025       103MB\r\nsealos.hub:5000/kube-apiserver            v1.27.7             44e520c7a8226       33.5MB\r\nsealos.hub:5000/kube-controller-manager   v1.27.7             58cbecfde1998       31MB\r\nsealos.hub:5000/kube-proxy                v1.27.7             21dd3d6f9c60d       23.9MB\r\nsealos.hub:5000/kube-scheduler            v1.27.7             c8c40891e65bd       18.2MB\r\nsealos.hub:5000/pause                     3.9                 e6f1816883972       319kB\r\n\r\n\r\n\r\n\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-05T01:06:00Z",
        "number": 4769,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4769",
        "login": "zhengyazhao",
        "is_bot": false
      },
      {
        "body": "版本： `v5.0.0-beta5`\r\n后端 `costcenter-frontend` 日志主要信息：\r\n\r\n```\r\nbody: {\r\n      kind: 'Status',\r\n      apiVersion: 'v1',\r\n      metadata: {},\r\n      status: 'Failure',\r\n      message: `Transfer.account.sealos.io \"5l0Lo9YkWm-1717084992510\" is invalid: metadata.name: Invalid value: \"5l0Lo9YkWm-1717084992510\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')`,\r\n      reason: 'Invalid',\r\n      details: [Object],\r\n      code: 422\r\n    },\r\n```\r\n\r\n原因分析：\r\n用户id有大写字母，导致不符合 RFC 1123",
        "closed": true,
        "closedAt": "2024-06-27T13:42:04Z",
        "number": 4766,
        "state": "CLOSED",
        "title": "Transfer error: transfer error",
        "url": "https://github.com/labring/sealos/issues/4766",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "```bash\r\ninstall.sh \\\r\n    --zh  \\\r\n    --cloud-version=v5.0.0-beta5 \\\r\n    --kubernetes-version=1.28.10 \\\r\n    --pod-cidr=100.64.0.0/10 \\\r\n    --service-cidr=192.168.0.0/16  \\\r\n    --cloud-domain=x.com  \\\r\n    --cloud-port=8443 \\\r\n    --cert-path=/root/x.com.crt  \\\r\n    --key-path=/root/x.com.key  \\\r\n    --ssh-password=x \\\r\n    --master-ips=10.0.0.50 \\\r\n    --image-registry=registry.cn-shanghai.aliyuncs.com\r\n```\r\n\r\ncloud-port=443时一切正常,cloud-port换成8443就错误了\r\n<img width=\"1111\" alt=\"image\" src=\"https://github.com/labring/sealos/assets/84728412/a4ecf39e-8fbd-46e0-83b7-6b855623289b\">\r\n\r\n以下是发生错误后获取的数据：\r\n\r\n```\r\nkubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS                            RESTARTS      AGE\r\ncert-manager                cert-manager-6dc66985d4-s52b4                                     1/1     Running                           0             6m51s\r\ncert-manager                cert-manager-cainjector-c7d4dbdd9-hhkwq                           1/1     Running                           0             6m51s\r\ncert-manager                cert-manager-webhook-847d7676c9-6c8pc                             1/1     Running                           0             6m51s\r\ncockroach-operator-system   cockroach-operator-manager-6fd5c68d58-z5ckb                       1/1     Running                           0             6m20s\r\ningress-nginx               ingress-nginx-controller-j9mk8                                    0/1     Running                           3 (18s ago)   3m40s\r\nkb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-fzc98                     1/1     Running                           0             3m31s\r\nkb-system                   kubeblocks-85ddddddd4-bfcjk                                       1/1     Running                           0             5m28s\r\nkb-system                   kubeblocks-dataprotection-7f9c76fb8f-b5xzt                        1/1     Running                           0             5m28s\r\nkube-system                 cilium-operator-774fcb74cb-x8zxx                                  1/1     Running                           0             7m21s\r\nkube-system                 cilium-wjcv9                                                      1/1     Running                           0             7m21s\r\nkube-system                 coredns-5dd5756b68-b4qg6                                          1/1     Running                           0             7m30s\r\nkube-system                 coredns-5dd5756b68-mmrdn                                          1/1     Running                           0             7m29s\r\nkube-system                 etcd-k8s-master-1                                                 1/1     Running                           1             7m53s\r\nkube-system                 kube-apiserver-k8s-master-1                                       1/1     Running                           1             7m55s\r\nkube-system                 kube-controller-manager-k8s-master-1                              1/1     Running                           1             7m55s\r\nkube-system                 kube-proxy-2st9p                                                  1/1     Running                           0             7m30s\r\nkube-system                 kube-scheduler-k8s-master-1                                       1/1     Running                           1             7m53s\r\nkube-system                 metrics-server-68f967f7dc-rp7r2                                   1/1     Running                           0             6m24s\r\nopenebs                     openebs-localpv-provisioner-56d6489bbc-jhmnl                      1/1     Running                           0             6m25s\r\nsealos                      desktop-frontend-7667c4cb97-wjwxp                                 0/1     Init:CreateContainerConfigError   0             2m25s\r\nsealos                      sealos-cockroachdb-0                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-cockroachdb-1                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-cockroachdb-2                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-mongodb-mongodb-0                                          3/3     Running                           0             3m19s\r\nvm                          victoria-metrics-k8s-stack-grafana-57c69bcd59-wp88s               3/3     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-wpnc2    1/1     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-pcxhv         1/1     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bj68sn   1/1     Running                           0             6m5s\r\nvm                          vmagent-victoria-metrics-k8s-stack-894699484-87whr                2/2     Running                           0             4m10s\r\nvm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-j68hm               2/2     Running                           0             5m57s\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running                           0             5m57s\r\nvm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-jfj56               1/1     Running                           0             5m57s\r\n```\r\n\r\n```\r\nkubectl get endpoints -A\r\nNAMESPACE                   NAME                                                   ENDPOINTS                                                             AGE\r\ncert-manager                cert-manager                                           100.64.0.198:9402                                                     10m\r\ncert-manager                cert-manager-webhook                                   100.64.0.24:10250                                                     10m\r\ncockroach-operator-system   cockroach-operator-webhook-service                     100.64.0.213:9443                                                     9m34s\r\ndefault                     kubernetes                                             10.0.0.50:6443                                                        11m\r\ningress-nginx               ingress-nginx-controller                                                                                                     8m59s\r\ningress-nginx               ingress-nginx-controller-admission                                                                                           8m59s\r\nkb-system                   kubeblocks                                             100.64.0.227:9443,100.64.0.237:9443                                   8m42s\r\nkube-system                 hubble-peer                                            10.0.0.50:4244                                                        10m\r\nkube-system                 kube-dns                                               100.64.0.106:53,100.64.0.84:53,100.64.0.106:53 + 3 more...            10m\r\nkube-system                 metrics-server                                         100.64.0.108:10250                                                    9m38s\r\nkube-system                 victoria-metrics-k8s-stack-coredns                     100.64.0.106:9153,100.64.0.84:9153                                    9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-controller-manager     10.0.0.50:10257                                                       9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-etcd                   10.0.0.50:2379                                                        9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-scheduler              10.0.0.50:10259                                                       9m19s\r\nopenebs                     openebs.io-local                                       <none>                                                                9m37s\r\nsealos                      desktop-frontend                                                                                                             5m39s\r\nsealos                      sealos-cockroachdb                                     100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s\r\nsealos                      sealos-cockroachdb-public                              100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s\r\nsealos                      sealos-mongodb-mongodb                                 100.64.0.225:27017                                                    6m34s\r\nsealos                      sealos-mongodb-mongodb-headless                        100.64.0.225:8888,100.64.0.225:50001,100.64.0.225:9216 + 2 more...    6m34s\r\nvm                          victoria-metrics-k8s-stack-grafana                     100.64.0.219:3000                                                     9m19s\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics          100.64.0.6:8080                                                       9m19s\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter    10.0.0.50:9100                                                        9m19s\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator   100.64.0.224:9443,100.64.0.224:8080                                   9m19s\r\nvm                          vmagent-victoria-metrics-k8s-stack                     100.64.0.39:8429                                                      9m8s\r\nvm                          vmalert-victoria-metrics-k8s-stack                     100.64.0.53:8080                                                      9m10s\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack              100.64.0.50:9094,100.64.0.50:9094,100.64.0.50:9093                    9m11s\r\nvm                          vmsingle-victoria-metrics-k8s-stack                    100.64.0.58:8429                                                      8m46s\r\n```\r\n\r\n```\r\nkubectl describe pod -n ingress-nginx               ingress-nginx-controller-j9mk8\r\nName:             ingress-nginx-controller-j9mk8\r\nNamespace:        ingress-nginx\r\nPriority:         0\r\nService Account:  ingress-nginx\r\nNode:             k8s-master-1/10.0.0.50\r\nStart Time:       Wed, 29 May 2024 12:32:13 -0400\r\nLabels:           app.kubernetes.io/component=controller\r\n                  app.kubernetes.io/instance=ingress-nginx\r\n                  app.kubernetes.io/managed-by=Helm\r\n                  app.kubernetes.io/name=ingress-nginx\r\n                  app.kubernetes.io/part-of=ingress-nginx\r\n                  app.kubernetes.io/version=1.9.4\r\n                  controller-revision-hash=674c9b795f\r\n                  helm.sh/chart=ingress-nginx-4.8.3\r\n                  pod-template-generation=4\r\nAnnotations:      <none>\r\nStatus:           Running\r\nIP:               10.0.0.50\r\nIPs:\r\n  IP:           10.0.0.50\r\nControlled By:  DaemonSet/ingress-nginx-controller\r\nContainers:\r\n  controller:\r\n    Container ID:  containerd://7ce271a4ac42f9c9065f6b17ebba031b05ebf43e09086c30ddd2b52944286f63\r\n    Image:         registry.k8s.io/ingress-nginx/controller:v1.9.4\r\n    Image ID:      sealos.hub:5000/ingress-nginx/controller@sha256:0115d7e01987c13e1be90b09c223c3e0d8e9a92e97c0421e712ad3577e2d78e5\r\n    Ports:         80/TCP, 443/TCP, 8443/TCP\r\n    Host Ports:    80/TCP, 443/TCP, 8443/TCP\r\n    Args:\r\n      /nginx-ingress-controller\r\n      --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\r\n      --election-id=ingress-nginx-leader\r\n      --controller-class=k8s.io/ingress-nginx\r\n      --ingress-class=nginx\r\n      --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\r\n      --validating-webhook=:8443\r\n      --validating-webhook-certificate=/usr/local/certificates/cert\r\n      --validating-webhook-key=/usr/local/certificates/key\r\n      --https-port=8443\r\n      --default-ssl-certificate=sealos-system/wildcard-cert\r\n    State:          Running\r\n      Started:      Wed, 29 May 2024 12:44:15 -0400\r\n    Last State:     Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 29 May 2024 12:40:27 -0400\r\n      Finished:     Wed, 29 May 2024 12:41:35 -0400\r\n    Ready:          False\r\n    Restart Count:  7\r\n    Requests:\r\n      cpu:      100m\r\n      memory:   90Mi\r\n    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=5\r\n    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      POD_NAME:       ingress-nginx-controller-j9mk8 (v1:metadata.name)\r\n      POD_NAMESPACE:  ingress-nginx (v1:metadata.namespace)\r\n      LD_PRELOAD:     /usr/local/lib/libmimalloc.so\r\n    Mounts:\r\n      /usr/local/certificates/ from webhook-cert (ro)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bsr9t (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  webhook-cert:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  ingress-nginx-admission\r\n    Optional:    false\r\n  kube-api-access-bsr9t:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              kubernetes.io/os=linux\r\nTolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists\r\n                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\r\n                             node.kubernetes.io/not-ready:NoExecute op=Exists\r\n                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists\r\n                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\r\nEvents:\r\n  Type     Reason     Age                     From                      Message\r\n  ----     ------     ----                    ----                      -------\r\n  Normal   Scheduled  12m                     default-scheduler         Successfully assigned ingress-nginx/ingress-nginx-controller-j9mk8 to k8s-master-1\r\n  Normal   RELOAD     12m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   Killing    11m                     kubelet                   Container controller failed liveness probe, will be restarted\r\n  Normal   Pulled     11m (x2 over 12m)       kubelet                   Container image \"registry.k8s.io/ingress-nginx/controller:v1.9.4\" already present on machine\r\n  Normal   Created    11m (x2 over 12m)       kubelet                   Created container controller\r\n  Normal   Started    11m (x2 over 12m)       kubelet                   Started container controller\r\n  Normal   RELOAD     11m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  Unhealthy  10m (x8 over 12m)       kubelet                   Liveness probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   RELOAD     10m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     8m57s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     7m47s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  Unhealthy  7m9s (x31 over 12m)     kubelet                   Readiness probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   RELOAD     6m37s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     4m5s                    nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  BackOff    2m17s (x13 over 5m28s)  kubelet                   Back-off restarting failed container controller in pod ingress-nginx-controller-j9mk8_ingress-nginx(f0a3d63a-6bca-4a43-b54a-120eca26069c)\r\n  Normal   RELOAD     16s                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n```",
        "closed": true,
        "closedAt": "2024-06-26T08:32:15Z",
        "number": 4764,
        "state": "CLOSED",
        "title": "cloud-port=8443时安装cloud会发生错误 `no endpoints available for service \"ingress-noinx-controller-admission\"`",
        "url": "https://github.com/labring/sealos/issues/4764",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L138\nhttps://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L147\n这两处获取到的值外面会有一层双引号，导致后续使用变量的时候造成严重的错误",
        "closed": true,
        "closedAt": "2024-05-29T07:01:03Z",
        "number": 4760,
        "state": "CLOSED",
        "title": "deploy/cloud/scripts/init.sh The wrong format of the variable causes the --env parameter parsing to fail.",
        "url": "https://github.com/labring/sealos/issues/4760",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "# Issue: 新增繁體中文語言支援\r\n\r\n目前，應用程式不支援繁體中文，這限制了那些更習慣使用繁體中文的用戶的可用性。\r\n\r\n## 如果你有解決方案，請描述一下\r\n加入社區自願翻譯，提交到github\r\n\r\n",
        "closed": true,
        "closedAt": "2024-09-27T06:54:11Z",
        "number": 4759,
        "state": "CLOSED",
        "title": "新增繁體中文語言支援",
        "url": "https://github.com/labring/sealos/issues/4759",
        "login": "TW199501",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sealos-admin",
    "issues": [
      {
        "body": "https://github.com/labring/sealos-admin/actions/runs/17451681116/job/49558437160 主要解决这个问题",
        "closed": true,
        "closedAt": "2025-09-04T04:10:00Z",
        "number": 35,
        "state": "CLOSED",
        "title": "${{ github.sha }} 太长，需要和 docker镜像保持一致",
        "url": "https://github.com/labring/sealos-admin/issues/35",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. 拆分双架构 amd64 使用runner是 buildjet-2vcpu-ubuntu-2204 而 arm64使用runner是 buildjet-4vcpu-ubuntu-2204-arm\n2. 仅限容器镜像版本，集群镜像无需修改。",
        "closed": true,
        "closedAt": "2025-09-04T03:30:56Z",
        "number": 33,
        "state": "CLOSED",
        "title": "ai: 拆分双架构",
        "url": "https://github.com/labring/sealos-admin/issues/33",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-04T01:59:14Z",
        "number": 31,
        "state": "CLOSED",
        "title": "workflow 添加保存tar包的流程",
        "url": "https://github.com/labring/sealos-admin/issues/31",
        "login": "cuisongliu",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sealos-pro",
    "issues": [
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 63,
        "state": "OPEN",
        "title": "插件化devbox sql",
        "url": "https://github.com/labring/sealos-pro/issues/63",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Error: buildx failed with: ERROR: failed to build: failed to solve: bitnami/git: failed to resolve source metadata for docker.io/bitnami/git:latest: docker.io/bitnami/git:latest: not found",
        "closed": true,
        "closedAt": "2025-10-01T04:23:59Z",
        "number": 62,
        "state": "CLOSED",
        "title": "模版仓库bitnami问题",
        "url": "https://github.com/labring/sealos-pro/issues/62",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n{\n\tname:                 \"ipv4 cidr too big\",\n\tcidr:                 *getIPnetFromCIDR(\"10.92.0.0/8\"),\n\tmaxCIDRBits:          20,\n\tcidrFlag:             \"--service-cluster-ip-range\",\n\texpectedErrorMessage: \"specified --service-cluster-ip-range is too large; for 32-bit addresses, the mask must be >= 12\",\n\texpectErrors:         true,\n},\n{\n\tname:                 \"ipv6 cidr too big\",\n\tcidr:                 *getIPnetFromCIDR(\"3000::/64\"),\n\tmaxCIDRBits:          20,\n\tcidrFlag:             \"--service-cluster-ip-range\",\n\texpectedErrorMessage: \"specified --service-cluster-ip-range is too large; for 128-bit addresses, the mask must be >= 108\",\n\texpectErrors:         true,\n},\n```",
        "closed": false,
        "closedAt": null,
        "number": 61,
        "state": "OPEN",
        "title": "支持 IPV6",
        "url": "https://github.com/labring/sealos-pro/issues/61",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-10-09T04:15:27Z",
        "number": 60,
        "state": "CLOSED",
        "title": "深信服内核检测不通过",
        "url": "https://github.com/labring/sealos-pro/issues/60",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\netcd:\n  local:\n    dataDir: {{ default \"/var/lib/etcd\" .KUBEADM_ETCD_DATA }}\n\n```",
        "closed": true,
        "closedAt": "2025-09-29T02:05:03Z",
        "number": 58,
        "state": "CLOSED",
        "title": "etc 支持自定义目录",
        "url": "https://github.com/labring/sealos-pro/issues/58",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "目前固定的 需要做到随机密码",
        "closed": true,
        "closedAt": "2025-10-05T16:09:41Z",
        "number": 57,
        "state": "CLOSED",
        "title": "admin 默认密码随机",
        "url": "https://github.com/labring/sealos-pro/issues/57",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"1450\" height=\"410\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9be0e69a-c714-4bee-bddb-24027f4ea93b\" />",
        "closed": false,
        "closedAt": null,
        "number": 56,
        "state": "OPEN",
        "title": "rootfs的联系方式变更",
        "url": "https://github.com/labring/sealos-pro/issues/56",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\ndb.properties.insertMany(\n    [{\n      \"name\": \"cpu\",\n      \"alias\": \"\",\n      \"enum\": 0,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 2.237442922,\n      \"encrypt_unit_price\": \"iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl\",\n      \"unit\": \"1m\"\n    },{\n      \"name\": \"memory\",\n      \"alias\": \"\",\n      \"enum\": 1,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 1.092501427,\n      \"encrypt_unit_price\": \"OSE6JKZaoZTdcNQsUA0GzdrObISwNrugpAkShRcq22DaOYpchmxf\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"storage\",\n      \"alias\": \"\",\n      \"enum\": 2,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 0,\n      \"encrypt_unit_price\": \"DPo+4f6FksbJ0k0g6SJifQ+PXz+rhUl/W/MahCc=\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"services.nodeports\",\n      \"alias\": \"\",\n      \"enum\": 4,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 500,\n      \"encrypt_unit_price\": \"OfHNWknr4vDHM+Amy6EfvqrSGjF+YIfZ9yBlBwp6vw==\",\n      \"unit\": \"1\",\n      \"view_price\": 500000\n    },{\n      \"name\": \"network\",\n      \"alias\": \"\",\n      \"enum\": 3,\n      \"price_type\": \"DIF\",\n      \"unit_price\": 781,\n      \"encrypt_unit_price\": \"4fGwqMywAqV/2O9ED/XB6udBdsqVKS5Y75rfzzW8pw==\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"gpu-NVIDIA-GeForce-RTX-4090\",\n      \"alias\": \"RTX-4090\",\n      \"enum\": 5,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 2.237442922,\n      \"encrypt_unit_price\": \"iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl\",\n      \"unit\": \"1m\"\n    }]\n)\n```\n```\n# 获取 mongodb 数据库的名字\nkubectl get cluster -n sealos\n# kbcli 连接 mongodb 数据库\nkbcli cluster connect sealos-mongodb -n sealos\n\nsealos-mongodb-mongodb [primary] admin> show dbs\nsealos-mongodb-mongodb [primary] admin> use sealos-resources\nsealos-mongodb-mongodb [primary] admin> show collections\n\n# 重启 account service 和 account controller\nkubectl rollout restart deploy account-controller-manager -n account-system\nkubectl rollout restart deploy account-service -n account-system\n```",
        "closed": true,
        "closedAt": "2025-09-27T07:06:05Z",
        "number": 52,
        "state": "CLOSED",
        "title": "费用支持",
        "url": "https://github.com/labring/sealos-pro/issues/52",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. 只有 cc 用到了,只是纳管起来而已\n2. 控制定时开关机的\n\nhttps://github.com/labring/sealos/pull/5559",
        "closed": false,
        "closedAt": null,
        "number": 49,
        "state": "OPEN",
        "title": "devboxscheduler",
        "url": "https://github.com/labring/sealos-pro/issues/49",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 48,
        "state": "OPEN",
        "title": "sealos add 需要支持 env 参数",
        "url": "https://github.com/labring/sealos-pro/issues/48",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/LuyFw89BwiL2IpkzCcxcwsLonzg?from=from_copylink",
        "closed": false,
        "closedAt": null,
        "number": 47,
        "state": "OPEN",
        "title": "支持GPU",
        "url": "https://github.com/labring/sealos-pro/issues/47",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 46,
        "state": "OPEN",
        "title": "修改containerd的版本号 区分上游版本和自己更改的版本",
        "url": "https://github.com/labring/sealos-pro/issues/46",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "apiserver\n```\nservice-cluster-ip-range: \"10.96.0.0/16\" # 重要！如果安装时未调整\nmax-requests-inflight: \"1000\"\nmax-mutating-requests-inflight: \"1000\"\n```\n\n\ncontroller-manager\n\n```\nconcurrent-deployment-syncs: \"100\"\nconcurrent-endpoint-syncs: \"50\"\nconcurrent-gc-syncs: \"100\"\nconcurrent-namespace-syncs: \"100\"\nconcurrent-rc-syncs: \"100\"\nconcurrent-replicaset-syncs: \"100\"\nconcurrent-service-endpoint-syncs: \"50\"\nconcurrent-service-syncs: \"100\"\nkube-api-burst: \"100\"\nkube-api-qps: \"100\"\n```",
        "closed": true,
        "closedAt": "2025-09-24T11:30:00Z",
        "number": 45,
        "state": "CLOSED",
        "title": "参数调优",
        "url": "https://github.com/labring/sealos-pro/issues/45",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"2332\" height=\"632\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f5e5d73b-d0b2-4b46-98fc-e7ed8b6b3c76\" />",
        "closed": true,
        "closedAt": "2025-09-24T06:51:35Z",
        "number": 40,
        "state": "CLOSED",
        "title": "labring4docker 的 curl-kubectl 镜像丢失",
        "url": "https://github.com/labring/sealos-pro/issues/40",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cockroachdb-backup\n  namespace: cockroach-operator-system\nspec:\n  concurrencyPolicy: Forbid\n  failedJobsHistoryLimit: 2\n  jobTemplate:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: cockroachdb-backup\n        job-type: scheduled\n    spec:\n      activeDeadlineSeconds: 1200\n      backoffLimit: 1\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: cockroachdb-backup\n        spec:\n          containers:\n          - command:\n            - /bin/bash\n            - -c\n            - |\n              set -e\n              \n              echo \"=== CockroachDB 每日备份开始 ===\"\n              echo \"备份开始时间（UTC）: $(date -u)\"\n              echo \"备份开始时间（北京）: $(TZ='Asia/Shanghai' date)\"\n              echo \"定期备份: 每天北京时间0点执行\"\n              \n              # 从共享文件读取桶名\n              LOCAL_BUCKET=$(cat /shared/local_bucket)\n              GLOBAL_BUCKET=$(cat /shared/global_bucket)\n              BACKUP_DATE=$(cat /shared/timestamp)\n              \n              echo \"每日备份配置:\"\n              echo \"  执行计划: 每天北京时间0点（UTC 16点）\"\n              echo \"  备份日期: $BACKUP_DATE\"\n              echo \"  本地桶: $LOCAL_BUCKET\"\n              echo \"  全局桶: $GLOBAL_BUCKET\"\n              echo \"  MinIO端点: $MINIO_ENDPOINT\"\n              echo \"  备份策略: 当前时间点全量备份\"\n              \n              # 首先检查GC阈值，不指定具体时间点进行当前时间备份\n              echo \"=== 检查数据库状态 ===\"\n              \n              # 函数：获取可用的备份时间点\n              get_safe_backup_time() {\n                local pod_name=$1\n                local certs_dir=$2\n                echo \"检查 $pod_name 的GC阈值...\"\n                \n                # 查询GC阈值\n                local gc_query=\"SELECT cluster_logical_timestamp() - (25 * 60 * 60 * 1000000000)::INT8 AS safe_timestamp;\"\n                \n                kubectl exec -n cockroach-operator-system \"$pod_name\" -- \\\n                  cockroach sql --certs-dir=\"$certs_dir\" --host=localhost:26257 \\\n                  --execute=\"$gc_query\" --format=csv | tail -n +2\n              }\n              \n              # 构建备份命令（不使用AS OF SYSTEM TIME，备份当前时间点）\n              GLOBAL_BACKUP_CMD=\"BACKUP TO 's3://${GLOBAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';\"\n              \n              LOCAL_BACKUP_CMD=\"BACKUP TO 's3://${LOCAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';\"\n              \n              # 执行全局备份\n              echo \"=== 开始全局备份 ===\"\n              GLOBAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb-global | grep Running | head -n1 | awk '{print $1}')\n              \n              if [ -n \"$GLOBAL_POD\" ] && [ \"$GLOBAL_POD\" != \"\" ]; then\n                echo \"找到全局节点: $GLOBAL_POD\"\n                \n                # 检查GC阈值\n                echo \"检查全局节点GC状态...\"\n                GLOBAL_SAFE_TIME=$(get_safe_backup_time \"$GLOBAL_POD\" \"/cockroach/data/certs\" || echo \"\")\n                \n                if [ -n \"$GLOBAL_SAFE_TIME\" ]; then\n                  echo \"全局节点安全时间戳: $GLOBAL_SAFE_TIME\"\n                fi\n                \n                echo \"执行全局备份命令（当前时间点）...\"\n                \n                if kubectl exec -n cockroach-operator-system \"$GLOBAL_POD\" -- \\\n                   cockroach sql --certs-dir=/cockroach/data/certs --host=localhost:26257 \\\n                   --execute \"$GLOBAL_BACKUP_CMD\"; then\n                  echo \"✓ 全局备份成功\"\n                else\n                  echo \"✗ 全局备份失败，但继续执行本地备份\"\n                fi\n              else\n                echo \"⚠ 未找到运行中的全局节点，跳过全局备份\"\n              fi\n              \n              # 执行本地备份\n              echo \"=== 开始本地备份 ===\"\n              LOCAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb | grep -v global | grep Running | head -n1 | awk '{print $1}')\n              \n              if [ -n \"$LOCAL_POD\" ] && [ \"$LOCAL_POD\" != \"\" ]; then\n                echo \"找到本地节点: $LOCAL_POD\"\n                \n                # 检查GC阈值\n                echo \"检查本地节点GC状态...\"\n                LOCAL_SAFE_TIME=$(get_safe_backup_time \"$LOCAL_POD\" \"/cockroach/cockroach-certs\" || echo \"\")\n                \n                if [ -n \"$LOCAL_SAFE_TIME\" ]; then\n                  echo \"本地节点安全时间戳: $LOCAL_SAFE_TIME\"\n                fi\n                \n                echo \"执行本地备份命令（当前时间点）...\"\n                \n                # 指定容器名称避免歧义\n                if kubectl exec -n cockroach-operator-system \"$LOCAL_POD\" -c db -- \\\n                   cockroach sql --certs-dir=/cockroach/cockroach-certs --host=localhost:26257 \\\n                   --execute \"$LOCAL_BACKUP_CMD\"; then\n                  echo \"✓ 本地备份成功\"\n                else\n                  echo \"✗ 本地备份失败\"\n                  exit 1\n                fi\n              else\n                echo \"✗ 未找到运行中的本地节点\"\n                exit 1\n              fi\n              \n              echo \"=== 备份验证 ===\"\n              echo \"验证备份文件...\"\n              \n              # 记录备份元数据\n              echo \"备份完成时间（UTC）: $(date -u)\" > /shared/backup_result\n              echo \"备份完成时间（北京）: $(TZ='Asia/Shanghai' date)\" >> /shared/backup_result\n              echo \"备份日期: $BACKUP_DATE\" >> /shared/backup_result\n              echo \"执行计划: 每天北京时间0点\" >> /shared/backup_result\n              echo \"全局备份桶: $GLOBAL_BUCKET\" >> /shared/backup_result\n              echo \"本地备份桶: $LOCAL_BUCKET\" >> /shared/backup_result\n              echo \"备份类型: 当前时间点备份\" >> /shared/backup_result\n              \n              echo \"=== 每日备份流程完成 ===\"\n            env:\n            - name: MINIO_ENDPOINT\n              value: https://objectstorageapi.hzh.sealos.run\n            - name: MINIO_ACCESS_KEY\n              value: xxx\n            - name: MINIO_SECRET_KEY\n              value: ffff\n            image: bitnami/kubectl:latest\n            imagePullPolicy: Always\n            name: cockroachdb-backup\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: 2Gi\n              requests:\n                cpu: 500m\n                memory: 1Gi\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - mountPath: /shared\n              name: shared-data\n          dnsPolicy: ClusterFirst\n          initContainers:\n          - command:\n            - /bin/bash\n            - -c\n            - |\n              set -e\n              \n              # 生成时间戳和随机字符串\n              TIMESTAMP=$(date +'%Y%m%d-%H%M')\n              RANDOM_STR=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 6 | head -n 1)\n              \n              export LOCAL_BUCKET=\"local-${TIMESTAMP}-${RANDOM_STR}\"\n              export GLOBAL_BUCKET=\"global-${TIMESTAMP}-${RANDOM_STR}\"\n              \n              echo \"=== 每日备份准备阶段 ===\"\n              echo \"当前UTC时间: $(date -u)\"\n              echo \"当前北京时间: $(TZ='Asia/Shanghai' date)\"\n              echo \"备份执行时间: 北京时间每天0点\"\n              echo \"备份日期: $TIMESTAMP\"\n              echo \"随机标识: $RANDOM_STR\"\n              echo \"本地备份桶: $LOCAL_BUCKET\"\n              echo \"全局备份桶: $GLOBAL_BUCKET\"\n              \n              # 配置 MinIO 连接\n              echo \"配置 MinIO 连接...\"\n              mc alias set backup \"${MINIO_ENDPOINT}\" \"${MINIO_ACCESS_KEY}\" \"${MINIO_SECRET_KEY}\"\n              \n              # 测试连接\n              echo \"测试 MinIO 连接...\"\n              mc admin info backup\n              \n              # 创建存储桶\n              echo \"创建存储桶...\"\n              mc mb \"backup/${LOCAL_BUCKET}\" || echo \"本地桶创建失败或已存在\"\n              mc mb \"backup/${GLOBAL_BUCKET}\" || echo \"全局桶创建失败或已存在\"\n              \n              # 将桶名写入共享文件\n              echo \"$LOCAL_BUCKET\" > /shared/local_bucket\n              echo \"$GLOBAL_BUCKET\" > /shared/global_bucket\n              echo \"$TIMESTAMP\" > /shared/timestamp\n              \n              echo \"=== 准备阶段完成 ===\"\n            env:\n            - name: MINIO_ENDPOINT\n              value: https://objectstorageapi.hzh.sealos.run\n            - name: MINIO_ACCESS_KEY\n              value: xxx\n            - name: MINIO_SECRET_KEY\n              value: ffff\n            image: bitnami/minio-client:latest\n            imagePullPolicy: Always\n            name: prepare-backup\n            resources:\n              limits:\n                cpu: 200m\n                memory: 256Mi\n              requests:\n                cpu: 100m\n                memory: 128Mi\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - mountPath: /shared\n              name: shared-data\n          restartPolicy: Never\n          schedulerName: default-scheduler\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n          serviceAccount: cockroachdb-backup-sa\n          serviceAccountName: cockroachdb-backup-sa\n          terminationGracePeriodSeconds: 60\n          volumes:\n          - emptyDir: {}\n            name: shared-data\n  schedule: 0 16 * * *\n  startingDeadlineSeconds: 1800\n  successfulJobsHistoryLimit: 5\n  suspend: false\nstatus:\n  lastScheduleTime: \"2025-09-23T08:00:00Z\"\n  lastSuccessfulTime: \"2025-09-23T08:01:51Z\"\n```",
        "closed": false,
        "closedAt": null,
        "number": 39,
        "state": "OPEN",
        "title": "小强数据库备份",
        "url": "https://github.com/labring/sealos-pro/issues/39",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-23T14:24:54Z",
        "number": 38,
        "state": "CLOSED",
        "title": "sealos-enterprise 的 payment不可重入",
        "url": "https://github.com/labring/sealos-pro/issues/38",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "kubectl edit cm -n costcenter-frontend   costcenter-frontend-config\n",
        "closed": true,
        "closedAt": "2025-09-23T14:26:15Z",
        "number": 37,
        "state": "CLOSED",
        "title": "前端没有开启payment",
        "url": "https://github.com/labring/sealos-pro/issues/37",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"1511\" height=\"555\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/af8ae4a9-57be-49d6-a320-d60d7af7fbcc\" />\n\n\n<img width=\"1106\" height=\"92\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/63b26e4b-796f-4c08-9711-e312cf2bc8a9\" />",
        "closed": false,
        "closedAt": null,
        "number": 35,
        "state": "OPEN",
        "title": "admin 的集群监控",
        "url": "https://github.com/labring/sealos-pro/issues/35",
        "login": "nowinkeyy",
        "is_bot": false
      },
      {
        "body": "如果告警使用grafana，则需要关闭alertmanager。详情参见\nhttps://fael3z0zfze.feishu.cn/wiki/HAW5wmKvvioB7IkrljVc88Xennd",
        "closed": true,
        "closedAt": "2025-09-23T11:19:30Z",
        "number": 33,
        "state": "CLOSED",
        "title": "vm关闭alertmanager",
        "url": "https://github.com/labring/sealos-pro/issues/33",
        "login": "bearslyricattack",
        "is_bot": false
      },
      {
        "body": "由于代码依赖还没处理完，且中短期内可能有 refactor 的需求，目前流量模块不存在 labring/sealos 中，而是在 https://github.com/dinoallo/sealos-nm-agent/ ，目前部署方式为集群镜像：\n\n```bash\nsealos run docker.io/dinoallo/sealos-nm-agent:v0.0.0-alpha -e \"NODE_CIDR=172.16.0.0/24\" -e \"POD_CIDR=100.64.0.0/10\" -e \"HOST_DEVS=eth0,eth1\"\n```\n其中：\n- `NODE_CIDR` ：为节点 IP 段\n- `POD_CIDR` ：为 POD IP 段\n- `HOST_DEVS` ：为节点网卡设备名称，可以指定多个（不存在跳过）",
        "closed": false,
        "closedAt": null,
        "number": 32,
        "state": "OPEN",
        "title": "Cost Center 流量模块",
        "url": "https://github.com/labring/sealos-pro/issues/32",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/QDeBw7AphiQb3sk7vGzcZPkFndg",
        "closed": true,
        "closedAt": "2025-09-29T01:56:35Z",
        "number": 31,
        "state": "CLOSED",
        "title": "2025 Q3 Devbox & Containerd 性能优化",
        "url": "https://github.com/labring/sealos-pro/issues/31",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf\n部署参考：https://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf ，目前只有镜像。",
        "closed": false,
        "closedAt": null,
        "number": 30,
        "state": "OPEN",
        "title": "2025 Q2 Higress 性能优化",
        "url": "https://github.com/labring/sealos-pro/issues/30",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/Uwljwf9buiQSLDkTDKxc117Oncg",
        "closed": false,
        "closedAt": null,
        "number": 29,
        "state": "OPEN",
        "title": "etcd event 拆分",
        "url": "https://github.com/labring/sealos-pro/issues/29",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/CBAowr3MaigLuAkrwHEcXQ9Unqx",
        "closed": false,
        "closedAt": null,
        "number": 28,
        "state": "OPEN",
        "title": "20250402 连接冲突导致 kubelet 无法连接 api server",
        "url": "https://github.com/labring/sealos-pro/issues/28",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "| Controller项目名称                    | 所属版本            | 负责人 |\n| ------------------------------------- | ------------------- | ------ |\n| sealos-cloud-account-controller       | all                 |        |\n| sealos-cloud-app-controller           | all                 |        |\n| sealos-cloud-db-adminer-controller    | 已经废弃            |        |\n| sealos-cloud-devbox-controller        | 商业&&公有云        |        |\n| sealos-cloud-job-controller           | all                 |        |\n| sealos-cloud-job-heartbeat-controller | all                 |        |\n| sealos-cloud-license-controller       | 商业版                 |        |\n| sealos-cloud-node-controller          | 商业&&公有云【GPU，暂未加入】 | 子毅       |\n| sealos-cloud-objectstorage-controller | 商业&&公有云        |        |\n| sealos-cloud-resources-controller     | all                 |        |\n| sealos-cloud-terminal-controller     | all                 |        |\n| sealos-cloud-user-controller     | all                 |        |\n---\n\n\n| Frontend项目名称                    | 所属版本 | 负责人 |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-desktop-frontend | all |        |\n| sealos-cloud-adminer-frontend | 已经废弃 |        |\n| sealos-cloud-aiproxy-frontend | 公有云【暂未加入】 |        |\n| sealos-cloud-applaunchpad-frontend | all |        |\n| sealos-cloud-cloudserver-frontend | 已经废弃【只有一个可用区使用】 |        |\n| sealos-cloud-costcenter-frontend | all |        |\n| sealos-cloud-cronjob-frontend | all |        |\n| sealos-cloud-dbprovider-frontend | all |        |\n| sealos-cloud-devbox-frontend | 商业&&公有云 |        |\n| sealos-cloud-invite-frontend | 公有云【邀请注册，暂未加入】 | |\n| sealos-cloud-kubepanel-frontend | 商业&&公有云 | |\n| sealos-cloud-license-frontend | 商业版 | |\n| sealos-cloud-objectstorage-frontend | 商业&&公有云 | |\n| sealos-cloud-switch-region-frontend | 公有云【region切换，暂未加入】 | |\n| sealos-cloud-template-frontend | all | |\n| sealos-cloud-terminal-frontend | all | |\n| sealos-cloud-workorder-frontend | 公有云【工单，暂未加入】 | |\n\n\n---\n\n| Service项目名称                    | 所属版本 | 负责人 |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-account-service       | all |        |\n| sealos-cloud-database-service       | all |        |\n| sealos-cloud-devbox-service       | 商业&&公有云 |        |\n| sealos-cloud-exceptionmonitor-service       | 公有云【数据库告警，暂未加入】 |        |\n| sealos-cloud-hubble-service       | 公有云【流量相关】 |        |\n| sealos-cloud-launchpad-service       | all |        |\n| sealos-cloud-license-service       | 公有云【https://license.sealos.io】 |        |\n| sealos-cloud-minio-service       | 商业&&公有云 |        |\n| sealos-cloud-pay-service       | 未上线 |        |\n| sealos-cloud-vlogs-service       | 商业&&公有云 |        |\n\n---\n\n| Webhook项目名称                    | 所属版本 | 负责人 |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-admission-webhook       | 公有云【域名劫持】 |  yy      |\n\n\n\n---\n| 其他项目名称                    | 功能说明 | 负责人 |\n| ------------------------------------- | -------- | ------ |\n| region-exporter【user-exporter】 | 可用区监控 | 佳辉 |\n| region-exporter【lvm-restore-exporter】 | lvm监控 + 后台监控pvc跟 lvm lv 不一致的进行恢复 | 佳辉 |\n| region-exporter【bi-info】 | 运营导出数据库 | 佳辉 |\n| https://github.com/bearslyricattack/CompliK | 挖矿检测 | 鹏宇 |\n| [chart2db](https://fael3z0zfze.feishu.cn/docx/O7H3dUE7EoFa3Fxn2DrcjgpGnig)  |  | 金虎 |\n| https://github.com/zijiren233/image-monitor | [镜像监控](https://github.com/user-attachments/files/22375128/Pod.-1758077188997.json) | 仪豪 |\n| https://github.com/zijiren233/node-zombie-detector | 节点监控告警 | 仪豪 |\n| cleanup-deleting-clusters | 数据库修复 | 金虎 |\n| cleanup-failed-opsrequests | 数据库修复 | 金虎 |\n| cleanup-succeeded-opsrequests | 数据库修复 | 金虎 |\n| clear-mongo-logs | 数据库修复 | 金虎 |\n| delete-pg-logs | 数据库修复 | 金虎 |\n| set-pg-failsafe-mode | 数据库修复 | 金虎 |\n| [sealos-admin](https://github.com/labring/sealos-admin)| 管理员模块 | 子毅 |\n| https://github.com/zijiren233/higress/tree/limit-whitelist| higress 插件| 仪豪 |\n| [deschduler](https://fael3z0zfze.feishu.cn/wiki/RnIXw0V6Ri0EDYkv4NLcooBAnYf)|deschduler组件，视情况定|仪豪 |\n| https://github.com/dinoallo/sealos-nm-agent/ |流量模块|潘云 |\n| https://github.com/labring/aiproxy/ |aiproxy|仪豪 |\n\n\n\n",
        "closed": false,
        "closedAt": null,
        "number": 27,
        "state": "OPEN",
        "title": "sealos项目结构统计",
        "url": "https://github.com/labring/sealos-pro/issues/27",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "![Image](https://github.com/user-attachments/assets/4ac1ca3e-810f-4fde-aa14-b1a21dd74e26)\n\n\n![Image](https://github.com/user-attachments/assets/7b23a9a5-b4ab-4bff-8d9c-cc33a4ae233a)",
        "closed": true,
        "closedAt": "2025-09-19T13:06:49Z",
        "number": 26,
        "state": "CLOSED",
        "title": "openebs 默认的优先级太低了",
        "url": "https://github.com/labring/sealos-pro/issues/26",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: region-exporter\n  namespace: account-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: region-exporter\n  template:\n    metadata:\n      labels:\n        app: region-exporter\n    spec:\n      serviceAccountName: account-controller-manager\n      containers:\n        - name: region-exporter\n          image: [registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531](http://registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531)\n          imagePullPolicy: Always\n          ports:\n            - name: metrics\n              containerPort: 8000\n          envFrom:\n            - configMapRef:\n                name: account-manager-env\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: region-exporter\n  namespace: account-system\n  labels:\n    app: region-exporter\nspec:\n  selector:\n    app: region-exporter\n  ports:\n    - protocol: TCP\n      port: 8000\n      targetPort: metrics\n      name: metrics\n---\napiVersion: [networking.k8s.io/v1](http://networking.k8s.io/v1)\nkind: Ingress\nmetadata:\n  name: region-exporter\n  namespace: account-system\n  annotations:\n    [kubernetes.io/ingress.class](http://kubernetes.io/ingress.class): nginx\n    [nginx.ingress.kubernetes.io/proxy-body-size](http://nginx.ingress.kubernetes.io/proxy-body-size): 32m\n    [nginx.ingress.kubernetes.io/server-snippet](http://nginx.ingress.kubernetes.io/server-snippet): |\n      client_header_buffer_size 64k;\n      large_client_header_buffers 4 128k;\n    [nginx.ingress.kubernetes.io/ssl-redirect](http://nginx.ingress.kubernetes.io/ssl-redirect): 'false'\n    [nginx.ingress.kubernetes.io/backend-protocol](http://nginx.ingress.kubernetes.io/backend-protocol): HTTP\n    [nginx.ingress.kubernetes.io/rewrite-target](http://nginx.ingress.kubernetes.io/rewrite-target): /$2\n    [nginx.ingress.kubernetes.io/client-body-buffer-size](http://nginx.ingress.kubernetes.io/client-body-buffer-size): 64k\n    [nginx.ingress.kubernetes.io/proxy-buffer-size](http://nginx.ingress.kubernetes.io/proxy-buffer-size): 64k\n    [nginx.ingress.kubernetes.io/proxy-send-timeout](http://nginx.ingress.kubernetes.io/proxy-send-timeout): '300'\n    [nginx.ingress.kubernetes.io/proxy-read-timeout](http://nginx.ingress.kubernetes.io/proxy-read-timeout): '300'\n    [nginx.ingress.kubernetes.io/configuration-snippet](http://nginx.ingress.kubernetes.io/configuration-snippet): |\n      if ($request_uri ~* \\.(js|css|gif|jpe?g|png)) {\n        expires 30d;\n        add_header Cache-Control \"public\";\n      }\nspec:\n  rules:\n    - host: xxx\n      http:\n        paths:\n          - pathType: Prefix\n            path: /()(.*)\n            backend:\n              service:\n                name: region-exporter\n                port:\n                  number: 8000\n  tls:\n    - hosts:\n        - xxx\n      secretName: wildcard-cert\n---\n#apiVersion: [monitoring.coreos.com/v1](http://monitoring.coreos.com/v1)\n#kind: ServiceMonitor\n#metadata:\n#  name: region-exporter\n#  namespace: account-system\n#  labels:\n#    release: prometheus\n#spec:\n#  selector:\n#    matchLabels:\n#      app: crd-exporter\n#  endpoints:\n#    - port: metrics\n#      interval: 300s\n```",
        "closed": false,
        "closedAt": null,
        "number": 25,
        "state": "OPEN",
        "title": "公有云-监控",
        "url": "https://github.com/labring/sealos-pro/issues/25",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: v1\nitems:\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"extensions.higress.io/v1alpha1\",\"kind\":\"WasmPlugin\",\"metadata\":{\"annotations\":{},\"name\":\"block-vite-fs\",\"namespace\":\"higress-system\"},\"spec\":{\"defaultConfig\":{\"block_regexp_urls\":[\"^.*/@fs/.*\\\\?(?:.*\\u0026)?(?:raw|import\\u0026raw)\\\\?\\\\?.*$\"]},\"matchRules\":null,\"url\":\"oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0\"}}\n    creationTimestamp: \"2025-06-16T07:09:07Z\"\n    generation: 1\n    name: block-vite-fs\n    namespace: higress-system\n    resourceVersion: \"3870988733\"\n    uid: c2ef1fe5-0c66-47fe-bcac-1897e8c7fb76\n  spec:\n    defaultConfig:\n      block_regexp_urls:\n      - ^.*/@fs/.*\\?(?:.*&)?(?:raw|import&raw)\\?\\?.*$\n    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    creationTimestamp: \"2025-01-23T04:27:44Z\"\n    generation: 3\n    name: ip-restriction-aiproxy\n    namespace: higress-system\n    resourceVersion: \"1888196182\"\n    uid: 3edd6726-b595-452d-bb81-79676a78881c\n  spec:\n    matchRules:\n    - config:\n        deny:\n        - 52.175.17.214\n        - 209.17.118.106\n        - 52.175.21.231\n        ip_header_name: x-forwarded-for\n        ip_source_type: header\n      ingress:\n      - ns-bjwgfnik/network-eeodjtvqdkbd\n    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/ip-restriction:1.0.0\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"extensions.higress.io/v1alpha1\",\"kind\":\"WasmPlugin\",\"metadata\":{\"annotations\":{},\"name\":\"limit\",\"namespace\":\"higress-system\"},\"spec\":{\"matchRules\":[{\"config\":{\"redis\":{\"password\":\"8bss65fz\",\"service_name\":\"redis.dns\",\"service_port\":6379,\"username\":\"default\"},\"rejected_code\":429,\"rejected_msg\":\"Too many requests, please use a custom domain name to remove the restriction\",\"rule_items\":[{\"limit_by_per_header\":\":authority\",\"limit_keys\":[{\"key\":\"regexp:^static-host.+\",\"query_per_minute\":10}]}],\"rule_name\":\"default_rule\",\"show_limit_quota_header\":true},\"domain\":[\"*.sealoshzh.site\"]}],\"priority\":600,\"url\":\"oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0\"}}\n    creationTimestamp: \"2025-03-21T13:23:25Z\"\n    generation: 3\n    name: limit\n    namespace: higress-system\n    resourceVersion: \"2650231697\"\n    uid: 4bee0edd-b021-4a7d-8fa9-f32436d9f0f1\n  spec:\n    matchRules:\n    - config:\n        redis:\n          password: *****\n          service_name: redis.dns\n          service_port: 6379\n          username: default\n        rejected_code: 429\n        rejected_msg: Too many requests, please use a custom domain name to remove\n          the restriction\n        rule_items:\n        - limit_by_per_header: :authority\n          limit_keys:\n          - key: regexp:^static-host.+\n            query_per_minute: 10\n        rule_name: default_rule\n        show_limit_quota_header: true\n      domain:\n      - '*.sealoshzh.site'\n    priority: 600\n    url: oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0\nkind: List\nmetadata:\n  resourceVersion: \"\"\n\n```",
        "closed": false,
        "closedAt": null,
        "number": 24,
        "state": "OPEN",
        "title": "公有云- wasmPlugins",
        "url": "https://github.com/labring/sealos-pro/issues/24",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://github.com/bearslyricattack/CompliK 挖坑",
        "closed": false,
        "closedAt": null,
        "number": 23,
        "state": "OPEN",
        "title": "合规检测",
        "url": "https://github.com/labring/sealos-pro/issues/23",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring/sealos/issues/5978",
        "closed": false,
        "closedAt": null,
        "number": 22,
        "state": "OPEN",
        "title": "优化项目  image-shim 动态加载",
        "url": "https://github.com/labring/sealos-pro/issues/22",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. 除了默认的监控面板还需要哪些？\n2. 对于cloud 来讲需要那些告警通知\n3. 有没有一些事监控定制的项目",
        "closed": true,
        "closedAt": "2025-09-19T13:34:42Z",
        "number": 21,
        "state": "CLOSED",
        "title": "其他任务 TOD 待办",
        "url": "https://github.com/labring/sealos-pro/issues/21",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n[20:23:11] [~/Workspaces/go/src/github.com/labring/sealos-pro/victoria-metrics-k8s-stack/v1.124.0] git(main) 🔥 ❱❱❱ sreg save --registry-dir=/tmp/registry .  --debug\nError: failed to get images in charts: template: grafana-operator/templates/rbac.yaml:14:22: executing \"grafana-operator/templates/rbac.yaml\" at <$watchNamespaces>: invalid value; expected string\nUsage:\n  sreg save [flags]\n\nExamples:\n\nsreg save --registry-dir=/tmp/registry .\nsreg save --registry-dir=/tmp/registry --images=containers-storage:docker.io/labring/coredns:v0.0.1\nsreg save --registry-dir=/tmp/registry --images=docker-daemon:docker.io/library/nginx:latest\nsreg save --registry-dir=/tmp/registry --tars=docker-archive:/root/config_main.tar@library/config_main\nsreg save --registry-dir=/tmp/registry --tars=oci-archive:/root/config_main.tar@library/config_main\nsreg save --registry-dir=/tmp/registry --images=docker.io/library/busybox:latest\n\nFlags:\n      --all                   Pull all images if SOURCE-IMAGE is a list\n      --arch string           pull images arch (default \"arm64\")\n  -h, --help                  help for save\n      --images strings        images list\n      --max-pull-procs int    maximum number of goroutines for pulling (default 5)\n      --registry-dir string   registry data dir path (default \"registry\")\n      --tars strings          tar list, eg: --tars=docker-archive:/root/config_main.tar@library/config_main\n\nGlobal Flags:\n      --debug   enable debug logger\n```",
        "closed": false,
        "closedAt": null,
        "number": 20,
        "state": "OPEN",
        "title": "无法渲染grafana-operator 的chart",
        "url": "https://github.com/labring/sealos-pro/issues/20",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "![Image](https://github.com/user-attachments/assets/52c0c6dc-2f71-4a99-b6c9-f7b08096293c)",
        "closed": false,
        "closedAt": null,
        "number": 19,
        "state": "OPEN",
        "title": "对象存储使用的service是LB 方式，有泄露风险",
        "url": "https://github.com/labring/sealos-pro/issues/19",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. 执行的时候需要获取地址 然后给containerd添加配置\n2. 需要测试",
        "closed": true,
        "closedAt": "2025-09-14T10:48:43Z",
        "number": 18,
        "state": "CLOSED",
        "title": "devbox如何修改镜像仓库地址",
        "url": "https://github.com/labring/sealos-pro/issues/18",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "开启cilium hubble的tls以增强访问的安全性。\nhttps://fael3z0zfze.feishu.cn/wiki/GWCywoCFBiDH8qkPS7ZcTblCnHg",
        "closed": true,
        "closedAt": "2025-09-12T04:03:31Z",
        "number": 17,
        "state": "CLOSED",
        "title": "cilium hubble安全性增强",
        "url": "https://github.com/labring/sealos-pro/issues/17",
        "login": "bearslyricattack",
        "is_bot": false
      },
      {
        "body": "# 修改 sync-images 逻辑\n\n1. 根据版本类型区分镜像名字\n2. 使用新名字 比如 sealos-pro-all\n3. tag需要支持 版本类型 以及cloud的版本\n4. 需要保存sealos sealctl 文件 从sealos release 下载。 在缓存路径 新建bin目录并保存",
        "closed": true,
        "closedAt": "2025-09-11T14:42:17Z",
        "number": 15,
        "state": "CLOSED",
        "title": "修改镜像名字",
        "url": "https://github.com/labring/sealos-pro/issues/15",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. 版本分商业版和公有云版本\n2. 如果是公有云版本后缀有个-cloud 比如版本 v2.1.3是商业版 v2.1.3-cloud是公有云版本，如果不存在-cloud版本则是使用商业版作为cloud的版本",
        "closed": true,
        "closedAt": "2025-09-11T14:14:55Z",
        "number": 13,
        "state": "CLOSED",
        "title": "sync-images.sh 实现一下feature的区分",
        "url": "https://github.com/labring/sealos-pro/issues/13",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-11T13:57:29Z",
        "number": 11,
        "state": "CLOSED",
        "title": "sync-images 在原来的逻辑上添加sealos-cloud 镜像，需要新增版本入参",
        "url": "https://github.com/labring/sealos-pro/issues/11",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).",
        "closed": true,
        "closedAt": "2025-09-11T13:00:09Z",
        "number": 9,
        "state": "CLOSED",
        "title": "✨ Set up Copilot instructions",
        "url": "https://github.com/labring/sealos-pro/issues/9",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1.  sync-images.sh 脚本从环境变量方式换成 参数的方式执行\n2. 缓存镜像不要在当前目录执行，可以换个目录执行",
        "closed": true,
        "closedAt": "2025-09-11T12:36:03Z",
        "number": 7,
        "state": "CLOSED",
        "title": "脚本重构",
        "url": "https://github.com/labring/sealos-pro/issues/7",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## 我需要写一个脚本放到根目录,它的名字叫 sync-images.sh\n\n1. 它支持目录扫描，如果不存在package.sh 则需要当前的镜像以及版本\n2. 他的镜像规则是 ghcr.io/labring/sealos-pro:${NAME}-${VERSION}\n3. 需要根据架构分别保存\n4. 需要单独写个Kubefile 只缓存这些镜像到registry，方法是创建一个 images/shim目录里面把我需要的镜像名字写到一个txt中去\n5. 在这个目录执行sealos build 即可缓存镜像\n6. 所有的镜像都是私有镜像 需要提前登录 ，这个镜像需要包括sealos login",
        "closed": true,
        "closedAt": "2025-09-11T12:20:04Z",
        "number": 5,
        "state": "CLOSED",
        "title": "sync image scripts",
        "url": "https://github.com/labring/sealos-pro/issues/5",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/PbFIwUs3OizAg5kR78mcK4QznQd\n\n```\nsealos run docker.io/labring/cert-manager:v1.17.1\n```",
        "closed": true,
        "closedAt": "2025-09-14T11:32:11Z",
        "number": 4,
        "state": "CLOSED",
        "title": "20250804 Cloudflare DNS API 升级导致证书无法签发",
        "url": "https://github.com/labring/sealos-pro/issues/4",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. https://fael3z0zfze.feishu.cn/wiki/APYLwFZOhiQN0Xk4NSpcB7n4nxe\n\n## 解法\n\nhttps://github.com/cert-manager/cert-manager/pull/1671 --max-concurrent-challenges",
        "closed": true,
        "closedAt": "2025-09-15T04:05:22Z",
        "number": 3,
        "state": "CLOSED",
        "title": "20250729 cert-manager申请证书卡住",
        "url": "https://github.com/labring/sealos-pro/issues/3",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. https://fael3z0zfze.feishu.cn/wiki/CmVtwPMpDiJHlPkMRp5c1Ejkn9f",
        "closed": false,
        "closedAt": null,
        "number": 2,
        "state": "OPEN",
        "title": "20250821 节点磁盘压力过大导致关键组件被驱除",
        "url": "https://github.com/labring/sealos-pro/issues/2",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 1,
        "state": "OPEN",
        "title": "参数调优",
        "url": "https://github.com/labring/sealos-pro/issues/1",
        "login": "cuisongliu",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sreg",
    "issues": []
  },
  {
    "repo": "labring-actions/cluster-image",
    "issues": [
      {
        "body": "请问能帮忙维护编译应用仓库吗？目前仅有几个允许accepted标签和编译，帮忙把常用的longhorn,openebs，metallb、calico、higress、apisix、kubeblocks打一下accepted标签，谢谢。",
        "closed": false,
        "closedAt": null,
        "number": 1232,
        "state": "OPEN",
        "title": "请继续维护这个仓库的accepted的标签，谢谢",
        "url": "https://github.com/labring-actions/cluster-image/issues/1232",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1231,
        "state": "OPEN",
        "title": "【Auto-build】minio",
        "url": "https://github.com/labring-actions/cluster-image/issues/1231",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps rainbond v6.3.1",
        "closed": false,
        "closedAt": null,
        "number": 1230,
        "state": "OPEN",
        "title": "【Auto-build】rainbond",
        "url": "https://github.com/labring-actions/cluster-image/issues/1230",
        "login": "ilyndon",
        "is_bot": false
      },
      {
        "body": "```\n/imagebuild_apps cilium  v1.17.4\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1229,
        "state": "OPEN",
        "title": "【Auto-build】cilium v1.17.4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1229",
        "login": "kaneleven",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps nerdctl v2.1.2\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps nerdctl v2.1.2\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1228,
        "state": "OPEN",
        "title": "【Auto-build】nerdctl",
        "url": "https://github.com/labring-actions/cluster-image/issues/1228",
        "login": "LinkMaq",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": false,
        "closedAt": null,
        "number": 1227,
        "state": "OPEN",
        "title": "【DaylyReport】 Auto build for laf 2025/6/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1227",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "Usage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps longhorn v1.8.0",
        "closed": false,
        "closedAt": null,
        "number": 1226,
        "state": "OPEN",
        "title": "【Auto-build】longhorn",
        "url": "https://github.com/labring-actions/cluster-image/issues/1226",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "Usage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2",
        "closed": true,
        "closedAt": "2025-06-09T01:30:36Z",
        "number": 1225,
        "state": "CLOSED",
        "title": "【Auto-build】longhorn",
        "url": "https://github.com/labring-actions/cluster-image/issues/1225",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps helm v3.8.2",
        "closed": true,
        "closedAt": "2025-05-28T07:59:00Z",
        "number": 1224,
        "state": "CLOSED",
        "title": "【Auto-build】helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1224",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps helm v3.8.2\n",
        "closed": true,
        "closedAt": "2025-05-29T08:47:44Z",
        "number": 1221,
        "state": "CLOSED",
        "title": "【Auto-build】helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1221",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2\n```\n/imagebuild_apps elasticsearch v8.16.2",
        "closed": false,
        "closedAt": null,
        "number": 1216,
        "state": "OPEN",
        "title": "【Auto-build】elasticsearch",
        "url": "https://github.com/labring-actions/cluster-image/issues/1216",
        "login": "lingdie",
        "is_bot": false
      },
      {
        "body": "1",
        "closed": true,
        "closedAt": "2025-04-19T18:04:18Z",
        "number": 1214,
        "state": "CLOSED",
        "title": "【Auto-build】helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1214",
        "login": "kjagsdq",
        "is_bot": false
      },
      {
        "body": "/imagebuild_dockerimages abc",
        "closed": true,
        "closedAt": "2025-04-19T18:04:24Z",
        "number": 1212,
        "state": "CLOSED",
        "title": "【Auto-build】helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1212",
        "login": "kjagsdq",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1209,
        "state": "OPEN",
        "title": "[DaylyReport] Auto build for sealos 2025/4/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1209",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-04-07T03:42:26Z",
        "number": 1208,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/4/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1208",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-04-02T17:57:28Z",
        "number": 1207,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1207",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-31T02:30:03Z",
        "number": 1206,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1206",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-28T03:52:26Z",
        "number": 1205,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1205",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-27T06:31:29Z",
        "number": 1204,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1204",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-26T09:44:54Z",
        "number": 1203,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1203",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-25T07:39:12Z",
        "number": 1202,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1202",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-24T08:58:37Z",
        "number": 1201,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1201",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-21T12:18:04Z",
        "number": 1199,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1199",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:52Z",
        "number": 1198,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1198",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:53Z",
        "number": 1197,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1197",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:53Z",
        "number": 1196,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1196",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:37Z",
        "number": 1195,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1195",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:37Z",
        "number": 1194,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1194",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:38Z",
        "number": 1193,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1193",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-05T07:03:07Z",
        "number": 1192,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1192",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-04T02:41:06Z",
        "number": 1191,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1191",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:49Z",
        "number": 1190,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1190",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:49Z",
        "number": 1189,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1189",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:50Z",
        "number": 1188,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1188",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-02T11:40:33Z",
        "number": 1187,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1187",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-28T04:52:39Z",
        "number": 1186,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1186",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-27T04:11:11Z",
        "number": 1185,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1185",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-26T02:50:12Z",
        "number": 1184,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1184",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-26T02:50:13Z",
        "number": 1183,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1183",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-25T07:24:44Z",
        "number": 1182,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1182",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-25T07:24:45Z",
        "number": 1181,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1181",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-24T10:47:03Z",
        "number": 1180,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1180",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-21T03:22:20Z",
        "number": 1179,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1179",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-20T03:00:38Z",
        "number": 1178,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1178",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-19T08:36:24Z",
        "number": 1176,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1176",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-18T04:00:40Z",
        "number": 1175,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1175",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-14T08:50:28Z",
        "number": 1174,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1174",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-13T06:20:21Z",
        "number": 1173,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1173",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-12T06:20:13Z",
        "number": 1172,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1172",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-11T07:02:02Z",
        "number": 1171,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1171",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-10T08:41:25Z",
        "number": 1170,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1170",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-07T06:25:40Z",
        "number": 1169,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1169",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-06T09:03:02Z",
        "number": 1168,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1168",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-27T04:27:16Z",
        "number": 1167,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1167",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-21T13:35:09Z",
        "number": 1166,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1166",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-20T03:41:03Z",
        "number": 1165,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1165",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-13T07:39:56Z",
        "number": 1164,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1164",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-13T07:39:57Z",
        "number": 1163,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1163",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps kubesphere v3.3.2\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": true,
        "closedAt": "2025-01-08T10:37:50Z",
        "number": 1162,
        "state": "CLOSED",
        "title": "【Auto-build】kubesphere",
        "url": "https://github.com/labring-actions/cluster-image/issues/1162",
        "login": "relleo",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-10T07:47:15Z",
        "number": 1161,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1161",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-08T06:11:07Z",
        "number": 1160,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1160",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-07T07:38:54Z",
        "number": 1159,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1159",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-07T07:38:55Z",
        "number": 1158,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1158",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-06T06:37:39Z",
        "number": 1157,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1157",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-06T06:37:39Z",
        "number": 1156,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1156",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-03T03:54:46Z",
        "number": 1155,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1155",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-31T06:33:59Z",
        "number": 1154,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1154",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-30T03:36:43Z",
        "number": 1153,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1153",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-27T08:55:47Z",
        "number": 1152,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1152",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-27T08:55:47Z",
        "number": 1151,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1151",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-26T05:03:46Z",
        "number": 1150,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1150",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-25T06:35:16Z",
        "number": 1149,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1149",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-25T06:35:17Z",
        "number": 1148,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1148",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-24T06:14:14Z",
        "number": 1147,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1147",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-20T02:32:34Z",
        "number": 1146,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1146",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-19T08:46:51Z",
        "number": 1145,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1145",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-18T07:30:21Z",
        "number": 1144,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1144",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1143,
        "state": "OPEN",
        "title": "【Auto-build】oceanbase-operator",
        "url": "https://github.com/labring-actions/cluster-image/issues/1143",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-16T03:09:09Z",
        "number": 1142,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1142",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-16T03:09:09Z",
        "number": 1141,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1141",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-14T04:17:37Z",
        "number": 1140,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1140",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-13T06:37:41Z",
        "number": 1139,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1139",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-12T03:28:00Z",
        "number": 1138,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1138",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-11T02:56:40Z",
        "number": 1137,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1137",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-10T07:47:13Z",
        "number": 1136,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1136",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-09T03:47:49Z",
        "number": 1135,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1135",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-06T03:42:48Z",
        "number": 1134,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1134",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-04T04:24:33Z",
        "number": 1133,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1133",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-03T02:36:14Z",
        "number": 1132,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1132",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Description\n\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\n\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n\n### What To Do?\n\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n\n**For further information, suggestions, and guidance:**\n- 📖 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- 💬 Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\n\nWe sincerely apologize for any inconvenience this may cause.\n\n**Thank you for your cooperation and understanding!** :pray:\n",
        "closed": false,
        "closedAt": null,
        "number": 1131,
        "state": "OPEN",
        "title": ":warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy",
        "url": "https://github.com/labring-actions/cluster-image/issues/1131",
        "login": "camilamacedo86",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-02T03:22:50Z",
        "number": 1130,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1130",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-29T04:11:24Z",
        "number": 1129,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1129",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-28T02:19:54Z",
        "number": 1128,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1128",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-28T02:19:54Z",
        "number": 1127,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1127",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-27T03:15:07Z",
        "number": 1126,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1126",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-26T09:21:09Z",
        "number": 1125,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1125",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-25T03:28:16Z",
        "number": 1124,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1124",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-25T03:28:17Z",
        "number": 1123,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1123",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-23T09:23:46Z",
        "number": 1122,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1122",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-22T03:06:48Z",
        "number": 1121,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1121",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-21T08:18:18Z",
        "number": 1120,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1120",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-20T07:30:09Z",
        "number": 1119,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1119",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-20T07:30:09Z",
        "number": 1118,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1118",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-19T06:46:01Z",
        "number": 1117,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1117",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-18T02:35:04Z",
        "number": 1115,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1115",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-15T03:55:10Z",
        "number": 1114,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1114",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-13T10:15:39Z",
        "number": 1113,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1113",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:48Z",
        "number": 1112,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1112",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:48Z",
        "number": 1111,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1111",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:49Z",
        "number": 1110,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1110",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-11T08:14:56Z",
        "number": 1109,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1109",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-08T02:50:10Z",
        "number": 1108,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1108",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-06T07:23:51Z",
        "number": 1107,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1107",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:19Z",
        "number": 1106,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1106",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:20Z",
        "number": 1105,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1105",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:20Z",
        "number": 1104,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1104",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-04T09:47:25Z",
        "number": 1103,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1103",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-04T09:47:25Z",
        "number": 1102,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1102",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-31T08:59:01Z",
        "number": 1101,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1101",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-29T02:41:56Z",
        "number": 1100,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1100",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-28T05:39:25Z",
        "number": 1099,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1099",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-25T12:42:19Z",
        "number": 1098,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1098",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-24T10:01:23Z",
        "number": 1097,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1097",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-23T08:07:42Z",
        "number": 1095,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1095",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-22T02:51:04Z",
        "number": 1094,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1094",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-21T03:06:47Z",
        "number": 1093,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/17",
        "url": "https://github.com/labring-actions/cluster-image/issues/1093",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-17T09:02:06Z",
        "number": 1092,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1092",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": true,
        "closedAt": "2025-06-10T06:19:44Z",
        "number": 1091,
        "state": "CLOSED",
        "title": "【DaylyReport】 Auto build for laf 2024/10/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1091",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1090,
        "state": "OPEN",
        "title": "【Auto-build】automq-operator",
        "url": "https://github.com/labring-actions/cluster-image/issues/1090",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-16T10:22:57Z",
        "number": 1089,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1089",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-15T07:24:13Z",
        "number": 1088,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1088",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-12T02:57:27Z",
        "number": 1087,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1087",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-11T03:00:02Z",
        "number": 1086,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1086",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-10T06:26:55Z",
        "number": 1085,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1085",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-09T02:40:54Z",
        "number": 1084,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1084",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-08T02:49:22Z",
        "number": 1083,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1083",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-28T04:24:29Z",
        "number": 1082,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1082",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_dockerimages appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n   buildArgs:  build args, split by ','\r\n\r\nExample:\r\n   /imagebuild_dockerimages helm v3.8.2 Key1=Value1,Key2=Value2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1081,
        "state": "OPEN",
        "title": "【Auto-build】curl-kubectl",
        "url": "https://github.com/labring-actions/cluster-image/issues/1081",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-27T03:09:26Z",
        "number": 1080,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1080",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-26T04:08:28Z",
        "number": 1078,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1078",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-25T03:13:17Z",
        "number": 1077,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1077",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-24T08:18:08Z",
        "number": 1076,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1076",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": true,
        "closedAt": "2024-09-21T15:57:39Z",
        "number": 1075,
        "state": "CLOSED",
        "title": "【Auto-build】metrics-server",
        "url": "https://github.com/labring-actions/cluster-image/issues/1075",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-23T02:40:47Z",
        "number": 1074,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1074",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-20T04:48:10Z",
        "number": 1073,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1073",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "Longhorn v1.7.1 build failed.\r\n\r\n```\r\nError: failed to save images: save image longhornio/openshift-origin-oauth-proxy:4.15: choosing an image from manifest list docker://longhornio/openshift-origin-oauth-proxy:4.15: no image found in manifest list for architecture arm64, variant \"\", OS linux\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/0229b976-3edd-49c0-a4ca-87a5df3c16fb)\r\n\r\nThere have no arm image\r\n![image](https://github.com/user-attachments/assets/ef52bd9e-2396-4716-937b-d6e132331d76)\r\n\r\nshould support \r\n```\r\n/imagebuild_apps longhorn v1.7.1 amd64\r\n```\r\n",
        "closed": true,
        "closedAt": "2024-12-15T07:20:31Z",
        "number": 1072,
        "state": "CLOSED",
        "title": "Feature: support selecting an architecture type when building an image",
        "url": "https://github.com/labring-actions/cluster-image/issues/1072",
        "login": "weironz",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-19T03:58:03Z",
        "number": 1071,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1071",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-19T03:58:04Z",
        "number": 1070,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1070",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-18T07:58:37Z",
        "number": 1069,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1069",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-14T10:03:16Z",
        "number": 1068,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1068",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-13T03:27:12Z",
        "number": 1067,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1067",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-12T03:13:00Z",
        "number": 1066,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1066",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-11T06:55:18Z",
        "number": 1065,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1065",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-10T09:50:29Z",
        "number": 1064,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1064",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-09T04:11:12Z",
        "number": 1063,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1063",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-06T02:50:05Z",
        "number": 1062,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1062",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-06T02:50:05Z",
        "number": 1061,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1061",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-05T03:33:05Z",
        "number": 1060,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1060",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-04T04:02:32Z",
        "number": 1059,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1059",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-03T08:33:08Z",
        "number": 1058,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1058",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-02T05:33:49Z",
        "number": 1057,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1057",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-02T05:33:50Z",
        "number": 1056,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1056",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-31T07:06:14Z",
        "number": 1055,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1055",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-30T03:12:36Z",
        "number": 1054,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1054",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-30T03:12:37Z",
        "number": 1053,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1053",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-29T04:43:15Z",
        "number": 1051,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1051",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-28T03:03:04Z",
        "number": 1050,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1050",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-27T07:19:38Z",
        "number": 1049,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1049",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-27T07:19:39Z",
        "number": 1048,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1048",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-24T07:31:21Z",
        "number": 1047,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1047",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-23T03:41:01Z",
        "number": 1046,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1046",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-23T03:41:01Z",
        "number": 1045,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1045",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-22T03:12:46Z",
        "number": 1044,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1044",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-21T03:32:55Z",
        "number": 1043,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1043",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-21T03:32:55Z",
        "number": 1042,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1042",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-20T06:53:11Z",
        "number": 1041,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1041",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": true,
        "closedAt": "2024-10-16T08:24:21Z",
        "number": 1040,
        "state": "CLOSED",
        "title": "【DaylyReport】 Auto build for laf 2024/8/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1040",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-16T06:07:44Z",
        "number": 1039,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1039",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-15T03:43:56Z",
        "number": 1038,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1038",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-14T03:36:31Z",
        "number": 1037,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1037",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-13T02:35:16Z",
        "number": 1036,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1036",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-13T02:35:16Z",
        "number": 1035,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1035",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps cilium v1.15.7",
        "closed": true,
        "closedAt": "2024-09-21T15:57:51Z",
        "number": 1034,
        "state": "CLOSED",
        "title": "【Auto-build】helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1034",
        "login": "hanxianzhai",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps cilium v1.16.0\r\nAvailable Args:\r\n   appName:  cilium\r\n   appVersion: v.1.16.0\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1033,
        "state": "OPEN",
        "title": "【Auto-build】cilium",
        "url": "https://github.com/labring-actions/cluster-image/issues/1033",
        "login": "hanxianzhai",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-12T08:51:57Z",
        "number": 1032,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1032",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-09T03:14:14Z",
        "number": 1031,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1031",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-08T03:56:22Z",
        "number": 1030,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1030",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-07T07:09:11Z",
        "number": 1029,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1029",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-07T07:09:11Z",
        "number": 1028,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1028",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-06T06:33:20Z",
        "number": 1027,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1027",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-05T07:09:16Z",
        "number": 1026,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1026",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-02T06:33:22Z",
        "number": 1025,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/1",
        "url": "https://github.com/labring-actions/cluster-image/issues/1025",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-01T08:49:35Z",
        "number": 1024,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1024",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-01T08:49:36Z",
        "number": 1023,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1023",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-31T11:29:39Z",
        "number": 1022,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1022",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-30T07:18:12Z",
        "number": 1021,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1021",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-29T03:54:33Z",
        "number": 1020,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1020",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-26T06:50:09Z",
        "number": 1019,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1019",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-24T02:44:36Z",
        "number": 1017,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1017",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-24T02:44:36Z",
        "number": 1018,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1018",
        "login": "sealos-ci-robot",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring-actions/sync-aliyun",
    "issues": []
  },
  {
    "repo": "labring-actions/templates",
    "issues": [
      {
        "body": "应用的名称和简介: odoo\n官网 https://waline.js.org/\nGITHUB地址：https://github.com/walinejs/waline\n我看应用商店有了artalk，但Waline的使用人数也很多。期待官方能上架Waline应用",
        "closed": false,
        "closedAt": null,
        "number": 479,
        "state": "OPEN",
        "title": "期待添加Waline应用",
        "url": "https://github.com/labring-actions/templates/issues/479",
        "login": "kok777",
        "is_bot": false
      },
      {
        "body": "为什么这么多pr没人审核呢？最早甚至有前年的。但我看仓库似乎一直都在更新，如果有人在维护就请多清清pr吧，sealos现在就那么些个模板，不够用",
        "closed": false,
        "closedAt": null,
        "number": 440,
        "state": "OPEN",
        "title": "仓库很多pr没人审核",
        "url": "https://github.com/labring-actions/templates/issues/440",
        "login": "ifzzh",
        "is_bot": false
      },
      {
        "body": "应用的名称和简介: odoo\n应用的官方网站或 GitHub 仓库地址: https://github.com/odoo/odoo, https://github.com/odoo/odoo.git\n应用的部署要求 (如：所需的环境变量、存储需求等): 说不清楚\n其他相关信息:  无",
        "closed": false,
        "closedAt": null,
        "number": 415,
        "state": "OPEN",
        "title": "odoo_Create odoo in Sealos APP store",
        "url": "https://github.com/labring-actions/templates/issues/415",
        "login": "PythonPlayerJack",
        "is_bot": false
      },
      {
        "body": "一开始是用的应用部署的dify可以正常使用，然后昨天晚上尝试着更新到0.6.14的版本后，出现了报错，然后就重新改会到0.6.11，\r\n![image](https://github.com/user-attachments/assets/4a3e6f46-c403-4327-97af-d16049688147)\r\n![0d44b7198cfc77c24e1320ae7cccad3](https://github.com/user-attachments/assets/c6beb754-4a8a-4141-b416-49a82fc3ce2f)\r\n\r\n\r\n后面看下了dify的更新日志，11之后的docker部署不一样了， 多了一个.env文件，所以应该不能升级到.11版本后面，但现在我要回滚到.11之前的版本也不行了，但是我部署一个新的dify，发现也运行不了，一直报跨域的问题",
        "closed": true,
        "closedAt": "2024-08-08T06:25:31Z",
        "number": 292,
        "state": "CLOSED",
        "title": "dify启动不了，现在访问一直报跨域的问题，应该api的服务没有启动",
        "url": "https://github.com/labring-actions/templates/issues/292",
        "login": "catdddmouse",
        "is_bot": false
      },
      {
        "body": "The TLS certificate does not work for the yaml config generated from the [current version of template](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/template.yaml)\r\n\r\nThe `tls.secretName` in it does not exist. According to the [Template Variables](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/example.md#built-in-system-variables), it should use **all upper case** letters `SEALOS_CERT_SECRET_NAME` instead of `SEALOS_Cert_Secret_Name`.\r\n\r\n\r\n\r\n",
        "closed": true,
        "closedAt": "2024-03-25T06:34:55Z",
        "number": 223,
        "state": "CLOSED",
        "title": "Wrong `tls.secretName` in Template Causes SSL Certificate Issue",
        "url": "https://github.com/labring-actions/templates/issues/223",
        "login": "tianshanghong",
        "is_bot": false
      },
      {
        "body": "The librechat template cannot be deployed in the beijing, guangzhou, and hangzhou availability zones of the sealos website. I checked the problem and it is that mongodb is always showing as creating. Is there a way to fix it? Thank you.\r\n\r\nthis screenshot is from https://gzg.sealos.run/\r\n\r\n![image](https://github.com/labring-actions/templates/assets/46660805/26fb20a2-81b2-4f77-9123-ebbe5f9d09c0)\r\n\r\n",
        "closed": true,
        "closedAt": "2024-03-25T06:37:55Z",
        "number": 205,
        "state": "CLOSED",
        "title": "LibreChat cannot be deployed in Guangzhou",
        "url": "https://github.com/labring-actions/templates/issues/205",
        "login": "BigTomM",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring-actions/templates/blob/6b57ae8428450bfd158d0ec9c3d0ff767634c56b/template/docker-stacks.yaml#L21",
        "closed": true,
        "closedAt": "2023-10-16T08:14:28Z",
        "number": 86,
        "state": "CLOSED",
        "title": "typo error",
        "url": "https://github.com/labring-actions/templates/issues/86",
        "login": "lingdie",
        "is_bot": false
      }
    ]
  }
]