[
  {
    "repo": "labring/cri-shim",
    "issues": []
  },
  {
    "repo": "labring/sealos",
    "issues": [
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 291   |\n| âœ… Successful | 192   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18201548304?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-10-03T02:42:50Z",
        "number": 6064,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6064",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nAfter running sealos gen -e defaultVIP=198.19.0.2, I manually changed networking.serviceSubnet to 198.19.0.0/16 in the Clusterfile. However, executing sealos apply -f Clusterfile still results in the error:\n\n\"Applied to cluster error: failed to init masters: generate init config error: ensure IP 198.19.0.2 is not in serviceSubnet range\".\n\n### What is the expected behavior?\n\nThe VIP has been correctly set to 198.19.0.2 without any errors.\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-30T13:06:43Z",
        "number": 6062,
        "state": "CLOSED",
        "title": "BUG: VIP Conflicts with Service Subnet in Sealos",
        "url": "https://github.com/labring/sealos/issues/6062",
        "login": "ancienter",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 291   |\n| âœ… Successful | 192   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in lifecycle/USERS.md\n\n* [TIMEOUT] <https://www.iflytek.com/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18106301832?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-30T13:02:33Z",
        "number": 6061,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6061",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.1.0-rc1\n\n### How to reproduce the bug?\n\nSEALOS_V2_CLOUD_DOMAIN=10.128.250.128.nip.io SEALOS_V2_MASTERS=\"10.128.250.128:22\" ./install-v2.sh\n\n### What is the expected behavior?\n\nInstall\n\n### What do you see instead?\n\n```sh\n2025-09-28T12:45:45 info start to install app in this cluster\n2025-09-28T12:45:45 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing PreProcess Pipeline in InstallProcessor\n2025-09-28T12:45:45 info Executing pipeline MountRootfs in InstallProcessor.\n2025-09-28T12:45:46 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-09-28T12:45:46 info Executing UpgradeIfNeed Pipeline in InstallProcessor\nError from server (NotFound): configmaps \"sealos-config\" not found\nError from server (NotFound): configmaps \"sealos-config\" not found\nnamespace/sealos-system configured\nnamespace/sealos configured\nRelease \"cert-config\" does not exist. Installing it now.\nError: Unable to continue with install: Certificate \"sealos-cloud\" in namespace \"sealos-system\" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error: missing key \"meta.helm.sh/release-name\": must be set to \"cert-config\"; annotation validation error: missing key \"meta.helm.sh/release-namespace\": must be set to \"sealos-system\"\n2025-09-28T12:45:47 info using v1beta3 kubeadm config\nError: exit status 1\n```\n\n### Operating environment\n\n```markdown\n- Sealos version:5.1.0-rc1\n- Docker version:\n- Kubernetes version:\n- Operating system: Ubuntu 24\n- Runtime environment:physical machine 200G mem\n- Cluster size:1 node\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 6057,
        "state": "OPEN",
        "title": "BUG: install fail",
        "url": "https://github.com/labring/sealos/issues/6057",
        "login": "xiachenrui",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.1.0-beta4\n\n### How to reproduce the bug?\n\n==========================================================================================================\nYou are installing the open source version of Sealos Cloud. Some features are missing. \nFor full functionality, please purchase the commercial edition: https://sealos.io/contact\n==========================================================================================================\n INFO [2025-09-28 17:37:02] >> Using GitHub proxy for downloads. \n INFO [2025-09-28 17:37:02] >> [Step 1] Environment and dependency checks... \n INFO [2025-09-28 17:37:02] >> Environment and dependency checks passed. \n INFO [2025-09-28 17:37:02] >> Sealos CLI is already installed. \n INFO [2025-09-28 17:37:02] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubernetes:v1.28.15 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cilium:v1.17.1 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cert-manager:v1.14.6 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/helm:v3.16.2 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/openebs:v3.10.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/higress:v2.1.3 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubeblocks:v0.8.2 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cockroach:v2.12.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/metrics-server:v0.6.4 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/victoria-metrics-k8s-stack:v1.124.0 \n INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos-cloud:latest \nError: initializing image from source docker://ghcr.nju.edu.cn/labring/sealos-cloud:latest: invalid character '<' looking for beginning of value\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-29T08:33:22Z",
        "number": 6056,
        "state": "CLOSED",
        "title": "ä½¿ç”¨å®˜æ–¹è„šæœ¬ï¼Œå®‰è£…å¤±è´¥",
        "url": "https://github.com/labring/sealos/issues/6056",
        "login": "ATCThunder",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 291   |\n| âœ… Successful | 192   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in CODE_OF_CONDUCT.md\n\n* [TIMEOUT] <https://www.contributor-covenant.org/version/2/1/code_of_conduct.html> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/18045649394?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-27T07:29:36Z",
        "number": 6046,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6046",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Documentation Section\n\nå®‰è£…æ–‡æ¡£\n\n### What is the current documentation state?\n\nsealos é›†ç¾¤ä¿¡æ¯ï¼Œæˆ‘åœ¨master1èŠ‚ç‚¹ä¸Šåˆ›å»ºçš„é›†ç¾¤ï¼Œå¦‚æœmaster1 æŒ‚äº†ï¼Œå°±GGã€‚\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### Documentation Section\n\nInstallation documentation\n\n### What is the current documentation state?\n\nsealos cluster information, the cluster I created on the master1 node, if master1 is hung, it will GG.\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change benefit?\n\n_No response_\n\n### Additional information\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-24T15:06:54Z",
        "number": 6036,
        "state": "CLOSED",
        "title": "Docs: brief description of your suggestion || Docs: brief description of your suggestions",
        "url": "https://github.com/labring/sealos/issues/6036",
        "login": "menchao1992",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 301   |\n| âœ… Successful | 201   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17897139865?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-22T02:45:28Z",
        "number": 6021,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6021",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 301   |\n| âœ… Successful | 201   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17883128560?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-21T01:49:33Z",
        "number": 6018,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6018",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 301   |\n| âœ… Successful | 201   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 98    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Rejected status code (this depends on your \"accept\" configuration): Forbidden\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [403] <https://www.npmjs.com/package/@zjy365/sealos-desktop-sdk> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17866203144?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-20T03:11:56Z",
        "number": 6014,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/6014",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-20T08:42:53Z",
        "number": 6007,
        "state": "CLOSED",
        "title": "Feature: admission webhook release tag",
        "url": "https://github.com/labring/sealos/issues/6007",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n$ sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\n\næ‰§è¡Œè¿™ä¸ªå®‰è£…å‘½ä»¤çš„æ—¶å€™æŠ¥ï¼š\nE0917 10:50:50.919408 39371 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"sealos.hub:5000/pause:3.9\\\": failed to resolve reference \\\"sealos.hub:5000/pause:3.9\\\": failed to do request: Head \\\"https://sealos.hub:5000/v2/pause/manifests/3.9\\\": EOF\" image=\"sealos.hub:5000/pause:3.9\" FATA[0001] pulling image: failed to pull and unpack image \"sealos.hub:5000/pause:3.9\": failed to resolve reference \"sealos.hub:5000/pause:3.9\": failed to do request: Head \"https://sealos.hub:5000/v2/pause/manifests/3.9\": EOF Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service â†’ /etc/systemd/system/kubelet.service. Error: failed to init masters: master pull image failed, error: exit status 1\n\næ”¹é…ç½®ï¼Œè®©å…¶é€šè¿‡httpè®¿é—®ï¼Œä½†ä¾ç„¶è¿˜æ˜¯é€šè¿‡httpsè®¿é—®\n\n### What is the expected behavior?\n\nèƒ½å¤Ÿæ­£å¸¸å®‰è£…æˆåŠŸ\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-18T02:24:34Z",
        "number": 5992,
        "state": "CLOSED",
        "title": "fail to install",
        "url": "https://github.com/labring/sealos/issues/5992",
        "login": "ATCThunder",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nç³»ç»Ÿï¼š rockylinux8.5\nsealos: v5.0.0 v5.0.1\nå¼‚å¸¸çš„k8sç‰ˆæœ¬ï¼šv1.31åŠä»¥ä¸Šï¼Œdockerç‰ˆæœ¬containerç‰ˆæœ¬éƒ½æ˜¯\nå®‰è£…å‘½ä»¤:\nimages='\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1\n'\nsealos run $images\n1.31 1.32 1.33éƒ½è¿›è¡Œäº†æµ‹è¯•\nä¸»è¦æŠ¥é”™ç‰‡æ®µï¼š\nvm.max_map_count = 2147483642 # sealos\n INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10 \nImage is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136\nCreated symlink /etc/systemd/system/multi-user.target.wants/kubelet.service â†’ /etc/systemd/system/kubelet.service.\n INFO [2025-09-15 23:10:33] >> init kubelet success \n INFO [2025-09-15 23:10:33] >> init rootfs success \n2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.\n2025-09-15T23:10:34 info Copying kubeadm config to master0\n2025-09-15T23:10:34 info start to generate cert and kubeConfig...\n2025-09-15T23:10:34 info start to generate and copy certs to masters...\n2025-09-15T23:10:34 info apiserver altNames : {map[apiserver.cluster.local:apiserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}\n2025-09-15T23:10:34 info Etcd altnames : {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128 ::1:::1]}, commonName : k8s-01\n2025-09-15T23:10:35 info start to copy etc pki files to masters\n2025-09-15T23:10:35 info start to create kubeconfig...\n2025-09-15T23:10:36 info start to copy kubeconfig files to masters\n2025-09-15T23:10:36 info start to copy static files to masters\n2025-09-15T23:10:36 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n\n<img width=\"1920\" height=\"671\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nSystem: rockylinux8.5\nsealos: v5.0.0 v5.0.1\nException k8s version: v1.31 and above, docker version container version are all\nInstallation command:\nimages='\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1\n'\nsealos run $images\n1.31 1.32 1.33 are all tested\nMain error report segments:\nvm.max_map_count = 2147483642 # sealos\n INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10\nImage is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136\nCreated symlink /etc/systemd/system/multi-user.target.wants/kubelet.service â†’ /etc/systemd/system/kubelet.service.\n INFO [2025-09-15 23:10:33] >> init kubelet success\n INFO [2025-09-15 23:10:33] >> init rootfs success\n2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.\n2025-09-15T23:10:34 info Copying kubeadm config to master0\n2025-09-15T23:10:34 info start to generate cert and kubeConfig...\n2025-09-15T23:10:34 info start to generate and copy certs to masters...\n2025-09-15T23:10:34 info apiserver altNames: {map[apserver.cluster.local:apserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}\n2025-09-15T23:10:34 info Etcd altnames: {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128::1:::1]}, commonName: k8s-01\n2025-09-15T23:10:35 info start to copy etc pki files to masters\n2025-09-15T23:10:35 info start to create kubeconfig...\n2025-09-15T23:10:36 info start to copy kubeconfig files to masters\n2025-09-15T23:10:36 info start to copy static files to masters\n2025-09-15T23:10:36 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n\n<img width=\"1920\" height=\"671\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-16T14:03:24Z",
        "number": 5987,
        "state": "CLOSED",
        "title": "BUG: sealos run å®‰è£…1.31åŠä»¥ä¸Šç‰ˆæœ¬çš„k8sæ—¶ï¼Œå…¨éƒ¨ä¼šå®‰è£…å¤±è´¥ï¼ || BUG: sealos run When installing k8s version 1.31 and above, all installations will fail!",
        "url": "https://github.com/labring/sealos/issues/5987",
        "login": "aiqianbao1314",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n<img width=\"2040\" height=\"204\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3d34da88-fbce-435f-a177-4499adc3ee34\" />\n\n```\nshim: /var/run/image-cri-shim.sock\ncri: /var/run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: false\ntimeout: 15m\nauth: admin:passw0rd\n\nregistries:\n- address: https://hub.192.168.64.4.nip.io\n  auth: admin:3c0e9478196ec93f2e5291d119408ddc\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-14T11:57:18Z",
        "number": 5979,
        "state": "CLOSED",
        "title": "BUG: images shim not sport local  registry",
        "url": "https://github.com/labring/sealos/issues/5979",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n```\n# Copyright Â© 2022 sealos.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nshim: /var/run/image-cri-shim.sock\ncri: /run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: true\nimage: /var/lib/image-cri-shim\ntimeout: 15m\nauth: admin:passw0rd\n\nregistries:\n- address: http://192.168.64.1:5000\n  auth: admin:passw0rd\n\n```\n\n### If you have solutionï¼Œplease describe it\n\nIt needs to be restarted, and dynamic loading is better.\n\n\n### What alternatives have you considered?\n\n```\n# Copyright Â© 2022 sealos.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nshim: /var/run/image-cri-shim.sock\ncri: /run/containerd/containerd.sock\naddress: http://sealos.hub:5000\nforce: true\ndebug: true\nimage: /var/lib/image-cri-shim\ntimeout: 15m\nauth: admin:passw0rd\n\nregistryConfig: /etc/image-cri-shim.registry.yaml\n```\n\n```\nregistries:\n- address: http://192.168.64.1:5000\n  auth: admin:passw0rd\n```",
        "closed": true,
        "closedAt": "2025-10-10T08:48:46Z",
        "number": 5978,
        "state": "CLOSED",
        "title": "Feature: image shim online regsitry auto load",
        "url": "https://github.com/labring/sealos/issues/5978",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 296   |\n| âœ… Successful | 200   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 95    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/node/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17700194044?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-14T04:54:44Z",
        "number": 5977,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5977",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIt is mainly to solve the problem of mirror offline caching. At present, all of them need to be packaged into tar before they can be offline. So is it more difficult for us to cache all cluster mirror images into registry format? We only need sealos run k8s-all to run all the clusters.\n\n\n### If you have solutionï¼Œplease describe it\n\nSealos starts the temporary registry by default, and then sealos pulls the cluster mirror image directly from this temporary registry, and sealos runs the sealos according to the configuration file.\n\n```\nFROM scratch\nMAINTAINER sealos\nLABEL init=\"...\n      \"apps.sealos.io/type\"=registry \\  \n      \"apps.sealos.io/version\"=v1beta1  \\  \n     \"apps.sealos.io/config\"=config.yaml \n```\n#### apps.sealos.io/type=registry \nDefine the type of cluster mirror as registry type.\n\n#### apps.sealos.io/config\n\nDefining the configuration of registry mainly solves the startup process problem. It will carry out sealos run according to the configuration of config. Of course, there will be no dependency problem here.\n\n```yaml\napiVersion: apps.sealos.io/v1beta1\nkind: Action\nspec:\n   check:\n      role: master,node\n      runCmd:\n      -  xxx\n      -  xxx\n   runApp:\n   - kubernetes:v1.28.15\n   - xxx\n   - xxx\n    \n\n```\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5975,
        "state": "OPEN",
        "title": "Feature: sealos v2 registry design",
        "url": "https://github.com/labring/sealos/issues/5975",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Submit PR according to my requirements\n\n1. Generate weekly community submission record PR and issue\n\n2. Count the number of PRs submitted by each person and the number of consolidations\n\n3. Or what you think is a good open source community weekly newspaper\n\n4. It's better to write a workflow timed trigger. If you can't do it, you can raise the issue regularly and submit my request.",
        "closed": true,
        "closedAt": "2025-09-11T11:01:15Z",
        "number": 5972,
        "state": "CLOSED",
        "title": "pr week report",
        "url": "https://github.com/labring/sealos/issues/5972",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 300   |\n| âœ… Successful | 203   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 96    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/license/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/architecture/controller/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17559881246?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-09T01:14:07Z",
        "number": 5953,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5953",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. Use https://github.com/labring/sealos/blob/main/scripts/cloud/build-offline-tar.sh build offline package.\n2. Use the install.sh in build path.\n3. desktop-frontend can't be runing, it always in Init:CrashLoopBackOff\n4. The log in desktop-frontend-xxx init-database container is:\n```\nPrisma schema loaded from desktop/prisma/global/schema.prisma\nDatasource \"db\": CockroachDB database \"global\", schema \"public\" at \"sealos-cockroachdb-public.sealos.svc.cluster.local:26257\"\n\nError: P1001: Can't reach database server at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`\n\nPlease make sure your database server is running at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`.\nPrisma schema loaded from desktop/prisma/region/schema.prisma\nDatasource \"db\": CockroachDB database, schema \"public\" at \"sealos-cockroachdb-public.sealos.svc.cluster.local:26257\"\n```\n\nI change the db url from:\n```\nglobalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global\"\nregionalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257\n```\nto\n```\nglobalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global?sslmode=require\"\n regionalCockroachdbURI: \"postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require\"\n```\n\nIt always have the same error.\n\n### What is the expected behavior?\n\ndesktop-frontend-xxx be running\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.1\n- Containerd version:v1.7.27\n- Kubernetes version:v1.28.11\n- Operating system:Ubuntu22.04 Linux k8s-master-01 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n- Runtime environment:\n- Cluster size:1master 4node\n```\n\n### Additional information\n\nThe cockroachdb can be connected use psql in k8s by postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require.",
        "closed": true,
        "closedAt": "2025-09-11T10:33:35Z",
        "number": 5952,
        "state": "CLOSED",
        "title": "BUG: desktop-frontend-xxx can't be running",
        "url": "https://github.com/labring/sealos/issues/5952",
        "login": "kevinydsk",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nä»€ä¹ˆæ—¶å€™æ”¯æŒå¾®è½¯çš„MSSQL\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\n### What is the problem this feature will solve?\n\nWhen will Microsoft MSSQL be supported\n\n### If you have solution, please describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_\n",
        "closed": true,
        "closedAt": "2025-09-08T08:58:43Z",
        "number": 5945,
        "state": "CLOSED",
        "title": "ä»€ä¹ˆæ—¶å€™æ”¯æŒå¾®è½¯çš„MSSQL || When will Microsoft's MSSQL be supported",
        "url": "https://github.com/labring/sealos/issues/5945",
        "login": "renziyou",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 300   |\n| âœ… Successful | 202   |\n| â³ Timeouts   | 2     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 96    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n\n### Errors in controllers/license/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17500925819?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-07T08:52:57Z",
        "number": 5944,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5944",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 300   |\n| âœ… Successful | 203   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 96    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 1     |\n\n## Errors per input\n\n### Errors in controllers/node/README.md\n\n* [ERROR] <https://kubernetes.io/docs/concepts/architecture/controller/> | Network error: error sending request for url (https://kubernetes.io/docs/concepts/architecture/controller/) Maybe a certificate error?\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17472737119?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-05T03:06:01Z",
        "number": 5938,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5938",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 342   |\n| âœ… Successful | 237   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 102   |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 3     |\n\n## Errors per input\n\n### Errors in deploy/base/victoria_metrics_k8s_stack/v1.124.0/charts/victoria-metrics-k8s-stack/charts/grafana/README.md\n\n* [404] <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#envvarsource-v1-core> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [ERROR] <https://yourgerritserver/a/%7Bproject-name%7D/branches/%7Bbranch-id%7D/files/%7Bfile-id%7D/content> | Network error: error sending request for url (https://yourgerritserver/a/%7Bproject-name%7D/branches/%7Bbranch-id%7D/files/%7Bfile-id%7D/content) Maybe a certificate error?\n* [ERROR] <https://yourgerritserver/a/user%2Frepo/branches/master/files/dir1%2Fdir2%2Fdashboard/content> | Network error: error sending request for url (https://yourgerritserver/a/user%2Frepo/branches/master/files/dir1%2Fdir2%2Fdashboard/content) Maybe a certificate error?\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17412049310?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-09-03T06:32:38Z",
        "number": 5926,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5926",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\n5.1.0\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-03T04:59:00Z",
        "number": 5924,
        "state": "CLOSED",
        "title": "BUG: delete admin and devbox deploy",
        "url": "https://github.com/labring/sealos/issues/5924",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.1.0\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/17283064704/job/49055050941 This action did not upload the mirror image normally, because there is no write permission of the package.\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T06:37:44Z",
        "number": 5906,
        "state": "CLOSED",
        "title": "BUG: release cloud error",
        "url": "https://github.com/labring/sealos/issues/5906",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nLICENSE\n\n### What is the current documentation state?\n\nCONTRIBUTING.md\nREADME.md\nREADME_zh.md\n\nIt is necessary to explain the LICENSE change declaration. According to the contenthttps://github.com/labring/sealos/blob/main/LICENSE.md and  https://github.com/labring/sealos/blob/main/CONTRIBUTOR_LICENSE_AGREEMENT.md\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-29T09:17:13Z",
        "number": 5902,
        "state": "CLOSED",
        "title": "Docs: LICENSE change declaration",
        "url": "https://github.com/labring/sealos/issues/5902",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "ä¸»è¦æµ‹è¯•ç¿»è¯‘è½¬æ¢\n<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->\n---\nMain test translation conversion\n",
        "closed": true,
        "closedAt": "2025-08-29T03:11:50Z",
        "number": 5901,
        "state": "CLOSED",
        "title": "æµ‹è¯•ç¿»è¯‘æ’ä»¶ || Test translation plugin",
        "url": "https://github.com/labring/sealos/issues/5901",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n- name: GitHub Translator\n\n  uses: lizheming/github-translate-action@1.1.2\n```",
        "closed": true,
        "closedAt": "2025-08-29T03:04:08Z",
        "number": 5899,
        "state": "CLOSED",
        "title": "GitHub Translator version error",
        "url": "https://github.com/labring/sealos/issues/5899",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "ä¸»è¦æ˜¯æµ‹ä¸€ä¸‹ç¿»è¯‘æ’ä»¶",
        "closed": true,
        "closedAt": "2025-08-29T02:31:47Z",
        "number": 5898,
        "state": "CLOSED",
        "title": "æµ‹è¯•çš„é—®é¢˜",
        "url": "https://github.com/labring/sealos/issues/5898",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).\n\n<Onboard this repo>",
        "closed": true,
        "closedAt": "2025-08-29T03:12:13Z",
        "number": 5895,
        "state": "CLOSED",
        "title": "âœ¨ Set up Copilot instructions",
        "url": "https://github.com/labring/sealos/issues/5895",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\næœˆ 27 16:26:46 slave1 kubelet[18965]: I0827 16:26:46.352575 18965 reconciler_common.go:258] \"operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host 8æœˆ 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321152 18965 scope.go:117] \"RemoveContainer\" containerID=\"301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a\" 8æœˆ 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321797 18965 scope.go:117] \"RemoveContainer\" containerID=\"301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a\" 8æœˆ 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.323939 18965 remote_runtime.go:385] \"RemoveContainer from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to set removing state for cont 8æœˆ 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324006 18965 kuberuntime_container.go:858] failed to remove pod init container \"mount-cgroup\": rpc error: code = Unknown desc = failed to set removing state 8æœˆ 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324401 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 10s 8æœˆ 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377405 18965 pod_startup_latency_tracker.go:102] \"Observed pod startup duration\" pod=\"kube-system/cilium-operator-6946ccbcc5-kfplj\" podStartSLOduration=3.187 8æœˆ 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377726 18965 pod_startup_latency_tracker.go:102] \"Observed pod startup duration\" pod=\"kube-system/kube-sealos-lvscare-slave1\" podStartSLOduration=4.377699505 8æœˆ 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.357959 18965 scope.go:117] \"RemoveContainer\" containerID=\"a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f\" 8æœˆ 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.358911 18965 scope.go:117] \"RemoveContainer\" containerID=\"a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f\" 8æœˆ 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361781 18965 remote_runtime.go:385] \"RemoveContainer from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to set removing state for cont 8æœˆ 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361857 18965 kuberuntime_container.go:858] failed to remove pod init container \"mount-cgroup\": rpc error: code = Unknown desc = failed to set removing state 8æœˆ 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.362621 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 20s 8æœˆ 27 16:27:17 slave1 kubelet[18965]: E0827 16:27:17.212276 18965 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \"StartContainer\" for \"mount-cgroup\" with CrashLoopBackOff: \"back-off 20s\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-09-11T10:34:05Z",
        "number": 5878,
        "state": "CLOSED",
        "title": "mount-cgroup",
        "url": "https://github.com/labring/sealos/issues/5878",
        "login": "dzy888",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| ğŸ” Total       | 1246  |\n| âœ… Successful  | 364   |\n| â³ Timeouts    | 0     |\n| ğŸ”€ Redirected  | 0     |\n| ğŸ‘» Excluded    | 864   |\n| â“ Unknown     | 0     |\n| ğŸš« Errors      | 16    |\n| â›” Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/4.0/i18n/zh-Hans/self-hosting/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Network error: error sending request for url (https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17192059970?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-25T06:38:02Z",
        "number": 5865,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5865",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| ğŸ” Total       | 1246  |\n| âœ… Successful  | 364   |\n| â³ Timeouts    | 0     |\n| ğŸ”€ Redirected  | 0     |\n| ğŸ‘» Excluded    | 864   |\n| â“ Unknown     | 0     |\n| ğŸš« Errors      | 16    |\n| â›” Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/4.0/i18n/zh-Hans/self-hosting/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Network error: error sending request for url (https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/build-image/build-image-with-kubeadm-run-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n\n### Errors in docs/archived/5.0/i18n/zh-Hans/developer-guide/lifecycle-management/operations/run-cluster/gen-apply-cluster.md\n\n* [ERROR] <https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/> | Error (cached)\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17178540205?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-25T02:42:52Z",
        "number": 5864,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5864",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status         | Count |\n|----------------|-------|\n| ğŸ” Total       | 1246  |\n| âœ… Successful  | 367   |\n| â³ Timeouts    | 0     |\n| ğŸ”€ Redirected  | 0     |\n| ğŸ‘» Excluded    | 864   |\n| â“ Unknown     | 0     |\n| ğŸš« Errors      | 13    |\n| â›” Unsupported | 2     |\n\n## Errors per input\n\n### Errors in docs/archived/3.0/command/exec.md\n\n* [404] <https://github.com/labring/sealos/blob/master/pkg/sshcmd/sshutil/scp.go> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/command/upgrade.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/plan/multi_network_install.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/quick_start.md\n\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_en.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Network error: error sending request for url (https://fuckcloudnative.io/) Maybe a certificate error?\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/README_zh.md\n\n* [ERROR] <https://fuckcloudnative.io/> | Error (cached)\n* [404] <https://www.sealyun.com/goodsDetail?type=cloud_kernel&name=kubernetes> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/instructions/1st> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in docs/archived/3.0/user_guide.md\n\n* [404] <https://github.com/labring/sealos/docs/etcdbackup.md> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n* [404] <https://www.sealyun.com/goodsList> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/17162608530?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-24T08:56:26Z",
        "number": 5863,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5863",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/17122518546/job/48566545145\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-29T02:30:09Z",
        "number": 5853,
        "state": "CLOSED",
        "title": "BUG:  issues-translator is not work",
        "url": "https://github.com/labring/sealos/issues/5853",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n```\n[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 -h\nAdd nodes into cluster\n\nExamples:\n\nadd to nodes :\n        sealos add --nodes x.x.x.x\n\nadd to default cluster:\n        sealos add --masters x.x.x.x --nodes x.x.x.x\n        sealos add --masters x.x.x.x-x.x.x.y --nodes x.x.x.x-x.x.x.y\n\nadd with different ssh setting:\n        sealos add --masters x.x.x.x --nodes x.x.x.x --passwd your_diff_passwd\nPlease note that the masters and nodes added in one command should have the save password.\n\nOptions:\n    --cluster='default':\n        name of cluster to applied join action\n\n    --masters='':\n        masters to be joined\n\n    --nodes='':\n        nodes to be joined\n\n    -p, --passwd='':\n        use given password to authenticate with\n\n    -i, --pk='/root/.ssh/id_rsa':\n        selects a file from which the identity (private key) for public key authentication is read\n\n    --pk-passwd='':\n        passphrase for decrypting a PEM encoded private key\n\n    --port=22:\n        port to connect to on the remote host\n\n    -u, --user='':\n        username to authenticate as\n\nUsage:\n  sealos add [flags] [options]\n\nUse \"sealos options\" for a list of global command-line options (applies to all commands).\n[root@192-168-100-62 ~]# \n\n[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 --port=65535\n2025-08-21T15:19:05 warn failed to get host arch: failed to create ssh session for 192.168.100.63:22: dial tcp 192.168.100.63:22: connect: connection refused, defaults to amd64\n2025-08-21T15:19:05 info start to scale this cluster\n2025-08-21T15:19:05 info Executing pipeline JoinCheck in ScaleProcessor.\n2025-08-21T15:19:05 info Warning: Using an even number of master nodes is a risky operation and can lead to reduced high availability and potential resource wastage. It is strongly recommended to use an odd number of master nodes for optimal cluster stability. Are you sure you want to proceed?\nDo you want to continue on '192-168-100-62' cluster? Input '192-168-100-62' to continue: 192-168-100-62\n2025-08-21T15:19:15 info checker:containerd [192.168.100.63:22]\n2025-08-21T15:19:15 info Executing pipeline PreProcess in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline PreProcessImage in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline RunConfig in ScaleProcessor.\n2025-08-21T15:19:16 info Executing pipeline MountRootfs in ScaleProcessor.\n2025-08-21T15:19:16 error error occur while sending mount image default-xlop4cm2: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n2025-08-21T15:19:16 error Applied to cluster error: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n2025-08-21T15:19:16 error failed to sync workdir: /root/.sealos/default error, failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\nError: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused\n[root@192-168-100-62 ~]#\n[root@192-168-100-62 ~]#\n\n\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-21T09:12:38Z",
        "number": 5852,
        "state": "CLOSED",
        "title": "BUG: sealos æ·»åŠ èŠ‚ç‚¹ï¼Œå‘ç°sshç«¯å£è¢«å›ºå®šæˆ22ï¼Œæ— æ³•ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£",
        "url": "https://github.com/labring/sealos/issues/5852",
        "login": "junlintianxiazhifulinzhongguo",
        "is_bot": false
      },
      {
        "body": "```\n  - name: Renew issue and Sync Images\n    uses: labring/gh-rebot@v0.0.6\n    if: ${{ github.repository_owner == env.DEFAULT_OWNER }}\n    with:\n      version: v0.0.8-rc1\n    env:\n      GH_TOKEN: \"${{ secrets.GITHUB_TOKEN }}\"\n      SEALOS_TYPE: \"issue_renew\"\n      SEALOS_ISSUE_TITLE: \"[DaylyReport] Auto build for sealos\"\n      SEALOS_ISSUE_BODYFILE: \"scripts/ISSUE_RENEW.md\"\n      SEALOS_ISSUE_LABEL: \"dayly-report\"\n      SEALOS_ISSUE_TYPE: \"day\"\n      SEALOS_ISSUE_REPO: \"labring-actions/cluster-image\"\n      SEALOS_COMMENT_BODY: \"/imagesync ghcr.io/${{ github.repository_owner }}/sealos-patch:latest\"\n\n```\n\n- [x]  delete sealos ci sync images\n- [x]  add auto sync image ci in cluster-image ",
        "closed": true,
        "closedAt": "2025-08-22T08:17:09Z",
        "number": 5850,
        "state": "CLOSED",
        "title": "remove sync images task",
        "url": "https://github.com/labring/sealos/issues/5850",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners\nTODO:\n\n- [x] add CODEOWNERS\n- [x] enable 'Require review from CODEOWNERS' in this repository",
        "closed": true,
        "closedAt": "2025-08-21T04:48:07Z",
        "number": 5848,
        "state": "CLOSED",
        "title": "add CODEOWNERS for sealos",
        "url": "https://github.com/labring/sealos/issues/5848",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Instruction:\n\nObjective: Draft a clear and actionable SECURITY.md file for Sealos. This document is intended for security researchers and users who need to report a vulnerability.\n\nPlease structure the output as follows:\n\n    Header: Title: \"Security Policy\"\n\n    Introduction: A brief statement thanking security researchers and users for helping to keep the project and its community safe.\n\n    Supported Versions: Create a table or section clearly stating which versions of the project are currently supported with security updates. Use placeholders for version numbers. For example:\n\n        [ Project Name ] | Version | Supported\n\n        ------------------ | ------- | -------------------------\n\n        [Latest Major Version] (e.g., 2.x) | >= 2.0.0 | âœ… Yes\n\n        [Previous Major Version] (e.g., 1.x) | >= 1.5.0, < 2.0.0 | âš ï¸ Best-effort\n\n        [Older Version] | < 1.5.0 | âŒ No\n\n    Reporting a Vulnerability: This is the most critical section. Provide a clear, step-by-step guide on how to report a vulnerability.\n\n        Preferred Method: Instruct reporters to privately disclose vulnerabilities by emailing a dedicated security address (e.g., security@sealos.io) or by using the provider's private feature (e.g., GitHub's \"Privately report a security vulnerability\" feature on the repository's \"Security\" tab). Emphasize the importance of NOT creating a public GitHub Issue for security bugs.\n\n        What to Include: Request that the report includes a clear description, the steps to reproduce the issue, the affected versions, and the potential impact. Mention that providing a fix or suggestion is appreciated, but not required.\n\n    What to Expect After Reporting: Set clear expectations for the reporter.\n\n        Response Time: State that the team will acknowledge the report within a specific timeframe (e.g., \"within 48 hours\").\n\n        Process Outline: Briefly explain the process: the team will triage the report, work on a fix, and keep the reporter updated throughout. Define the goal for a fix timeline (e.g., \"We aim to address critical vulnerabilities within 14 days\").\n\n        Post-Fix Disclosure: Explain the project's disclosure policy. The standard is to give reporters credit and issue a public disclosure (e.g., a GitHub Security Advisory) after a fix is released and users have had time to patch.\n\n    Vulnerability Management Philosophy: Briefly describe the project's approach to security (e.g., \"We treat security vulnerabilities as our highest priority,\" \"We believe in responsible disclosure\").\n\n    Security Updates: Explain how users will be notified of security updates. Typically, this is through:\n\n        GitHub Releases (tagged with a [security] prefix or a security fix version).\n\n        GitHub Security Advisories (GHSA).\n\n        The project's official blog or mailing list.\n\n    Final Instructions for the AI:\n\n        Use clear, concise, and professional language.\n\n        Do not invent specific contact details; Use contact information: security@sealos.io\n\n        The tone should be grateful and collaborative, encouraging security research.\n\n- Additional Reference: https://github.com/electron/electron/blob/main/SECURITY.md",
        "closed": true,
        "closedAt": "2025-09-01T05:08:21Z",
        "number": 5846,
        "state": "CLOSED",
        "title": "add SECURITY.md for sealos",
        "url": "https://github.com/labring/sealos/issues/5846",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Objective: Draft a professional and inclusive Code of Conduct for an open-source software project. The document should be ready to use but will be customized by the project maintainers.\n\nPlease structure the output as follows:\n\n    Header: Title: \"Code of Conduct\"\n\n    Introduction: A brief, welcoming paragraph stating the project's commitment to providing a welcoming and harassment-free experience for everyone, regardless of background or identity. Mention that it applies to all project spaces (e.g., GitHub repos, issue trackers, Discord/Slack, mailing lists, in-person events).\n\n    Core Principles / Our Pledge: Frame this positively. As members and maintainers, we pledge to make participation in our community a harassment-free experience for everyone. We will act and interact in ways that contribute to an open, friendly, diverse, inclusive, and healthy community.\n\n    Expected Behavior: Provide a bulleted list of positive behaviors to encourage. Include examples such as:\n\n        Using welcoming and inclusive language.\n\n        Being respectful of differing viewpoints and experiences.\n\n        Gracefully accepting constructive criticism.\n\n        Focusing on what is best for the community.\n\n        Showing empathy towards other community members.\n\n    Unacceptable Behavior: Provide a bulleted list of behaviors that are considered harassment and are unacceptable. Include examples such as:\n\n        The use of sexualized language or imagery, and unwelcome sexual attention or advances.\n\n        Trolling, insulting/derogatory comments, and personal or political attacks.\n\n        Public or private harassment.\n\n        Publishing others' private information, such as a physical or email address, without their explicit permission.\n\n        Other conduct which could reasonably be considered inappropriate in a professional setting.\n\n    Scope: Clearly state that this Code of Conduct applies within all project spaces and also applies when an individual is officially representing the project in public spaces (e.g., using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an event).\n\n    Enforcement Responsibilities & Guidelines: State that project maintainers are responsible for clarifying and enforcing standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior they deem inappropriate.\n\n        Mention that maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.\n\n    Reporting and Enforcement Procedures:\n\n        Reporting: Instruct community members to contact the project team via a private, designated channel (e.g., a dedicated email address like conduct@project.example.com or a direct message to a specific team member on a platform). Assure them that all reports will be reviewed and investigated promptly and fairly. State that the project team is obligated to maintain confidentiality with regard to the reporter of an incident.\n\n        Enforcement: Outline a general process. For example:\n\n            Correction: A private, written warning from the maintainers, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n            Warning: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period.\n\n            Temporary Ban: A temporary ban from any sort of public interaction or communication with the community.\n\n            Permanent Ban: A permanent ban from the community.\n\n    Attribution: Include an attribution line at the bottom. Use the following text:\n\n        \"This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/), version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\"\n\n    Final Instructions:\n\n        Use clear, professional, and accessible language.\n\n        Use gender-neutral language (e.g., \"they/them\").\n\n        Use contact information: contact@sealos.io.\n\n        Do not invent a specific project name; keep it generic.",
        "closed": true,
        "closedAt": "2025-08-28T06:57:22Z",
        "number": 5844,
        "state": "CLOSED",
        "title": "add CODE_OF_CONDUCT for sealos",
        "url": "https://github.com/labring/sealos/issues/5844",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nè¿›è¡Œè§†é¢‘æ›´æ–°æ›¿æ¢\n\n### What is the current documentation state?\n\n[https://objectstorageapi.hzh.sealos.run/ecv14ujz-appstore/GitHub-2.mp4](url)\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-22T03:06:40Z",
        "number": 5827,
        "state": "CLOSED",
        "title": "sealosæ¼”ç¤ºè§†é¢‘æ›´æ–°",
        "url": "https://github.com/labring/sealos/issues/5827",
        "login": "43669522",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 281   |\n| âœ… Successful | 197   |\n| â³ Timeouts   | 1     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 83    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 0     |\n\n## Errors per input\n\n### Errors in controllers/app/README.md\n\n* [TIMEOUT] <https://kubernetes.io/docs/concepts/extend-kubernetes/operator/> | Timeout\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16917045049?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-12T21:41:14Z",
        "number": 5817,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5817",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 284   |\n| âœ… Successful | 200   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 82    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16864760813?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-11T01:52:57Z",
        "number": 5810,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5810",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 284   |\n| âœ… Successful | 200   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 82    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16852233507?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-09T18:57:45Z",
        "number": 5808,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5808",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 284   |\n| âœ… Successful | 200   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 82    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16837333192?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-11T03:34:25Z",
        "number": 5805,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5805",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nInstall sealos using the [cloud installation script](https://github.com/labring/sealos/blob/main/scripts/cloud/install.sh), after changing the used Kubernetes version to v1.31.10\n\n### What is the expected behavior?\n\nSuccessful installation of sealos\n\n### What do you see instead?\n\nInstalling sealos with Kubernetes v1.13.10 fails:\n```\nAug 08 14:27:22 cloud-m1 kubelet[241103]: E0808 14:27:22.751318  241103 run.go:72] \"command failed\" err=\"failed to validate kubelet configuration, error: invalid configuration: duplicated enforcements \\\"pods\\\" in enforceNodeAllocatable (--enforce-node-allocatable), path: &TypeMeta{Kind:,APIVersion:,}\"\nAug 08 14:27:22 cloud-m1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE\n```\nCheck `/var/lib/kubelet/config.yaml`:\n```\n# grep enforceNodeAllocatable -A 2 /var/lib/kubelet/config.yaml\nenforceNodeAllocatable:\n- pods\n- pods\n```\nRemoving duplicate `pods` results in successful installation.\n\n### Operating environment\n\n```markdown\n- Sealos version: main\n- Docker version:\n- Kubernetes version: v1.31.10\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5804,
        "state": "OPEN",
        "title": "BUG: duplicate `pods` enforcements in `enforceNodeAllocatable` in `/var/lib/kubelet/config.yaml`",
        "url": "https://github.com/labring/sealos/issues/5804",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "# Summary\n\n| Status        | Count |\n|---------------|-------|\n| ğŸ” Total      | 284   |\n| âœ… Successful | 200   |\n| â³ Timeouts   | 0     |\n| ğŸ”€ Redirected | 0     |\n| ğŸ‘» Excluded   | 82    |\n| â“ Unknown    | 0     |\n| ğŸš« Errors     | 2     |\n\n## Errors per input\n\n### Errors in frontend/packages/client-sdk/README.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n\n### Errors in frontend/packages/client-sdk/README_zh.md\n\n* [404] <https://github.com/labring/sealos/blob/main/LICENSE> | Rejected status code (this depends on your \"accept\" configuration): Not Found\n[Full Github Actions output](https://github.com/labring/sealos/actions/runs/16812254845?check_suite_focus=true)\n",
        "closed": true,
        "closedAt": "2025-08-08T23:33:14Z",
        "number": 5803,
        "state": "CLOSED",
        "title": "ğŸ¤– Link Checker Report",
        "url": "https://github.com/labring/sealos/issues/5803",
        "login": "app/github-actions",
        "is_bot": true
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n# Sealos Version\nç”±äº sealos v5.0.1 release å®‰è£… kubernetes 1.31.10 ä¼šå‡ºç°å¡åœ¨ master0 çš„ cri ç‰ˆæœ¬é”™è¯¯ä¸Šï¼Œå‚ç…§ https://github.com/labring/sealos/issues/5644 çš„ä¿®å¤ï¼Œç›´æ¥æ”¹ç”¨ master çš„æºç è¿›è¡Œç¼–è¯‘çš„\n\n#  å®‰è£…æµç¨‹\n```\n sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.10 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1-amd64 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1-amd64 --masters x.x.x.x --nodes x.x.x.x\n```\n\n# é”™è¯¯æˆªå›¾\n<img width=\"975\" height=\"588\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/60411bdc-ad79-454a-ba6e-b8d61440c6ae\" />\næç¤º cilium çš„é…ç½®ä¸­å‡ºç°å‚æ•°é—æ¼é—®é¢˜ï¼Œå¯¼è‡´å®‰è£…å¤±è´¥\n\n### What is the expected behavior?\n\nkubernetes å¯ä»¥æ­£å¸¸å®‰è£…\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1(self build from latest master)\n- Docker version: None\n- Kubernetes version: 1.31.10\n- Operating system: OpenEuler\n- Runtime environment: \n- Cluster size: 1 master + 1 worker\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5797,
        "state": "OPEN",
        "title": "BUG: sealos install kubernetes 1.31.10 with cilium 1.16.1 with kubeproxyreplacement error",
        "url": "https://github.com/labring/sealos/issues/5797",
        "login": "benz9527",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nfix default values for Kubelet\n\n```\neventBurst: 100\neventRecordQPS: 50\n...\nkubeAPIBurst: 100\nkubeAPIQPS: 50\n```\n\n### If you have solutionï¼Œplease describe it\n\nhttps://github.com/kubernetes/kubernetes/pull/116121/files\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-28T09:02:57Z",
        "number": 5789,
        "state": "CLOSED",
        "title": "Feature: set Kubelet new default version ",
        "url": "https://github.com/labring/sealos/issues/5789",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-25T07:52:58Z",
        "number": 5787,
        "state": "CLOSED",
        "title": "Feature: auto assign action",
        "url": "https://github.com/labring/sealos/issues/5787",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./lifecycle) | @cuisongliu |",
        "closed": true,
        "closedAt": "2025-08-19T08:43:37Z",
        "number": 5779,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 lifecycle",
        "url": "https://github.com/labring/sealos/issues/5779",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/node) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5778,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/node",
        "url": "https://github.com/labring/sealos/issues/5778",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/account) | @zijiren233 |",
        "closed": false,
        "closedAt": null,
        "number": 5777,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/account",
        "url": "https://github.com/labring/sealos/issues/5777",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/db) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5776,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/db",
        "url": "https://github.com/labring/sealos/issues/5776",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/objectstorage) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:18Z",
        "number": 5775,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/objectstorage",
        "url": "https://github.com/labring/sealos/issues/5775",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/license) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5774,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/license",
        "url": "https://github.com/labring/sealos/issues/5774",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/app) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5773,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/app",
        "url": "https://github.com/labring/sealos/issues/5773",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:17Z",
        "number": 5772,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/job/heartbeat",
        "url": "https://github.com/labring/sealos/issues/5772",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/job/init) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5771,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/job/init",
        "url": "https://github.com/labring/sealos/issues/5771",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/terminal) | @lingdie |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5770,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/terminal",
        "url": "https://github.com/labring/sealos/issues/5770",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/user) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:16Z",
        "number": 5769,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/user",
        "url": "https://github.com/labring/sealos/issues/5769",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/resources) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5768,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/resources",
        "url": "https://github.com/labring/sealos/issues/5768",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/pkg) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5767,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/pkg",
        "url": "https://github.com/labring/sealos/issues/5767",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/devbox) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5766,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/devbox",
        "url": "https://github.com/labring/sealos/issues/5766",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./controllers/db/adminer) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:15Z",
        "number": 5765,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 controllers/db/adminer",
        "url": "https://github.com/labring/sealos/issues/5765",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/launchpad) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5764,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/launchpad",
        "url": "https://github.com/labring/sealos/issues/5764",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/database) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5763,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/database",
        "url": "https://github.com/labring/sealos/issues/5763",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/account) | @bxy4543 |",
        "closed": false,
        "closedAt": null,
        "number": 5762,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0 service/account",
        "url": "https://github.com/labring/sealos/issues/5762",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-30T10:31:57Z",
        "number": 5761,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service",
        "url": "https://github.com/labring/sealos/issues/5761",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/exceptionmonitor) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:14Z",
        "number": 5760,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/exceptionmonitor",
        "url": "https://github.com/labring/sealos/issues/5760",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/pay) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-20T09:49:13Z",
        "number": 5759,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/pay",
        "url": "https://github.com/labring/sealos/issues/5759",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "**PR: Bump golangci-lint to v2.3.0**\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/vlogs) | @zijiren233 |",
        "closed": true,
        "closedAt": "2025-08-30T10:32:05Z",
        "number": 5758,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/vlogs",
        "url": "https://github.com/labring/sealos/issues/5758",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "## PR: Bump golangci-lint to v2.3.0\n\n### ğŸ“‹ Task Assignment & Completion Status\n\n#### Services Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./service/devbox) | @bxy4543 |\n",
        "closed": true,
        "closedAt": "2025-08-20T09:49:39Z",
        "number": 5756,
        "state": "CLOSED",
        "title": "Feature: bump golangci-lint to v2.3.0 service/devbox",
        "url": "https://github.com/labring/sealos/issues/5756",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## PR: Bump golangci-lint to v2.3.1\n\nneed close #5755\n\n### ğŸ“‹ Task Assignment & Completion Status\n\n#### Services Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| âœ… | golangci-lint (./service/devbox) | @zijiren233 |\n| âœ… | golangci-lint (./service/vlogs) | @zijiren233 |\n| âœ… | golangci-lint (./service/pay) | @zijiren233 |\n| âœ… | golangci-lint (./service/exceptionmonitor) | @zijiren233 |\n| âœ… | golangci-lint (./service) | @zijiren233 |\n| â¬œ | golangci-lint (./service/account) | @bxy4543 |\n| âœ… | golangci-lint (./service/database) | @zijiren233 |\n| âœ… | golangci-lint (./service/launchpad) | @zijiren233 |\n\n#### Controllers Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| âœ… | golangci-lint (./controllers/db/adminer) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/devbox) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/pkg) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/resources) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/user) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/terminal) | @lingdie |\n| âœ… | golangci-lint (./controllers/job/init) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/app) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/license) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/objectstorage) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/db) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/account) | @zijiren233 |\n| âœ… | golangci-lint (./controllers/node) | @zijiren233 |\n\n#### Kubernetes Group\n| Status | Module | Assignee |\n|--------|--------|----------|\n| â¬œ | golangci-lint (./lifecycle) | @cuisongliu  |\n\n*Note: Replace â¬œ with âœ… when completed*\n\n### ğŸ› ï¸ Local Testing & Quick Fix Guide\n\nTo test and fix lint issues locally:\n\n```bash\n# Navigate to your module directory\ncd controllers/account\n\n# Run linter to check for issues\ngolangci-lint-v2 run --color=always --config=../../.golangci.yml\n\n# Auto-fix issues (only for linters that support --fix)\ngolangci-lint-v2 run --color=always --config=../../.golangci.yml --fix\n```\n\n**Note:** The `--fix` flag only works for certain linters. Manual fixes may still be required for some issues.",
        "closed": false,
        "closedAt": null,
        "number": 5755,
        "state": "OPEN",
        "title": "Feature: bump golangci-lint to v2.3.0",
        "url": "https://github.com/labring/sealos/issues/5755",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "åˆšç»å†ksåˆ åº“ï¼Œä¸çŸ¥é“æ•¢ä¸æ•¢å…¥æ‰‹è¿™ä¸ªæ¶æ„",
        "closed": true,
        "closedAt": "2025-08-05T08:54:09Z",
        "number": 5747,
        "state": "CLOSED",
        "title": "æƒ³é—®é—®æœªæ¥ä¼šä¸ä¼šåƒkubesphereè¿™æ ·åˆ åº“ï¼Ÿ",
        "url": "https://github.com/labring/sealos/issues/5747",
        "login": "Conmi-WhiteJoker",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nhttps://github.com/labring/sealos/pull/5719\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5722,
        "state": "OPEN",
        "title": "Feature: change sealos LICENSE task",
        "url": "https://github.com/labring/sealos/issues/5722",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\nhttps://github.com/labring/sealos/actions/runs/16489814949/job/46622113359\n\nci error\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-02T09:46:35Z",
        "number": 5721,
        "state": "CLOSED",
        "title": "BUG: ci error sealos container build",
        "url": "https://github.com/labring/sealos/issues/5721",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1ã€fedoraç¯å¢ƒï¼Œå®‰è£…5.0.1ç‰ˆæœ¬ï¼Œå®‰è£…ä¹‹åï¼Œä½¿ç”¨è‡ªç­¾åè¯ä¹¦ï¼šIP.nip.ip\n\næ­£å¸¸è®¿é—®è¯¥IP.nip.io,ä½†æ˜¯ä½¿ç”¨ç”¨æˆ·åå’Œå¯†ç ç™»å½•åï¼Œç‚¹å‡»åº”ç”¨ç®¡ç†ï¼Œè·³è½¬åˆ°äº†æ–°çš„åŸŸåä¸­\næç¤ºï¼š\nç½‘å€ä¸º https://applaunchpad.IP.nip.io/ çš„ç½‘é¡µå¯èƒ½æš‚æ—¶æ— æ³•è¿æ¥ï¼Œæˆ–è€…å®ƒå·²æ°¸ä¹…æ€§åœ°ç§»åŠ¨åˆ°äº†æ–°ç½‘å€ã€‚\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5711,
        "state": "OPEN",
        "title": "BUG: åŸŸåé…ç½®",
        "url": "https://github.com/labring/sealos/issues/5711",
        "login": "Xiaomajohn",
        "is_bot": false
      },
      {
        "body": "æºmaster 1  sealos  192.168.0.10\næºmaster 2 192.168.0.11\næºmaster 3 192.168.0.12\næƒ³é€šè¿‡æ–°å¢èŠ‚ç‚¹å‰”é™¤æ—§èŠ‚ç‚¹çš„æ–¹å¼å®ŒæˆèŠ‚ç‚¹æ›´æ¢ï¼Œé—®é¢˜æ˜¯æºmaster 1æ€ä¹ˆå‰”é™¤ï¼Œsealoså°±åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæœ€ç»ˆç»“æœå¸Œæœ›æ˜¯ï¼š\næ–°master 1 sealos  192.168.0.13\næ–°master 2 192.168.0.14\næ–°master 3 192.168.0.15",
        "closed": false,
        "closedAt": null,
        "number": 5697,
        "state": "OPEN",
        "title": "sealosæ€ä¹ˆå°†å®ƒæ‰€åœ¨çš„èŠ‚ç‚¹delete",
        "url": "https://github.com/labring/sealos/issues/5697",
        "login": "chenbsun",
        "is_bot": false
      },
      {
        "body": "1.  **æ ¸å¿ƒé—®é¢˜**ï¼š\n    *   `kube-apiserver` å‘Šè­¦ `KubeAPIServerLatencyHigh`ã€‚\n    *   æ’æŸ¥å‘ç°ï¼Œæ ¹æœ¬åŸå› æ˜¯ `enable-admission-plugins` å‚æ•°è¢«é”™è¯¯åœ°è®¾ç½®ä¸º `NodeRestriction`ï¼Œå¯¼è‡´ `validatingadmissionwebhookconfigurations` ç­‰èµ„æºç±»å‹ä¸å­˜åœ¨ã€‚\n2.  **å·²å°è¯•çš„ã€æ— æ•ˆçš„ä¿®å¤æ‰‹æ®µ**ï¼š\n    *   **è¯æ®ä¸€**ï¼šç›´æ¥ä¿®æ”¹ `/etc/kubernetes/manifests/kube-apiserver.yaml` æ–‡ä»¶ä¸ºæ­£ç¡®é…ç½®åï¼Œå¼ºåˆ¶åˆ é™¤ Pod (`kubectl delete pod ...`)ï¼Œé‡å»ºçš„å®¹å™¨é€šè¿‡ `crictl inspect` æŸ¥çœ‹ï¼Œå…¶å¯åŠ¨å‚æ•°**ä¾ç„¶æ˜¯é”™è¯¯çš„**ã€‚\n    *   **è¯æ®äºŒ**ï¼šä¿®æ”¹ `kube-system/kubeadm-config` `ConfigMap` ä¸ºæ­£ç¡®é…ç½®åï¼Œè¿è¡Œ `kubeadm upgrade apply ...` å‘½ä»¤ï¼Œè™½ç„¶å‘½ä»¤æœ¬èº«å®£å‘ŠæˆåŠŸï¼Œä½†é—®é¢˜**ä¾æ—§å­˜åœ¨**ã€‚\n    *   **è¯æ®ä¸‰**ï¼šä¿®æ”¹ `containerd` çš„é…ç½®ä»¥è§£å†³ç§æœ‰ä»“åº“è®¿é—®é—®é¢˜åï¼Œ`kubeadm` ä¾ç„¶æ— æ³•è§£å†³æ ¹æœ¬é—®é¢˜ã€‚\n    *   **è¯æ®å››**ï¼šæ£€æŸ¥äº† `/var/lib/sealos/data/default/rootfs/scripts/` ç›®å½•ä¸‹çš„è„šæœ¬ï¼Œæ²¡æœ‰å‘ç°ç›´æ¥è¦†ç›–é…ç½®çš„è¡Œä¸ºã€‚é‡å¯ `kubelet` ä¹Ÿ**æ— æ•ˆ**ã€‚\n    *   **è¯æ®äº”**ï¼šå°è¯•é€šè¿‡ `kubectl edit clusters.apps.sealos.io` æ¥ä¿®æ”¹ Sealos çš„ CRï¼Œä½†å‘ç°è¯¥ CRD **ä¸å­˜åœ¨**ã€‚\n \n## è¯·é—®ï¼ŒSealos æ˜¯é€šè¿‡ä»€ä¹ˆæœºåˆ¶æ¥ä¿è¯ `kube-apiserver` çš„é…ç½®çš„ï¼Ÿè¿™ä¸ªæœºåˆ¶çš„é…ç½®æ–‡ä»¶åœ¨å“ªé‡Œï¼Ÿæˆ‘åº”è¯¥å¦‚ä½•æ­£ç¡®åœ°ã€æ°¸ä¹…æ€§åœ°ä¿®æ”¹ `kube-apiserver` çš„å¯åŠ¨å‚æ•°ï¼Ÿ",
        "closed": true,
        "closedAt": "2025-07-18T23:59:04Z",
        "number": 5694,
        "state": "CLOSED",
        "title": "ã€ç´§æ€¥æ±‚åŠ©ã€‘Sealos å®‰è£…çš„ K8s v1.27.7 é›†ç¾¤ï¼Œkube-apiserver å¯åŠ¨å‚æ•°è¢«å¼ºåˆ¶å›ºå®šï¼Œæ— æ³•ä¿®æ”¹",
        "url": "https://github.com/labring/sealos/issues/5694",
        "login": "zinda2000",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-08-07T06:55:39Z",
        "number": 5689,
        "state": "CLOSED",
        "title": "åˆè§å´©æºƒ",
        "url": "https://github.com/labring/sealos/issues/5689",
        "login": "neo515",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n<img width=\"1902\" height=\"78\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b6ce5ee7-3c61-4840-ae71-497d9deb0b76\" />\n\n```shell\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-07-09T10:40:38 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T03:20:06Z",
        "number": 5688,
        "state": "CLOSED",
        "title": "BUG: deploy k8s cluster failed",
        "url": "https://github.com/labring/sealos/issues/5688",
        "login": "bestgopher",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nmacOS armç¯å¢ƒ\n\nä» https://github.com/labring/sealos/releases/download/v5.0.1/sealos_5.0.1_linux_arm64.tar.gz ä¸‹è½½çš„åŒ…\n\næ‰§è¡Œsealoså‘½ä»¤å¤±è´¥ï¼š\n\n```\n./sealos \nzsh: exec format error: ./sealos\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-22T15:16:20Z",
        "number": 5687,
        "state": "CLOSED",
        "title": "BUG: MacOS armç¯å¢ƒsealosäºŒè¿›åˆ¶æ–‡ä»¶æ‰§è¡Œå¤±è´¥",
        "url": "https://github.com/labring/sealos/issues/5687",
        "login": "xcbeyond",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\n1. sealosä½¿ç”¨çš„4.3.7ç‰ˆæœ¬ï¼›kuberneteså®‰è£…çš„æ˜¯1.20.15ç‰ˆæœ¬ï¼Œcorednså®‰è£…çš„æ˜¯1.8.4ç‰ˆæœ¬ï¼›ç³»ç»Ÿç‰ˆæœ¬ï¼šubuntu20.04ï¼›ç³»ç»Ÿå†…æ ¸ç‰ˆæœ¬ï¼š5.4.0-128-generic\n2. 1ä¸ªmasterèŠ‚ç‚¹\n3. å‡ºç°çš„é—®é¢˜ï¼šåœ¨podä¸­è§£æsvcæ—¶  è¶…æ—¶\nroot@master-01:~# kubectl exec -it -n rel-apollo kjtsyxzx-msa-apollo-rv-portal-5f94b47d4b-rhw5v -- sh\n/opt/msa # nslookup hd-apollo.hd-apollo.svc.cluster.local\n;; connection timed out; no servers could be reached\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5682,
        "state": "OPEN",
        "title": "corednsè§£æå¼‚å¸¸",
        "url": "https://github.com/labring/sealos/issues/5682",
        "login": "xiao1wen2",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nBase env:\n    a. OS version: Centos 7\n    b. Kernel version: 5.4.226-1.el7.elrepo.x86_64\n    c. master * 1\n\nCriVersionInfo:\n  RuntimeApiVersion: v1\n  RuntimeName: containerd\n  RuntimeVersion: v1.7.27\n  Version: 0.1.0\n\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\nHow to reproduct(offline install):\n    a. sealos pull labring/kubernetes:v1.31.0\n    b. sealos save ... && copy by manual\n    c. sealos load\n    d. sealos run labring/kubernetes:v1.31.0 -e cirData=/data/containerd --debug=true --single\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n\n```\n\n### Additional information\n\nSOME LOGS\n```\n2025-07-03T22:23:00 info start to copy static files to masters\n2025-07-03T22:23:00 info start to init master0...\nI0703 22:23:00.622532    6445 interface.go:432] Looking for default routes with IPv4 addresses\nI0703 22:23:00.622588    6445 interface.go:437] Default route transits interface \"ens160\"\nI0703 22:23:00.623008    6445 interface.go:209] Interface ens160 is up\nI0703 22:23:00.623063    6445 interface.go:257] Interface \"ens160\" has 4 addresses :[172.30.0.135/24 fe80::a1ed:122:73a0:f8ed/64 fe80::9d3b:4bf8:7733:7810/64 fe80::f6d0:5eef:587f:b799/64].\nI0703 22:23:00.623081    6445 interface.go:224] Checking addr  172.30.0.135/24.\nI0703 22:23:00.623100    6445 interface.go:231] IP found 172.30.0.135\nI0703 22:23:00.623117    6445 interface.go:263] Found valid IPv4 address 172.30.0.135 for interface \"ens160\".\nI0703 22:23:00.623125    6445 interface.go:443] Found active IP 172.30.0.135 \nI0703 22:23:00.623148    6445 kubelet.go:195] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\nvalidate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nfailed to create new CRI runtime service\nk8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect\n        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1\n        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:392\ngithub.com/spf13/cobra.(*Command).execute\n        github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n        github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n        github.com/spf13/cobra@v1.8.1/command.go:1041\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47\nmain.main\n        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n        runtime/proc.go:271\nruntime.goexit\n        runtime/asm_amd64.s:1695\n2025-07-03T22:23:00 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\n2025-07-03T22:23:00 debug save objects into local: /root/.sealos/default/Clusterfile, objects: apiVersion: apps.sealos.io/v1beta1\nkind: Cluster\nmetadata:\n  creationTimestamp: \"2025-07-03T14:23:00Z\"\n  name: default\nspec:\n  hosts:\n  - ips:\n    - 172.30.0.135:22\n    roles:\n    - master\n    - amd64\n  image:\n  - labring/kubernetes:v1.31.0\n  ssh: {}\nstatus:\n  conditions:\n  - lastHeartbeatTime: \"2025-07-03T14:23:00Z\"\n    message: 'failed to init masters: master pull image failed, error: exit status\n      1'\n    reason: Apply Cluster\n    status: \"False\"\n    type: ApplyClusterError\n  mounts:\n  - env:\n      SEALOS_SYS_CRI_ENDPOINT: /var/run/containerd/containerd.sock\n      SEALOS_SYS_IMAGE_ENDPOINT: /var/run/image-cri-shim.sock\n      cirData: /data/containerd\n      criData: /var/lib/containerd\n      defaultVIP: 10.103.97.2\n      disableApparmor: \"false\"\n      registryConfig: /etc/registry\n      registryData: /var/lib/registry\n      registryDomain: sealos.hub\n      registryPassword: passw0rd\n      registryPort: \"5000\"\n      registryUsername: admin\n      sandboxImage: pause:3.10\n    imageName: labring/kubernetes:v1.31.0\n    labels:\n      check: check.sh $registryData\n      clean: clean.sh && bash clean-cri.sh $criData\n      clean-registry: clean-registry.sh $registryData $registryConfig\n      image: ghcr.io/labring/lvscare:v5.0.1\n      init: init-cri.sh $registryDomain $registryPort && bash init.sh\n      init-registry: init-registry.sh $registryData $registryConfig\n      io.buildah.version: 1.30.0\n      org.opencontainers.image.description: kubernetes-v1.31.0 container image\n      org.opencontainers.image.licenses: MIT\n      org.opencontainers.image.source: https://github.com/labring-actions/cache\n      sealos.io.type: rootfs\n      sealos.io.version: v1beta1\n      version: v1.31.0\n      vip: $defaultVIP\n    mountPoint: /var/lib/containers/storage/overlay/e72dc732ee4dd7e9eea09ec5f3dea9a6d8c1435968e05317c1962bdb8c0748f7/merged\n    name: default-7oh654x7\n    type: rootfs\n  phase: ClusterFailed\n\n2025-07-03T22:23:00 debug sync workdir: /root/.sealos/default\n2025-07-03T22:23:00 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 172.30.0.135:22 locally\nError: failed to init masters: master pull image failed, error: exit status 1\n```",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5677,
        "state": "CLOSED",
        "title": "BUG: failed to init masters: master pull image failed",
        "url": "https://github.com/labring/sealos/issues/5677",
        "login": "Axm11",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.1.3\n\n### How to reproduce the bug?\n\n<img width=\"1496\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/474da4fb-a54c-457f-97ab-32b69d8a862f\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5676,
        "state": "OPEN",
        "title": "BUG: æ²¡æœ‰æ‰‹æœºå·ç™»é™†äº†",
        "url": "https://github.com/labring/sealos/issues/5676",
        "login": "qq547475331",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nAs u know,  u can not access docker.io directly in China. \nso I got an error when I use sealos build\n\n> Storing signatures\n> WARN[0123] Failed, retrying in 1s ... (1/3). Error: initializing source docker://alpine:latest: pinging container registry registry-1.docker.io: Get \"http://registry-1.docker.io/v2/\": dial tcp 75.126.164.178:80: i/o timeout \n\nCan I set a proxy address to connect docker.io in Sealos? How can I do that?\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5668,
        "state": "OPEN",
        "title": "Can Sealos set proxy address to connect docker.io?",
        "url": "https://github.com/labring/sealos/issues/5668",
        "login": "allenonline",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nsealos å¦‚ä½•æ„å»ºç¦»çº¿kubesphere éƒ¨ç½²é•œåƒ\n\n### What is the current documentation state?\n\nsealos å¦‚ä½•æ„å»ºç¦»çº¿kubesphere éƒ¨ç½²é•œåƒ\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-05T07:48:39Z",
        "number": 5667,
        "state": "CLOSED",
        "title": "sealos å¦‚ä½•æ„å»ºç¦»çº¿kubesphere éƒ¨ç½²é•œåƒ",
        "url": "https://github.com/labring/sealos/issues/5667",
        "login": "shunzi115",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n[root@gpustack-server01 ~]# sealos version\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\n### What is the expected behavior?\n\nå‚è€ƒæ–‡æ¡£è¿›è¡Œæ“ä½œ https://sealos.run/docs/k8s/operations/registry/sealos_sync_backup_solution\nsealos pull labring/kubernetes:v1.24.0 \nsealos create labring/kubernetes:v1.24.0\nsealos registry serve filesystem -p 9090 registry\nsealos login sealos.hub:5000\nsealos registry sync 127.0.0.1:9090 sealos.hub:5000\n\n### What do you see instead?\n\nåŒæ­¥æ“ä½œå¤±è´¥\n[root@gpustack-server01 ~]# sealos --debug  registry sync 127.0.0.1:9090 http://sealos.hub:5000\n2025-06-25T16:25:34 debug checking if endpoint http://sealos.hub:5000/v2/ is alive\n2025-06-25T16:25:34 debug checking if endpoint http://127.0.0.1:9090/v2/ is alive\n2025-06-25T16:25:34 debug http endpoint http://127.0.0.1:9090/v2/ is alive\nError: registry http status code not 200\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5663,
        "state": "OPEN",
        "title": "BUG: Sealosé›†ç¾¤é•œåƒåŒæ­¥å¤±è´¥",
        "url": "https://github.com/labring/sealos/issues/5663",
        "login": "fengjing1009",
        "is_bot": false
      },
      {
        "body": "ä¸èƒ½é‡æ–°æ³¨å†Œè´¦å·ï¼Œä¹Ÿä¸èƒ½æ¢å¤è´¦å·äº†å—ï¼Ÿæ²¡æœ‰åœ¨é¡µé¢ä¸Šçœ‹åˆ°æœ‰ä»»ä½•æç¤ºå‘¢",
        "closed": true,
        "closedAt": "2025-08-06T02:16:14Z",
        "number": 5661,
        "state": "CLOSED",
        "title": "æ³¨é”€è´¦å·åï¼Œå†æ¬¡ç™»å½•ä¸€ç›´æ˜¾ç¤ºéªŒè¯ç é”™è¯¯ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ",
        "url": "https://github.com/labring/sealos/issues/5661",
        "login": "yanweiyi11",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nä»v1.30.13 å‡çº§åˆ°v1.31.0å‡ºç°can not mix '--config' with arguments [certificate-renewal]\nroot@k8s-1:~/.sealos/default# sealos  run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\n2025-06-19T07:37:14 info start to install app in this cluster\n2025-06-19T07:37:14 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-06-19T07:37:14 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-06-19T07:37:14 info are you sure to override these following apps? \nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\nDo you want to continue on 'k8s-1' cluster? Input 'k8s-1' to continue: k8s-1\n2025-06-19T07:37:17 info Executing PreProcess Pipeline in InstallProcessor\n2025-06-19T07:37:17 info Executing pipeline MountRootfs in InstallProcessor.\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.102:22es to 192025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n2025-06-19T07:37:47 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-06-19T07:37:47 info trying default http mode to sync images to hosts [192.168.2.101:22]\n2025-06-19T07:37:49 info Executing UpgradeIfNeed Pipeline in InstallProcessor\n2025-06-19T07:37:49 info Change ClusterConfiguration up to newVersion if need.\n2025-06-19T07:37:49 info start to upgrade master0\n[config/images] Pulled registry.k8s.io/kube-apiserver:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-scheduler:v1.31.0\n[config/images] Pulled registry.k8s.io/kube-proxy:v1.31.0\n[config/images] Pulled registry.k8s.io/coredns/coredns:v1.11.3\n[config/images] Pulled registry.k8s.io/pause:3.9\n[config/images] Pulled registry.k8s.io/etcd:3.5.15-0\ncan not mix '--config' with arguments [certificate-renewal]\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:37:54 error upgrade cluster failed\nError: exit status 1\n\næ¢æˆ1.31.10 ä¹Ÿæ˜¯ä¸€æ ·çš„\nroot@k8s-1:~/.sealos/default# sealos  run labring/kubernetes:v1.31.10\n2025-06-19T07:41:54 info start to install app in this cluster\n2025-06-19T07:41:54 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-06-19T07:41:54 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-06-19T07:41:54 info Executing PreProcess Pipeline in InstallProcessor\nTrying to pull docker.lishuai.fun/labring/kubernetes:v1.31.10...\nGetting image source signatures\nCopying blob 98e5ac018847 done  \nCopying blob 2587c72eb500 done  \nCopying blob d9dc5de7feeb done  \nCopying blob f4b501a28252 done  \nCopying config 792e378009 done  \nWriting manifest to image destination\nStoring signatures\n2025-06-19T07:42:59 info Executing pipeline MountRootfs in InstallProcessor.\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.103:22es to 192025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n192.168.2.102:22es to 192025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed\n192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed\n2025-06-19T07:43:31 info Executing pipeline MirrorRegistry in InstallProcessor.\n2025-06-19T07:43:31 info trying default http mode to sync images to hosts [192.168.2.101:22]\n2025-06-19T07:43:32 info Executing UpgradeIfNeed Pipeline in InstallProcessor\n2025-06-19T07:43:32 info Change ClusterConfiguration up to newVersion if need.\n2025-06-19T07:43:32 info start to upgrade master0\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:43:33 warn image pull pre-upgrade failed: master pull image failed, error: exit status 1\ncan not mix '--config' with arguments [certificate-renewal]\nTo see the stack trace of this error execute with --v=5 or higher\n2025-06-19T07:43:33 error upgrade cluster failed\nError: exit status 1\næ„Ÿè§‰v5.0.1 è¿™ä¸ªç‰ˆæœ¬å¥½ä¹…æ²¡æ›´æ–°äº†\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1\n- Docker version:\n- Kubernetes version: 1.30.13\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5657,
        "state": "OPEN",
        "title": "BUG: sealos v5.0.1 ä»v1.30.xå‡çº§åˆ°1.31.xå¤±è´¥ï¼Œcan not mix '--config' with arguments [certificate-renewal]",
        "url": "https://github.com/labring/sealos/issues/5657",
        "login": "912988434",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nThe [build-multi-arch-image](https://sealos.run/docs/k8s/operations/build-image/build-multi-arch-image) is designed to build images targeting multi-architecture clusters. However, the images currently loaded into the cluster are single-architecture only. As a result, in mixed-architecture clusters, nodes may pull images that do not match their architecture, leading to unexpected behavior.\n\n### If you have solutionï¼Œplease describe it\n\nTo address this, I propose introducing a flag such as `--multi-arch` or `--save-multi-arch` to allow saving multi-architecture images at build time, it call into sreg and use `copy.CopyAllImages`  to save images.\n\nIâ€™ve done some preliminary work in this area:\n\n- https://github.com/labring/sealos/compare/main...cnfatal:sealos:main\n\n- https://github.com/labring/sreg/compare/main...cnfatal:sreg:main\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5652,
        "state": "OPEN",
        "title": "Feature: Allow save multi-arch images on `sealos build`",
        "url": "https://github.com/labring/sealos/issues/5652",
        "login": "cnfatal",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\næ‹‰å–çš„ç‰ˆæœ¬æ˜¯ï¼šdocker pull labring/kubernetes:v1.33.1-5.0.1 åˆ°è‡ªå·±çš„harbarä»“åº“ä¸­\n\næ‰§è¡Œ\n`sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \\\n            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \\\n            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \\\n            --masters 10.1.21.175 --nodes 10.1.21.176 \\\n            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \\\n            --pk /root/.ssh/id_rsa --user=root --cluster test131  > /root/cluster-file/test131/Clusterfile`\n\nç„¶åæ‰§è¡Œï¼š\nsealos apply -f Clusterfile --debug\n\næœ€åæŠ¥é”™ï¼š\n`Error: failed to init masters: master pull image failed, error: run command `kubeadm config images pull --cri-socket unix:///var/run/image-cri-shim.sock  --kubernetes-version v1.32.5  -v 6` on 10.1.21.175:22, output: I0610 18:39:09.550114  588091 interface.go:432] Looking for default routes with IPv4 addresses\nI0610 18:39:09.550175  588091 interface.go:437] Default route transits interface \"ens192\"\nI0610 18:39:09.550494  588091 interface.go:209] Interface ens192 is up\nI0610 18:39:09.550554  588091 interface.go:257] Interface \"ens192\" has 3 addresses :[10.1.21.175/24 10:1:21::175/64 fe80::250:56ff:fe9f:2775/64].\nI0610 18:39:09.550570  588091 interface.go:224] Checking addr  10.1.21.175/24.\nI0610 18:39:09.550579  588091 interface.go:231] IP found 10.1.21.175\nI0610 18:39:09.550602  588091 interface.go:263] Found valid IPv4 address 10.1.21.175 for interface \"ens192\".\nI0610 18:39:09.550611  588091 interface.go:443] Found active IP 10.1.21.175\nI0610 18:39:09.550630  588091 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\nvalidate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nfailed to create new CRI runtime service\nk8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect\n        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1\n        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:387\ngithub.com/spf13/cobra.(*Command).execute\n        github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n        github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n        github.com/spf13/cobra@v1.8.1/command.go:1041\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47\nmain.main\n        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n        runtime/proc.go:272\nruntime.goexit\n        runtime/asm_amd64.s:1700\n, error: Process exited with status 1,\n`\n\næˆ‘ç”¨1.29.9ä¸ä¼šæŠ¥é”™ï¼Œ1.32.5å’Œ1.33.1éƒ½ä¼šæŠ¥åŒæ ·çš„é”™è¯¯\ncontainerdç‰ˆæœ¬æ˜¯ï¼šv1.6.15\n\né”™è¯¯å¦‚ä½•è§£å†³å‘¢\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5644,
        "state": "CLOSED",
        "title": "sealoséƒ¨ç½²k8s v1.32.5é›†ç¾¤é”™è¯¯",
        "url": "https://github.com/labring/sealos/issues/5644",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\nliunxç¯å¢ƒ\nç”ŸæˆClusterfileæ–‡ä»¶å‘½ä»¤ï¼š\n`sealos gen  harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/pause:3.10 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-apiserver:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-controller-manager:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-scheduler:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/kube-proxy:v1.33.1 \\\n\t\t\tharbor.paas.nl:10081/paas_public/kube-system/coredns:v1.12.0 \\\n            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \\\n            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \\\n            --masters 10.1.21.175 --nodes 10.1.21.176 \\\n            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \\\n            --pk /root/.ssh/id_rsa --user=root --cluster test3  > /root/cluster-file/test3/Clusterfile`\n\nå…¶ä¸­kubernetes-sealos:v1.33.1-5.0.1æ˜¯æ‹‰å–äº†docker pull labring/kubernetes:v1.33.1-5.0.1 è¿™ä¸ªé•œåƒä¼ åˆ°æˆ‘è‡ªå·±çš„harborä»“åº“ä¸­çš„åœ°å€\n\næ‰§è¡ŒåæŠ¥é”™ï¼š\n\n<img width=\"1499\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/172d263c-c36d-462c-a25c-5efc815c843f\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.1\n- Docker version:æ— \n- Kubernetes version:1.33.1\n- Operating system:centos7\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T06:08:46Z",
        "number": 5642,
        "state": "CLOSED",
        "title": "BUG: sealosè‡ªå®šä¹‰å®‰è£…ï¼Œk8sç‰ˆæœ¬v1.33.1éƒ¨ç½²é›†ç¾¤å¤±è´¥",
        "url": "https://github.com/labring/sealos/issues/5642",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nnone\n\n### If you have solutionï¼Œplease describe it\n\nnone\n\n### What alternatives have you considered?\n\nnone",
        "closed": false,
        "closedAt": null,
        "number": 5640,
        "state": "OPEN",
        "title": "Feature: ä»€ä¹ˆæ—¶å€™æ”¯æŒFlutter",
        "url": "https://github.com/labring/sealos/issues/5640",
        "login": "JocelynFloresz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1.sealos version\nv5.0.1\n\n![Image](https://github.com/user-attachments/assets/f5f5358c-6158-476a-b9df-ed182d165e16)\n2.linux Operating system kernel version:\n\n![Image](https://github.com/user-attachments/assets/6f6c93c5-955d-4bc9-a4bc-c7e8a7f8aff8)\n\n3.sealos run --debug registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8\n\n4.error\n\n![Image](https://github.com/user-attachments/assets/0c859efa-f0c9-43d3-b603-04e411040f14)\n\n5.ä¸¤å‘¨å‰æ˜¯å¯ä»¥éƒ¨ç½²çš„ï¼Œæœ€è¿‘ä¸€ç›´æŠ¥ä¸Šé¢çš„é”™è¯¯ï¼Œæˆ‘ç†è§£è¿™ä¸ªåº”è¯¥æ˜¯ä¸ªbugï¼Œä¹Ÿä¸çŸ¥é“å¦‚ä½•é…ç½®èƒ½å¤Ÿè®©sealos æ‹‰å–æœ¬åœ°é•œåƒï¼Œä½¿ç”¨sealos pull å¯ä»¥æ­£å¸¸æ‹‰å–kubeç›¸å…³é•œåƒ\n\n![Image](https://github.com/user-attachments/assets/0f03d4c8-d09b-49f1-b36e-586f18cde874)\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-30T06:12:02Z",
        "number": 5638,
        "state": "CLOSED",
        "title": "The latest version ( v5.0.1 ) of the sealos deployment k8s will fail due to a public image pull",
        "url": "https://github.com/labring/sealos/issues/5638",
        "login": "yifeng-x",
        "is_bot": false
      },
      {
        "body": "æŒ‰ç…§å®˜æ–¹æ–‡æ¡£è¿›è¡Œç¦»çº¿éƒ¨ç½²ï¼Œå‘ç°sealos runæ‰§è¡Œä¸­ï¼Œä»ç„¶ä¼šä»è¿œç«¯æ‹‰å–kubeç›¸å…³çš„é•œåƒï¼Œè¯·é—®è¯¥å¦‚ä½•å¤„ç†\n\n![Image](https://github.com/user-attachments/assets/1a8ceffb-530c-4042-9299-0056b457d06f)",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5637,
        "state": "CLOSED",
        "title": "ç¦»çº¿éƒ¨ç½²sealosï¼Œé•œåƒæ‹‰å–å¤±è´¥å¯¼è‡´éƒ¨ç½²å¤±è´¥",
        "url": "https://github.com/labring/sealos/issues/5637",
        "login": "yifeng-x",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nPress the button to stop all projects at once to ensure no financial expenses.\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5634,
        "state": "OPEN",
        "title": "One-click to start or stop billing",
        "url": "https://github.com/labring/sealos/issues/5634",
        "login": "maqun02",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\nrebooté›†ç¾¤å ï¼Œk8sæ— æ³•è‡ªåŠ¨èµ·æ¥ï¼Œå¹¶ä¸”æ‰¾åˆ°ä¸åˆ° k8s ,reset ä¹‹åé‡æ–° run æŠ¥é”™ warn failed to copy image 127.0.0.1:40945/coredns:1.7.0: copying system image from manifest list: trying to reuse blob sha256:c6568d217a0023041ef9f729e8836b19f863bcdb612bb3a329ebc165539f5a80 at destination: pinging container registry 192.31.16.161:5050: received unexpected HTTP status: 503 Service Unavailable\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5616,
        "state": "OPEN",
        "title": "BUG:sealos é‡å¯æœºå™¨å é›†ç¾¤æ— æ³•è‡ªåŠ¨é‡å¯",
        "url": "https://github.com/labring/sealos/issues/5616",
        "login": "notionev",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nI attempted to test how Sealos recovers after a failure of the master0 node. During this, I discovered an issue: on control plane nodes other than master0, the host file on the host machine and the host file inside the pods are inconsistent. For example, in the controller-manager pod, when master0 becomes unavailable, controller-manager fails. The same behavior is observed in other control plane components as well.\n### Reproduce\nThe cluster is configured with three control plane nodes: master1, master2, and master3, and one worker node node1. The cluster was started with the following command:\n```sh\nroot@master1:~# sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 \\\n     --masters 192.168.64.15,192.168.64.16,192.168.64.17 \\\n     --nodes 192.168.64.18 \\\n     -u root --pk='/root/.ssh/multipass_key' \\\n     --output Clusterfile\nsealos apply -f Clusterfile\n```\nThen shutdown the master1. Below is the output from controller-manager on master2 after master1 (master0) went offline. As you can see, the DNS resolution of apiserver.cluster.local points to master1 (master0).\n```sh\nroot@master2:~# kubectl logs -n kube-system kube-controller-manager-master2 | tail -n10\nE0526 13:58:46.446458       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:58:52.590254       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:58:58.736356       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:01.811818       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:04.879391       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:11.023681       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:14.104059       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:17.166370       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:20.240788       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\nE0526 13:59:26.395983       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s\": dial tcp 192.168.64.15:6443: connect: no route to host\n```\nBy directly inspecting the hosts file under /var/lib/kubelet/pods, I confirmed this was indeed the case.\n```sh\nroot@master2:~# grep -i \"apiserver.cluster.local\" -r /var/lib/kubelet/pods/\n/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/volumes/kubernetes.io~configmap/kube-proxy/..2025_05_26_02_51_19.923862820/kubeconfig.conf:    server: https://apiserver.cluster.local:6443\n/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/etc-hosts:192.168.64.15 apiserver.cluster.local\n/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/etc-hosts:192.168.64.16 apiserver.cluster.local\nroot@master2:~# ls /var/lib/kubelet/pods/*/containers\n/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/containers:\nkube-scheduler\n\n/var/lib/kubelet/pods/362d9138-58c5-4961-b8b9-39d9aa57f8d7/containers:\ncoredns\n\n/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/containers:\nkube-controller-manager\n\n/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/containers:\nkube-proxy\n\n/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/containers:\netcd\n\n/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/containers:\napply-sysctl-overwrites  cilium-agent  clean-cilium-state  config  install-cni-binaries  mount-bpf-fs  mount-cgroup\n\n/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/containers:\nkube-apiserver\n\n/var/lib/kubelet/pods/e9afef92-76ae-461a-9fde-f105f5521e07/containers:\ncoredns\n```\nWe can see that pod 56a1a6061487d03b440de1b2e6d4cba5 is kube-controller-manager with host record\n192.168.64.15 apiserver.cluster.local.\nHowever, the host machineâ€™s /etc/hosts file looks like this:\n```sh\nroot@master2:~# cat /etc/hosts\n# Your system has configured 'manage_etc_hosts' as True.\n# As a result, if you wish for changes to this file to persist\n# then you will need to either\n# a.) make changes to the master file in /etc/cloud/templates/hosts.debian.tmpl\n# b.) change or remove the value of 'manage_etc_hosts' in\n#     /etc/cloud/cloud.cfg or cloud-config from user-data\n#\n127.0.1.1 master2 master2\n127.0.0.1 localhost\n# The following lines are desirable for IPv6 capable hosts\n::1 localhost ip6-localhost ip6-loopback\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n192.168.64.15 sealos.hub\n192.168.64.16 apiserver.cluster.local\n```\nAnd not all control plane component pods have their apiserver.cluster.local entry pointing to master1 (master0) â€” some point to the host machine instead.\nAfter reading parts of Sealosâ€™ source code, I suspect this inconsistency is caused by a data race. During the initialization of control plane nodes, Sealos modifies the host machineâ€™s /etc/hosts file twice: the first time during the init phase, it checks whether the node is a master, and if so, it points to master0. This makes sense because during join master, the node locates master0 through apiserver.cluster.local. The second modification happens after the kubeadm join command completes, at which point it only waits for the API server to become ready, but not all control plane components.\nThe first host modification in init stage\n```go\ndefaultInitializers = append(defaultInitializers, &registryHostApplier{}, &registryApplier{}, &defaultCRIInitializer{}, &apiServerHostApplier{}, &lvscareHostApplier{}, &defaultInitializer{})\n\nfunc (a *apiServerHostApplier) Apply(ctx Context, host string) error {\n    if slices.Contains(ctx.GetCluster().GetMasterIPAndPortList(), host) {\n    \tif err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetMaster0IP(), constants.DefaultAPIServerDomain); err != nil {\n    \t\treturn fmt.Errorf(\"failed to add hosts: %v\", err)\n    \t}\n    \treturn nil\n    }\n    if err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetVIP(), constants.DefaultAPIServerDomain); err != nil {\n    \treturn fmt.Errorf(\"failed to add hosts: %v\", err)\n    }\n    \n    return nil\n}\n```\nThe second host modification after kubeadm join\n```go\nfunc (k *KubeadmRuntime) joinMasters(masters []string) error {\n    // ...\n    err = k.sshCmdAsync(master, joinCmd)\n    if err != nil {\n    \treturn fmt.Errorf(\"exec kubeadm join in %s failed %v\", master, err)\n    }\n    \n    err = k.execHostsAppend(master, master, k.getAPIServerDomain())\n    if err != nil {\n    \treturn fmt.Errorf(\"add master0 apiserver domain hosts in %s failed %v\", master, err)\n    }\n    // ...\n}\n```\nThe possible cases\n```\ncase1 apiserver.cluster.local points to master0: first host modification --> pod start --> second host modification\ncase2 apiserver.cluster.local points to host machine: first host modification --> second host modification --> pod start \n```\nHere is the doc about behavior of [kubeadm join](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#feature-gates)\n```\nWithout the feature gate enabled, kubeadm will only wait for the kube-apiserver on a control plane node to become ready. \nThe wait process starts right after the kubelet on the host is started by kubeadm. \nYou are advised to enable this feature gate in case you wish to observe a ready state from all control plane components \nduring the kubeadm init or kubeadm join command execution.\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5614,
        "state": "OPEN",
        "title": "BUG: data race between control plane components and host modification",
        "url": "https://github.com/labring/sealos/issues/5614",
        "login": "A2ureStone",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\ninstall\n\n### What is the current documentation state?\n\npodå†…è®¿é—®ä¸€ä¸ªå¤–éƒ¨æœåŠ¡åœ°å€ï¼Œè¯¥åœ°å€è§£æçš„ipä¹Ÿä¸º100.64å¼€å¤´ï¼Œç»“æœæ— æ³•æ­£å¸¸è®¿é—®ï¼Œä¿®æ”¹podIPæ®µåæ‰æ­£å¸¸\n\n\n### What is your suggestion?\n\nå»ºè®®é»˜è®¤podIPç«¯è®¾ç½®ä¸ºå†…ç½‘å¸¸è§åœ°å€æ®µï¼Œå¦‚192.168ã€10ã€172ç­‰ã€‚\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-23T06:43:05Z",
        "number": 5608,
        "state": "CLOSED",
        "title": "podIPæ®µé»˜è®¤ä½¿ç”¨äº†100åœ°å€ï¼Œå¯¼è‡´æŸäº›åœºæ™¯ä¸‹ç½‘æ®µå†²çªä¸é€š",
        "url": "https://github.com/labring/sealos/issues/5608",
        "login": "gitfxx",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\nå¯¹äºä½¿ç”¨sealoséƒ¨ç½²çš„å·²ç»è¿è¡Œçš„é›†ç¾¤å»ä¿®æ”¹é…ç½®ï¼Œæ˜¯ä¸ç”Ÿæ•ˆçš„ã€‚\n\nå®˜æ–¹æ–‡æ¡£ä¸­æœ‰å…³äºè‡ªå®šä¹‰é…ç½®æ›´æ–°çš„æè¿°â€œé›†ç¾¤è¿è¡ŒæˆåŠŸåä¼šæŠŠ Clusterfile ä¿å­˜åˆ° .sealos/default/Clusterfile æ–‡ä»¶ä¸­ï¼Œå¯ä»¥ä¿®æ”¹å…¶ä¸­å­—æ®µæ¥é‡æ–° apply å¯¹é›†ç¾¤è¿›è¡Œå˜æ›´ã€‚â€ ä¸çŸ¥é“è¿™ä¸ªæ˜¯æˆ‘ç†è§£é”™è¯¯ï¼Œè¿˜æ˜¯bugæ²¡æœ‰ç”Ÿæ•ˆã€‚\nhttps://sealos.run/docs/k8s/operations/run-cluster/gen-apply-cluster \n\næˆ‘é›†ç¾¤éƒ¨ç½²è¿è¡Œèµ·æ¥åï¼Œå‘ç°calicoç½‘ç»œæœ‰äº›å¼‚å¸¸ï¼Œéœ€è¦æŒ‡å®šä¸€ä¸‹interfaceï¼Œç„¶åæˆ‘åˆ°Clusterfileä¸­æ·»åŠ äº†ç›¸å…³çš„calicoé…ç½®ï¼Œé‡æ–°sealos apply -f Clusterfile,å‘ç°æ²¡æœ‰ç”Ÿæ•ˆã€‚æˆ‘æŒ‡å®šé…ç½®çš„æ–¹æ³•ä¹Ÿæ˜¯å‚è€ƒå®˜æ–¹æ–‡æ¡£\n---\napiVersion: apps.sealos.io/v1beta1\nkind: Config\nmetadata:\n  name: calico\nspec:\n  path: charts/calico/values.yaml\n  strategy: merge\n  data: |\n    installation:\n      enabled: true\n      kubernetesProvider: \"\"\n      calicoNetwork:\n        ipPools:\n        - blockSize: 26\n          cidr: 10.160.0.0/12\n          encapsulation: IPIP\n          natOutgoing: Enabled\n          nodeSelector: all()\n        nodeAddressAutodetectionV4:\n          interface: \"eth0\"\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-23T06:42:43Z",
        "number": 5604,
        "state": "CLOSED",
        "title": "BUG: sealosè‡ªå®šä¹‰é…ç½®æ›´æ–°ä¸ç”Ÿæ•ˆ",
        "url": "https://github.com/labring/sealos/issues/5604",
        "login": "maolchen",
        "is_bot": false
      },
      {
        "body": "I'm entering the system through wsl -d Ubuntu-22.04, and this system is newly installed. I want to install a single-node version using Sealos on it. However, during the installation, it keeps saying that port 6433 is occupied.\n\nBut when I check with sudo lsof -i :6433, it shows nothing is using it. Why is that?\n\nMy installation command is:\nbash ./install.sh --cloud-version=v5.0.0 --zh --single\n\n![Image](https://github.com/user-attachments/assets/966da201-9a7b-403a-bb06-1b079670316c)",
        "closed": true,
        "closedAt": "2025-08-30T10:28:23Z",
        "number": 5596,
        "state": "CLOSED",
        "title": "When installing, I get an error saying that port 6433 is already in use.",
        "url": "https://github.com/labring/sealos/issues/5596",
        "login": "dwtyxugy",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. In this env:\n```\nLinux k8s-master-0 6.8.0-58-generic #60-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:09:50 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/arm64\n```\n2. With this config (cluster.yaml):\n```\napiVersion: apps.sealos.io/v1beta1\nkind: Cluster\nmetadata:\n  name: default\nspec:\n  hosts:\n    - ips:\n      - \"192.168.64.15\"\n      roles:\n        - master\n        - arm64\n    - ips:\n      - \"192.168.64.21\"\n      - \"192.168.64.22\"\n      - \"192.168.64.23\"\n      roles:\n        - node\n        - arm64 \n  image:\n    - registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0\n    - registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1\n    - registry.cn-shanghai.aliyuncs.com/labring/cilium:1.16.3\n  ssh:\n    passwd: xxxx\n    pk: /root/.ssh/id_rsa\n    port: 22\n    user: root\n```\n3. Run:\n```\nsealos apply -f cluster.yaml\n```\n4. See Error:\n```\n2025-05-05T00:06:40 info start to copy kubeconfig files to masters\n2025-05-05T00:06:40 info start to copy static files to masters\n2025-05-05T00:06:40 info start to init master0...\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\nTo see the stack trace of this error execute with --v=5 or higher\n2025-05-05T00:06:40 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\nError: failed to init masters: master pull image failed, error: exit status 1\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-24T06:47:27Z",
        "number": 5577,
        "state": "CLOSED",
        "title": "BUG: image pull failed",
        "url": "https://github.com/labring/sealos/issues/5577",
        "login": "monshunter",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. ubuntu 24.04  server 3å°æœºå™¨ï¼Œä½¿ç”¨çš„å¦‚ä¸‹å‘½ä»¤åœ¨ä¸€å°æœºå™¨ä¸Šå®‰è£…ï¼š\nï¼ˆåŸŸå+å†…ç½‘ipçš„æ–¹å¼å®‰è£…çš„ï¼‰\n$ curl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\n  --cloud-version=v5.0.1 \\\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\n  --proxy-prefix=$PROXY_PREFIX \\\n  --cloud-domain=<your_domain>\n\n2. å®‰è£…è¿‡ç¨‹å‡ºç°å¦‚ä¸‹é”™è¯¯ï¼Œä½†ä¸å½±å“å®‰è£…æ‰§è¡Œã€‚\nError from server (NotFound): configmaps \"desktop-frontend-config\" not found\n![Image](https://github.com/user-attachments/assets/5cb2025e-e428-4335-b934-e2a9b118e770)\n\n4. å®‰è£…å®Œæˆä¹‹åï¼Œk8sæŸ¥çœ‹podæ—¥å¿—å¦‚ä¸‹ï¼š\n\n![Image](https://github.com/user-attachments/assets/faa81804-729f-484a-b46e-62cf9590ab65)\n\n\n1ã€å®‰è£…è¿‡ç¨‹å‡ºç°é—®é¢˜ï¼šError from server (NotFound): configmaps \"desktop-frontend-config\" not foundã€‚\næ˜¯å¦éœ€è¦è§£å†³ï¼Ÿ\n\n2ã€ä½¿ç”¨åŸŸå+å†…ç½‘ipçš„è®¿é—®ä¸åˆ°ï¼Œè¿™ä¸ªæ€ä¹ˆè§£å†³ï¼Ÿ\n\n![Image](https://github.com/user-attachments/assets/3d4db61e-75bb-4270-8cc2-f0125e8b50e4)\n\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5571,
        "state": "OPEN",
        "title": "BUG: install success but doesn't work",
        "url": "https://github.com/labring/sealos/issues/5571",
        "login": "shadowlaser",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5570,
        "state": "OPEN",
        "title": "BUG: jaevor/go-nanoid deleted, replace to github.com/matoous/go-nanoid/v2 v2.1.0",
        "url": "https://github.com/labring/sealos/issues/5570",
        "login": "lingdie",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nå®‰è£…å®Œæ²¡æœ‰devboxï¼Œåº”ç”¨å•†åº—ä¹Ÿæ²¡æœ‰\n![Image](https://github.com/user-attachments/assets/269830d7-0faf-4777-a45d-6d2bf9edf326)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T10:29:40Z",
        "number": 5558,
        "state": "CLOSED",
        "title": "Feature: install cloud miss devbox runtime",
        "url": "https://github.com/labring/sealos/issues/5558",
        "login": "cx2c",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nç½‘é¡µç‰ˆæœ€æ–°\n\n### How to reproduce the bug?\n\nè¯·é—®æˆ‘æŠŠDevBoxå‘å¸ƒæˆåº”ç”¨ä¹‹åï¼Œpodä¸€ç›´æ‹‰é•œåƒå¤±è´¥æ˜¯å› ä¸ºå•¥ï¼Ÿ\n\n<img width=\"2289\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5a1061c6-8bd1-4927-97c8-b5a3729e8b7b\" />\n\n<img width=\"2558\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/31fdd36e-3e7c-463a-a30a-46f9e8e65e7a\" />\n\n<img width=\"2558\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/43ec5d0d-c8d5-4c28-a83b-ac98ea5be3fd\" />\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-04-17T06:35:04Z",
        "number": 5531,
        "state": "CLOSED",
        "title": "æ­£å¼åº”ç”¨æ— æ³•æ‹‰å–é•œåƒ",
        "url": "https://github.com/labring/sealos/issues/5531",
        "login": "Zoey-Zhang922",
        "is_bot": false
      },
      {
        "body": "According to [this post](https://securitylab.github.com/resources/github-actions-preventing-pwn-requests/), the workflow files are susceptible to leaking sensitive data like secrets of repositories.\nAs a result, we should make the following changes to the workflow files:\n1. Use [GITHUB_TOKEN](https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication) everywhere.\n2. The actions should be triggered by `pull_request` or similarly less permissive event type.\n\nThis is the issue for tracking the above change. All pull requests should reference this issue.",
        "closed": true,
        "closedAt": "2025-08-25T07:48:45Z",
        "number": 5516,
        "state": "CLOSED",
        "title": "Tracking issue for Github Actions workflow files reworking",
        "url": "https://github.com/labring/sealos/issues/5516",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n```\n[root@haisen1 ~]# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4                   registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8           --masters $MASTER_NODE_1                                                 -p Hxxxxxxen123\n2025-03-23T12:46:49 info start to install app in this cluster\n2025-03-23T12:46:49 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\n2025-03-23T12:46:49 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\n2025-03-23T12:46:49 info are you sure to override these following apps?\nregistry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4\nregistry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8\nâœ— Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue: â–ˆ\n```\n\næˆ‘æƒ³åœ¨è„šæœ¬é‡Œæ‰§è¡Œï¼Œä½†æ˜¯å®ƒä¼šé˜»å¡ä½\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-26T03:51:18Z",
        "number": 5498,
        "state": "CLOSED",
        "title": "How to automatically default and avoid interactive input Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue",
        "url": "https://github.com/labring/sealos/issues/5498",
        "login": "lhsaq2009",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n```\nwget https://****-public.oss-cn-beijing.aliyuncs.com/sealos_5.0.1_linux_amd64.rpm \\\n    && rpm -ivh sealos_5.0.1_linux_amd64.rpm                                           \\\n    && sudo yum install -y chrony iproute socat iproute-tc jq\n\nMASTER_NODE_1=$(hostname -I | awk '{print $1}')\nWORKER_NODE_1=\"${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 1 ))\"\nWORKER_NODE_2=\"${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 2 ))\"\n\n# ä»…éœ€ç¬¬ä¸€ä¸ª Master èŠ‚ç‚¹ä¸Šè¿è¡Œ sealos run å‘½ä»¤å³å¯\n# ä½¿ç”¨å†…å¤–åœ°å€å¤§çº¦ 4 åˆ†é’Ÿ é›†ç¾¤æ„å»ºå®Œæˆ\n\n# https://hub.docker.com/r/labring/kubernetes/tags\n# https://hub.docker.com/r/labring/helm/tags\n\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.8 \\\n           registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4       \\\n           registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.5     \\\n     --masters $MASTER_NODE_1                                           \\\n     --nodes   $WORKER_NODE_1,$WORKER_NODE_2                            \\\n     -p Haisen123\n\n# å®‰è£…å¤±è´¥é‡è¯•å‰ï¼Œå…ˆæ‰§è¡Œæ¸…ç†ï¼šsealos reset\n\nkubectl get nodes\n    \n    NAME      STATUS   ROLES           AGE   VERSION\n    haisen1   Ready    control-plane   67s   v1.28.8\n    haisen2   Ready    <none>          43s   v1.28.8\n    haisen3   Ready    <none>          42s   v1.28.8\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-17T12:30:32Z",
        "number": 5488,
        "state": "CLOSED",
        "title": "After installation, there is no master?",
        "url": "https://github.com/labring/sealos/issues/5488",
        "login": "lhsaq2009",
        "is_bot": false
      },
      {
        "body": "æ˜¯å¦æ”¯æŒåœ¨å·²æœ‰k8sé›†ç¾¤ä¸Š run appé•œåƒ",
        "closed": true,
        "closedAt": "2025-03-30T05:14:59Z",
        "number": 5469,
        "state": "CLOSED",
        "title": "Is it supported run app image on existing k8s clusters?",
        "url": "https://github.com/labring/sealos/issues/5469",
        "login": "BussanQ",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1-beta2-22d9a138f\n\n### How to reproduce the bug?\n\n```\ncurl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\n  --cloud-version=v5.0.1 \\\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\n  --proxy-prefix=$PROXY_PREFIX\n```\n![Image](https://github.com/user-attachments/assets/60b154ff-0ca5-4c2d-a910-5f21a2ab9b1a)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system: Ubuntu 24.04.2 LTS\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-08T13:20:31Z",
        "number": 5455,
        "state": "CLOSED",
        "title": "BUG: Error: run chmod to rootfs failed: exit status 1",
        "url": "https://github.com/labring/sealos/issues/5455",
        "login": "hubyao",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n1\n\n### What is the current documentation state?\n\nhttps://github.com/user-attachments/assets/b0a7937f-ca81-4021-a743-9d5d605fa61d\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-06T12:47:24Z",
        "number": 5442,
        "state": "CLOSED",
        "title": "update readme",
        "url": "https://github.com/labring/sealos/issues/5442",
        "login": "zuoFeng59556",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n[root@k8s-host218 cluster-file]# sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.24.2 \\\n>   --masters [10:1:12::122] --nodes [10:1:12::236] \\\n>   --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123 \\\n>   --pk /root/.ssh/id_rsa --user=root --cluster ipv6test  > /root/cluster-file/Clusterfile-ipv6\nError: invalid ip: [10:1:12::122]\n\næ— æ³•ç”¨ipv6åœ°å€ç”Ÿæˆ Clusterfileæ–‡ä»¶\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-07-15T17:30:10Z",
        "number": 5420,
        "state": "CLOSED",
        "title": "Supports sealos gen to generate ipv6 Clusterfile, and can be successfully deployed",
        "url": "https://github.com/labring/sealos/issues/5420",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nroot@ubuntu:/home/ubuntu# sealos version\nSealosVersion:\n  buildDate: \"2024-10-09T02:18:27Z\"\n  compiler: gc\n  gitCommit: 2b74a1281\n  gitVersion: 5.0.1\n  goVersion: go1.20.14\n  platform: linux/amd64\n\nroot@ubuntu:/home/ubuntu# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\nFlag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node\n2025-02-25T00:38:42 info Start to create a new cluster: master [192.168.110.132], worker [], registry 192.168.110.132\n2025-02-25T00:38:42 info Executing pipeline Check in CreateProcessor.\n2025-02-25T00:38:42 info checker:hostname [192.168.110.132:22]\n2025-02-25T00:38:42 info checker:timeSync [192.168.110.132:22]\n2025-02-25T00:38:42 info checker:containerd [192.168.110.132:22]\nError: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-25T08:51:01Z",
        "number": 5407,
        "state": "CLOSED",
        "title": "BUG: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first",
        "url": "https://github.com/labring/sealos/issues/5407",
        "login": "maybe-why-not",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n 5.0.1\n\n### How to reproduce the bug?\n\nsealos load -i xxx.tar\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n[root@localhost ~]# sealos load -i 11111.tar\nGetting image source signatures\nCopying blob 5879b58524ff done  \nCopying blob 6e76332bdae4 done  \nCopying blob ce8d20826ef0 done  \nCopying blob 8aa9e05fa73d done  \nGetting image source signatures\nCopying blob 8aa9e05fa73d done  \nCopying blob 6e76332bdae4 done  \nCopying blob 5879b58524ff done  \nCopying blob ce8d20826ef0 done  \nError: payload does not match any of the supported image formats:\n * oci: initializing source oci:11111.tar:: open 11111.tar/index.json: not a directory\n * oci-archive: writing blob: adding layer with blob \"sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552\": processing tar fil\ne(open /bin/kubeadm: permission denied): exit status 1\n * docker-archive: writing blob: adding layer with blob \"sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552\": processing tar \nfile(open /bin/kubeadm: permission denied): exit status 1\n * dir: open 11111.tar/manifest.json: not a directory\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-22T01:17:00Z",
        "number": 5397,
        "state": "CLOSED",
        "title": "BUG: Error: payload does not match any of the supported image formats",
        "url": "https://github.com/labring/sealos/issues/5397",
        "login": "zhaoyangpp",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0ã€5.0.1\n\n### How to reproduce the bug?\n\n1ã€æˆ‘å°†ç¦»çº¿åŒ…loadåˆ°æœ¬åœ°åï¼Œç„¶åç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶\n[root@k8s-1 ~]# sealos images\nREPOSITORY                         TAG        IMAGE ID       CREATED         SIZE\nlocalhost/labring/kubernetes       v1.28.14   4ab085e9b7cb   4 months ago    630 MB\nlocalhost/labring/metrics-server   v0.6.4     79507e56a004   17 months ago   30.1 MB\nlocalhost/labring/calico           v3.24.6    3d5490e2bcb4   21 months ago   355 MB\nlocalhost/labring/flannel          v0.20.2    af12a008a763   2 years ago     90 MB\n\n#ç”Ÿæˆé…ç½®æ–‡ä»¶\nsealos gen localhost/labring/kubernetes:v1.28.14 localhost/labring/calico:v3.24.6 --masters 192.168.80.141 -o clusterfile.yaml\n\n2ã€ä½¿ç”¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å®‰è£…k8sï¼Œé‡åˆ°å¦‚ä¸‹è§£æimage nameé”™è¯¯\n[root@k8s-1 ~]# sealos run -f clusterfile.yaml \n2025-02-21T01:04:16 info Start to create a new cluster: master [192.168.80.141], worker [], registry 192.168.80.141\n2025-02-21T01:04:16 info Executing pipeline Check in CreateProcessor.\n2025-02-21T01:04:16 info checker:hostname [192.168.80.141:22]\n2025-02-21T01:04:16 info checker:timeSync [192.168.80.141:22]\n2025-02-21T01:04:16 info checker:containerd [192.168.80.141:22]\n2025-02-21T01:04:16 info Executing pipeline PreProcess in CreateProcessor.\nError: error parsing image name \"//clusterfile.yaml\": pinging container registry registry-1.docker.io: Get \"http://registry-1.docker.io/v2/\": dial tcp 31.13.96.194:80: connect: connection refused\n\n### What is the expected behavior?\n\næ­£å¸¸æƒ…å†µä¸‹åº”è¯¥æ˜¯ä¸ç”¨èµ°å¤–ç½‘æ‹‰é•œåƒäº†ï¼Œè¿™é‡Œä¸çŸ¥é“ä¸ºä»€ä¹ˆä¼šè¿™æ ·\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-21T11:17:48Z",
        "number": 5396,
        "state": "CLOSED",
        "title": "BUG: Offline mode deployment encounters problem of still pulling the network image",
        "url": "https://github.com/labring/sealos/issues/5396",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1ã€sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.21.0 \\\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.12.0 \\\nregistry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1 \\\nregistry.cn-shanghai.aliyuncs.com/labring/cert-manager:v1.8.0 \\\n --masters 192.168.73.201  --nodes 192.168.73.202 -p centos -o  Clusterfile\n\n2ã€sealos apply -f Clusterfile\n\n### What is the expected behavior?\n\nk8så®‰è£…æˆåŠŸ\n\n### What do you see instead?\n\nk8så®‰è£…å¤±è´¥ï¼Œä»¥ä¸‹ä¸ºä½¿ç”¨sealos v5.0.1å®‰è£…k8s1.21.0çš„æŠ¥é”™æ—¥å¿—ï¼Œä½†æ˜¯ä½¿ç”¨åŒæ ·çš„å‘½ä»¤å®‰è£…k8s1.27.7å°±å¯ä»¥æˆåŠŸå®‰è£…k8s\næŠ¥é”™æ—¥å¿—å¦‚ä¸‹ï¼š\nW0219 23:19:27.942539   24132 strict.go:47] unknown configuration schema.GroupVersionKind{Group:\"kubeadm.k8s.io\", Version:\"v1beta3\", Kind:\"InitConfiguration\"} for scheme definitions in \"k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31\" and \"k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28\"\nno kind \"InitConfiguration\" is registered for version \"kubeadm.k8s.io/v1beta3\" in scheme \"k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31\"\nTo see the stack trace of this error execute with --v=5 or higher\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.1\n- Docker version:\n- Kubernetes version:v1.21.0\n- Operating system: centos7\n- Runtime environment: vm\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-24T01:51:39Z",
        "number": 5388,
        "state": "CLOSED",
        "title": "sealos 5.0.1 An error was reported when installing k8s 1.21 and below, and the installation of 1.27.7 was successful.",
        "url": "https://github.com/labring/sealos/issues/5388",
        "login": "houbaionee",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nç”¨github actionæ‰§è¡Œci-patch-image.ymlçš„å·¥ä½œæµç”Ÿæˆçš„sealos-amd64æ–‡ä»¶ä¼šæŠ¥é”™ï¼š\n`sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.38' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.33' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.34' not found (required by sealos2)\nsealos2: /usr/lib64/libc.so.6: version `GLIBC_2.32' not found (required by sealos2)`\n\n### If you have solutionï¼Œplease describe it\n\næˆ‘å…ˆç”¨ä½ ä»¬æä¾›çš„æ–¹å¼è¯•è¯•\nBXY4543:\nMakefileé‡Œè¾¹çš„ make\n\nBXY4543:\nOptions:\n  DEBUG            Whether or not to generate debug symbols. Default is 0.\n\n  BINS             Binaries to build. Default is all binaries under cmd.\n                   This option is available when using: make {build}(.multiarch)\n                   Example: make build BINS=\"sealos sealctl\"\n\n  PLATFORMS        Platform to build for. Default is linux_arm64 and linux_amd64.\n                   This option is available when using: make {build}.multiarch\n                   Example: make build.multiarch PLATFORMS=\"linux_arm64 linux_amd64\"\n\n  V                Set to 1 enable verbose build. Default is 0.\n\nBXY4543:\nmake build BINS=\"sealos\" PLATFORMS=\"linux_arm64 linux_amd64\"\n\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-19T16:30:49Z",
        "number": 5384,
        "state": "CLOSED",
        "title": "The system provides a non-root-run version of sealos",
        "url": "https://github.com/labring/sealos/issues/5384",
        "login": "zhuyoulong",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n1. k8s==1.28.12\n2. å½“æˆ‘ç¦æ­¢anonymous-authæ—¶ï¼Œmasterä¸nodeèŠ‚ç‚¹notreadyã€‚   æŸ¥çœ‹nodeä¸Šçš„lvscareæœåŠ¡ï¼Œæ˜¾ç¤ºè¿ä¸ä¸Šapi-server\nvim etc/kubernetes/manifests/kube-apiserver.yaml\n - --anonymous-auth=false\n\n3. curl api-server.local:6443\nè¿”å›401 unauthorized\n\n### What is the expected behavior?\n\nå¸Œæœ›èƒ½å¤Ÿæ­£å¸¸ç¦æ­¢åŒ¿åç™»å½•\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.1\n- Docker version: 26.1.4\n- Kubernetes version: 1.28.12\n- Operating system: ubuntu 24.04\n- Runtime environment: ubuntu 24.04\n- Cluster size: 3 master 3 node\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-16T20:50:58Z",
        "number": 5377,
        "state": "CLOSED",
        "title": "BUG: ç¦æ­¢åŒ¿åç™»å½•ï¼Œanonymous-auth=falseï¼ŒèŠ‚ç‚¹notready",
        "url": "https://github.com/labring/sealos/issues/5377",
        "login": "wscjxky",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\n# æ¦‚è¿°\n- æ„Ÿè§‰ä¸»è¦æ˜¯å®‰è£…å‚æ•°` --master-ips=172.19.117.70,172.19.117.71,172.19.117.72 `å¯¼è‡´çš„ï¼Œä½¿ç”¨å•èŠ‚ç‚¹æ–¹å¼ï¼Œæ²¡æœ‰è¯¥é—®é¢˜ï¼Œå¯ä»¥æ­£å¸¸æ‰‹åŠ¨æ·»åŠ \n- åœ¨ubuntu 24.04ä¸Šåº”è¯¥æ˜¯å¿…ç°é—®é¢˜ï¼Œåœ¨é˜¿é‡Œäº‘å’Œæœ¬åœ°éƒ½èƒ½å¤ç°\n\n\n# å®‰è£…å‘½ä»¤\n```\nbash install.sh  --zh   --cloud-version=v5.0.1  --image-registry=registry.cn-shanghai.aliyuncs.com  --proxy-prefix=https://mirror.ghproxy.com  --pod-cidr=100.64.0.0/10  --service-cidr=10.96.0.0/22  --cloud-port=443  --ssh-private-key=/root/.ssh/id_rsa  --kubernetes-version=1.28.11  --master-ips=172.19.117.70,172.19.117.71,172.19.117.72  --cloud-domain=sealos.mydomain.com\n```\n\n# æ“ä½œç³»ç»Ÿ\nroot@iZuf6cb62qby5xh37bhzv6Z:~/sealos_5.0.1# cat /etc/os-release \nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n\n\n\n![Image](https://github.com/user-attachments/assets/2c23128d-f072-4e89-b39a-9d1c6bb59735)\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-16T04:07:14Z",
        "number": 5376,
        "state": "CLOSED",
        "title": "BUG: Error: client rate limiter Wait returned an error: context deadline exceeded",
        "url": "https://github.com/labring/sealos/issues/5376",
        "login": "steinvenic",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nè„šæœ¬ä¸å¤ªå¥½ç”¨ï¼Œèƒ½åšåˆ°1panelé¢æ¿é‚£æ ·çš„å®‰è£…å°±å¥½äº†\n\n### What is the current documentation state?\n\nsealos é›†ç¾¤éƒ¨ç½² è¿™ä¸€å—çœ‹ä¸æ‡‚\n\n### What is your suggestion?\n\nåŠ å…¥å“”å“©å“”å“©çš„å®‰è£…è§†é¢‘\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-08-30T10:30:06Z",
        "number": 5375,
        "state": "CLOSED",
        "title": "= Is it OK to deploy sealos for a video version? Now these look a little dizzyğŸ˜µâ€ğŸ’«",
        "url": "https://github.com/labring/sealos/issues/5375",
        "login": "docker-master",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nplugin download\n\n### What is the current documentation state?\n\nhttps://hzh.sealos.run/?bd_vid=&k=&s=\nI can't open the online project from this location using curser, it tells me. \nCursor's Devbox is often not the latest. If there are any issues, please manually install the plugin referenced this URI.\n\nMicrosoft's Plugin Store does not have a download option for devbox. \nWhere can I download the plugin devbox?\n\n<img width=\"1136\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8f2bbda7-aad7-4cbf-8e42-1f216c182769\" />\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-26T00:19:42Z",
        "number": 5372,
        "state": "CLOSED",
        "title": "Docs: devbox plugin file , how  to download it?",
        "url": "https://github.com/labring/sealos/issues/5372",
        "login": "jemerci",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-05-24T20:04:09Z",
        "number": 5358,
        "state": "CLOSED",
        "title": "the ` tailscale-options` envoy filter dosn't pass headscale's `Upgrade: tailscale-control-protocol` to backend",
        "url": "https://github.com/labring/sealos/issues/5358",
        "login": "lin-calvin",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. å…ˆé€šè¿‡sealos run å®‰è£…çš„master1åŠnode1, æ­£å¸¸è¿è¡Œã€‚\n```\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.30.5 \\\nregistry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 \\\nregistry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \\\n--masters 10.100.0.1 \\\n--nodes 10.100.0.11 \\\n--user root \\\n--passwd abcd1234\n```\n2. åœ¨node1èŠ‚ç‚¹ä¸Šä¸€ä¸ªpodé‡Œé¢ä¹Ÿå¯æ­£å¸¸è®¿é—®ç½‘ç»œã€‚\n```\nroot@node1:~# kubectl exec -it 678a3ac94db887506c4d9fa0  -n xd -- /bin/sh\nDefaulted container \"app\" out of: app, lifecycle-sidecar\n/app # ping www.baidu.com\nPING www.baidu.com (110.242.68.3): 56 data bytes\n64 bytes from 110.242.68.3: seq=0 ttl=43 time=13.281 ms\n64 bytes from 110.242.68.3: seq=1 ttl=43 time=12.461 ms\n64 bytes from 110.242.68.3: seq=2 ttl=43 time=12.353 ms\n64 bytes from 110.242.68.3: seq=3 ttl=43 time=12.365 ms\n^Z[1]+  Stopped                    ping www.baidu.com\n/app # exit\nYou have stopped jobs.\n/app # exit\nroot@node1:~# ping www.baidu.com\nPING www.a.shifen.com (110.242.69.21) 56(84) bytes of data.\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=1 ttl=45 time=13.0 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=2 ttl=45 time=12.7 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=3 ttl=45 time=12.5 ms\n64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=4 ttl=45 time=12.5 ms\n^Z\n[1]+  Stopped                 ping www.baidu.com\nroot@node1:~# \n```\n\né—®é¢˜ï¼šç„¶åé€šè¿‡`sealos add --nodes 10.100.0.13` æ·»åŠ æ–°çš„èŠ‚ç‚¹ï¼Œå‘ç°é‡Œé¢åˆ›å»ºçš„podä¸èƒ½è®¿é—®ç½‘ç»œ, ä½†èŠ‚ç‚¹æœ¬èº«å¯è®¿é—®ç½‘ç»œã€‚\n```\nroot@node3:~# kubectl exec -it 6788b47bd5b570fe47ef8b0e  -n xd -- /bin/sh\nDefaulted container \"app\" out of: app, lifecycle-sidecar\n/app # ping www.baidu.com\n^Z[1]+  Stopped                    ping www.baidu.com\n/app # exit\nYou have stopped jobs.\n/app # exit\nroot@node3:~# \nroot@node3:~# ping www.baidu.com\nPING www.a.shifen.com (180.101.50.242) 56(84) bytes of data.\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=1 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=2 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=3 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=4 ttl=47 time=11.3 ms\n64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=5 ttl=47 time=11.3 ms\n^Z\n[5]+  Stopped                 ping www.baidu.com\nroot@node3:~# \n```\n\nä»¥ä¸‹æ˜¯podä¿¡æ¯ï¼š\n```\nroot@master1:~# kubectl get pods -o wide -A\nNAMESPACE     NAME                               READY   STATUS    RESTARTS          AGE     IP            NODE      NOMINATED NODE   READINESS GATES\nkube-system   cilium-2s2h4                       1/1     Running   4 (5d10h ago)     5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   cilium-6g7lm                       1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   cilium-djjgr                       1/1     Running   0                 16h     10.100.0.12   node2     <none>           <none>\nkube-system   cilium-dtcmn                       1/1     Running   0                 16h     10.100.0.13   node3     <none>           <none>\nkube-system   cilium-operator-5dccd84bff-f7jhc   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   coredns-55cb58b774-4x4xz           1/1     Running   0                 5d10h   10.0.0.215    node1     <none>           <none>\nkube-system   coredns-55cb58b774-f558l           1/1     Running   0                 5d10h   10.0.0.106    node1     <none>           <none>\nkube-system   etcd-master1                       1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-apiserver-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-controller-manager-master1    1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-proxy-284zd                   1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>\nkube-system   kube-proxy-gmtkj                   1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-proxy-n688p                   1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>\nkube-system   kube-proxy-zkcl7                   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   kube-scheduler-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>\nkube-system   kube-sealos-lvscare-node1          1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>\nkube-system   kube-sealos-lvscare-node2          1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>\nkube-system   kube-sealos-lvscare-node3          1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>\nxd          6788a6ead5b570fe47ef8b0d           2/2     Running   10 (2m35s ago)    18h     10.0.0.11     node1     <none>           <none>\nxd          6788b47bd5b570fe47ef8b0e           2/2     Running   2 (8h ago)        16h     10.0.3.78     node3     <none>           <none>\nroot@master1:~# \n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\n- Docker version:\n- Kubernetes version:\n- Operating system:\n- Runtime environment:\n- Cluster size:\n- Additional information:\n```\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-05-21T21:00:21Z",
        "number": 5355,
        "state": "CLOSED",
        "title": "sealos add can not visit network",
        "url": "https://github.com/labring/sealos/issues/5355",
        "login": "allran",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nmain\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-10T03:17:01Z",
        "number": 5332,
        "state": "CLOSED",
        "title": "BUG: frontend devbox ingress host is not correct",
        "url": "https://github.com/labring/sealos/issues/5332",
        "login": "liseri",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nSealos Version: 5.0.1\r\n\r\n### How to reproduce the bug?\r\n\r\n1. In this environment:\r\n   - Sealos 5.0.1 version installed.\r\n   - Cluster consists of 5 nodes:\r\n     - Master nodes: 192.168.37.10, 192.168.37.11, 192.168.37.12\r\n     - Worker nodes: 192.168.37.13, 192.168.37.14\r\n   - Using default Sealos configurations for cluster setup.\r\n\r\n2. With this config:\r\n   - Default configuration with Kubernetes setup and basic network settings.\r\n   - No custom modifications made to the Sealos configuration.\r\n\r\n3. Run:\r\n   Run `sealos reset --nodes 192.168.37.13` to reset only the node `192.168.37.13`.\r\n\r\n4. See error:\r\n   Instead of resetting just the node `192.168.37.13`, the command resets the entire cluster, including master and other worker nodes. This behavior is not expected, as I only intended to reset the specified node.\r\n\r\n### What is the expected behavior?\r\n\r\nI expect that when running `sealos reset --nodes 192.168.37.13`, only the node `192.168.37.13` should be reset, leaving the rest of the cluster (master and other worker nodes) intact.\r\n\r\n### What do you see instead?\r\n\r\nInstead of resetting just the specified node `192.168.37.13`, the entire cluster gets reset, including the master and other worker nodes, which is not the intended behavior.\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: 5.0.1\r\n- Docker version:\r\n- Kubernetes version: 1.28.10\r\n- Operating system: Ubuntu 22.04\r\n- Runtime environment: Physical machine (16G memory, 8 core CPU, 200GB storage)\r\n- Cluster size: 3 master, 2 node\r\n- Additional information: No additional services like Istio or Dashboard enabled.\r\n```\r\n\r\n\r\n### Additional information\r\n- The issue occurred when running the `sealos reset --nodes` command, where the reset command affected the entire cluster instead of the specified node.\r\n- Cluster is using default Sealos configuration with Kubernetes setup.\r\n- No custom configurations were applied to Sealos or Kubernetes.",
        "closed": true,
        "closedAt": "2025-04-23T16:06:14Z",
        "number": 5303,
        "state": "CLOSED",
        "title": "BUG: sealos reset Command Resets Entire Cluster Instead of Specified Node",
        "url": "https://github.com/labring/sealos/issues/5303",
        "login": "villain6666",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nError when upgrade kuberenetes from v1.29.7 to v1.30.5\r\ncan not mix '--config' with arguments [certificate-renewal]\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.0\r\n- Containerd version: 1.6.28\r\n- Kubernetes version: v1.29.7\r\n- Operating system: ubuntu 22.04\r\n- Runtime environment: containerd 1.6.28\r\n- Cluster size: 3 master\r\n- Additional information: none\n```\n\n\n### Additional information\n\n_No response_",
        "closed": false,
        "closedAt": null,
        "number": 5283,
        "state": "OPEN",
        "title": "BUG: error when upgrade kuberenetes from v1.29.7 to v1.30.5, can not mix '--config' with arguments [certificate-renewal]",
        "url": "https://github.com/labring/sealos/issues/5283",
        "login": "Ironeie",
        "is_bot": false
      },
      {
        "body": "\n## Description\n\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\n\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n\n### What To Do?\n\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n\n**For further information, suggestions, and guidance:**\n- ğŸ“– [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- ğŸ’¬ Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\n\nWe sincerely apologize for any inconvenience this may cause.\n\n**Thank you for your cooperation and understanding!** :pray:\n",
        "closed": false,
        "closedAt": null,
        "number": 5250,
        "state": "OPEN",
        "title": ":warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy",
        "url": "https://github.com/labring/sealos/issues/5250",
        "login": "camilamacedo86",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\nsealos run labring/kubernetes:v1.31.0 labring/helm:v3.12.0 labring/calico:v3.24.6 --masters xxx, return error \r\n2024-11-23T17:33:21 info start to init master0...\r\nfailed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/image-cri-shim.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\r\nTo see the stack trace of this error execute with --v=5 or higher\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-06-26T18:31:15Z",
        "number": 5231,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/5231",
        "login": "qikaih",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.1\n\n### How to reproduce the bug?\n\n1. install sealos tool v5.0.1\r\n2. install k8s use:\r\n```\r\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \\\r\n     --masters 172.17.132.44 \\\r\n     --nodes 172.17.132.43 -p xxxxxxx\r\n\r\n```\r\n3. install sealos use:\r\n```\r\n$ curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \\\r\n  --cloud-version=v5.0.1 \\\r\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\r\n  --proxy-prefix=https://mirror.ghproxy.com\r\n```\r\n4. installation failed, stuck at the â€œrun launchpad monitoringâ€ step. log is:\r\n[root@hkserver-44~.sealoslogs.txt](https://github.com/user-attachments/files/17797950/root%40hkserver-44.sealoslogs.txt)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0.1\r\n- Docker version: no\r\n- Kubernetes version:1.28.11\r\n- Operating system: centos8\r\n- Runtime environment: glibc 2.28; kernel 5.4\r\n- Cluster size: 2(Time synched)\r\n- Additional information:\r\n\r\n[root@hkserver-44 logs]# kubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS              RESTARTS      AGE\r\naccount-system              account-controller-manager-6fd48875b8-627mg                       2/2     Running             1 (14m ago)   15m\r\naccount-system              account-service-5f94b5f779-7ww8r                                  1/1     Running             0             15m\r\naccount-system              init-job-5kqgb                                                    0/1     Completed           1             15m\r\naccount-system              license-controller-manager-54fdd5d6f7-ckg9j                       2/2     Running             0             15m\r\napp-system                  app-controller-manager-fb7b86fb8-b7cbr                            2/2     Running             0             16m\r\napplaunchpad-frontend       applaunchpad-frontend-55b94f448f-rn6kf                            1/1     Running             0             14m\r\ncert-manager                cert-manager-656b9496d4-qjm7b                                     1/1     Running             0             24m\r\ncert-manager                cert-manager-cainjector-766bf6f44c-6vbf2                          1/1     Running             0             24m\r\ncert-manager                cert-manager-webhook-66b5cd7597-4rjwf                             1/1     Running             0             24m\r\ncockroach-operator-system   cockroach-operator-manager-77cdd79458-lxshg                       1/1     Running             0             23m\r\ncostcenter-frontend         costcenter-frontend-6fd7c894f-cwdn4                               1/1     Running             0             13m\r\ncronjob-frontend            cronjob-frontend-d9cbb67b4-stpjc                                  1/1     Running             0             12m\r\ndbprovider-frontend         dbprovider-frontend-b4fd55fb7-f68cg                               1/1     Running             0             13m\r\nhigress-system              higress-controller-6766c9486f-kj47d                               0/2     Pending             0             22m\r\nhigress-system              higress-gateway-8cr6w                                             0/1     ContainerCreating   0             22m\r\nhigress-system              higress-gateway-lznr7                                             0/1     ContainerCreating   0             22m\r\nkb-system                   csi-attacher-s3-0                                                 1/1     Running             0             19m\r\nkb-system                   csi-provisioner-s3-0                                              2/2     Running             0             19m\r\nkb-system                   csi-s3-jm7sg                                                      2/2     Running             0             19m\r\nkb-system                   csi-s3-r7r2p                                                      2/2     Running             0             19m\r\nkb-system                   kb-addon-migration-dt-platform-548c895976-xltfd                   0/1     ImagePullBackOff    0             19m\r\nkb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-npgvv                     1/1     Running             0             19m\r\nkb-system                   kubeblocks-85ddddddd4-rblm9                                       1/1     Running             0             21m\r\nkb-system                   kubeblocks-dataprotection-7f9c76fb8f-79nxp                        1/1     Running             0             21m\r\nkube-system                 cilium-dtfkj                                                      1/1     Running             0             142m\r\nkube-system                 cilium-operator-5448c894cc-xcwp9                                  1/1     Running             0             142m\r\nkube-system                 cilium-zrrtj                                                      1/1     Running             0             142m\r\nkube-system                 coredns-5dd5756b68-p5lh2                                          1/1     Running             0             143m\r\nkube-system                 coredns-5dd5756b68-rtpnp                                          1/1     Running             0             143m\r\nkube-system                 etcd-hkserver-44                                                  1/1     Running             4             143m\r\nkube-system                 kube-apiserver-hkserver-44                                        1/1     Running             4             143m\r\nkube-system                 kube-controller-manager-hkserver-44                               1/1     Running             11            143m\r\nkube-system                 kube-proxy-8mxl7                                                  1/1     Running             0             143m\r\nkube-system                 kube-proxy-lw465                                                  1/1     Running             0             142m\r\nkube-system                 kube-scheduler-hkserver-44                                        1/1     Running             12            143m\r\nkube-system                 kube-sealos-lvscare-hkserver-43                                   1/1     Running             3             142m\r\nkube-system                 metrics-server-68f967f7dc-j76nc                                   1/1     Running             0             23m\r\nkuboard                     kuboard-v3-hkserver-44                                            1/1     Running             0             43m\r\nlicense-frontend            license-frontend-597c57bfc9-nzxb5                                 1/1     Running             0             13m\r\nopenebs                     openebs-localpv-provisioner-56d6489bbc-zdgpb                      1/1     Running             0             23m\r\nresources-system            resources-controller-manager-77fdd6fdd4-r4vn4                     2/2     Running             0             15m\r\nsealos                      database-monitor-deployment-6b964bd695-wm42h                      1/1     Running             0             12m\r\nsealos                      desktop-frontend-59958dd9df-gktp4                                 1/1     Running             0             16m\r\nsealos                      launchpad-monitor-deployment-84fdd6884f-q9mpr                     1/1     Running             0             12m\r\nsealos                      sealos-cockroachdb-0                                              1/1     Running             0             19m\r\nsealos                      sealos-cockroachdb-1                                              1/1     Running             0             19m\r\nsealos                      sealos-cockroachdb-2                                              1/1     Running             0             19m\r\nsealos                      sealos-mongodb-mongodb-0                                          3/3     Running             0             19m\r\ntemplate-frontend           template-frontend-df96f944d-97llk                                 1/1     Running             0             13m\r\nterminal-frontend           terminal-frontend-7969b49c99-5drs5                                1/1     Running             0             14m\r\nterminal-system             terminal-controller-manager-79fc658d89-qzm4k                      2/2     Running             0             16m\r\nuser-system                 user-controller-manager-658d4445bb-dqxcn                          2/2     Running             0             16m\r\nvm                          victoria-metrics-k8s-stack-grafana-76cfcccd7f-7tsgg               3/3     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-tbbmj    1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-p6794         1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-rzd6l         1/1     Running             0             22m\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bch94f   1/1     Running             0             22m\r\nvm                          vmagent-victoria-metrics-k8s-stack-69857ff7c7-458hw               2/2     Running             0             19m\r\nvm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-s6sx7               2/2     Running             0             22m\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running             0             22m\r\nvm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-dxrls               1/1     Running             0             22m\r\n```\n```\n\n\n### Additional information\n\nhow to debug/log/resolve it;",
        "closed": true,
        "closedAt": "2025-01-07T10:41:48Z",
        "number": 5213,
        "state": "CLOSED",
        "title": "Sealos v5.0.1 self-hosted installation failed, stuck at the â€œrun launchpad monitoringâ€ step.",
        "url": "https://github.com/labring/sealos/issues/5213",
        "login": "liseri",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\næ€ä¹ˆæŠŠæœ¬åœ°å·²å­˜åœ¨çš„ä»£ç é¡¹ç›®å…³è”åˆ°è¿œç¨‹åˆšåˆ›å»ºçš„Devbooxé¡¹ç›®ä¸‹é¢ï¼Œè¿™éƒ¨åˆ†çš„æµç¨‹æ–‡æ¡£è¯´æ˜\n\n### What is the current documentation state?\n\nå»åˆ°æ–‡æ¡£é“¾æ¥éƒ¨åˆ†æœªçœ‹åˆ°è¿™éƒ¨åˆ†çš„æ–‡æ¡£è¾“å‡º\n\n### What is your suggestion?\n\nè¡¥å……ä¸€ä¸‹æ€ä¹ˆæŠŠæœ¬åœ°å·²å­˜åœ¨çš„ä»£ç é¡¹ç›®å…³è”åˆ°è¿œç¨‹åˆšåˆ›å»ºçš„Devbooxé¡¹ç›®ä¸‹é¢ï¼Œè¿™éƒ¨åˆ†çš„æµç¨‹æ–‡æ¡£è¯´æ˜\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-16T16:10:07Z",
        "number": 5211,
        "state": "CLOSED",
        "title": "About the local existing code associated with the remotely created Devbox project",
        "url": "https://github.com/labring/sealos/issues/5211",
        "login": "cfy3133",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nç½‘é¡µç‰ˆçš„\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\nvueåº”ç”¨è¿æ¥ä¸ä¸Šcursor\r\n\n\n### What do you see instead?\n\nä¸€ç›´æ˜¾ç¤º â€˜â€™è®¾ç½® SSH ä¸»æœº gzg.sealos.run-ns-d1l8go8n-devboxreact-scxvqiz699: æ­£åœ¨ä¸‹è½½ VS Code æœåŠ¡å™¨â€˜â€™  ä¸€ç›´è½¬åœˆåœˆè¿ä¸ä¸Š\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nnode.jsæ˜¯å¯ä»¥è¿æ¥ä¸Š",
        "closed": true,
        "closedAt": "2025-03-10T04:46:03Z",
        "number": 5201,
        "state": "CLOSED",
        "title": "vue application cannot connect to cursor",
        "url": "https://github.com/labring/sealos/issues/5201",
        "login": "wenbo-quan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nå½“å‰ç‰ˆæœ¬\n\n### How to reproduce the bug?\n\næ–°åŠ å¡åŒºäº‘å¼€å‘idï¼špxyyxos06z\r\næ— æ³•å¯åŠ¨ï¼Œæ— æ³•åœæ­¢ï¼Œå¯¼è‡´æ— æ³•åˆ é™¤ã€‚\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-10T04:46:02Z",
        "number": 5200,
        "state": "CLOSED",
        "title": "Singapore cloud development ID: pxyyxos06z cannot be started, cannot be stopped, and cannot be deleted.",
        "url": "https://github.com/labring/sealos/issues/5200",
        "login": "imococo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0\n\n### How to reproduce the bug?\n\nProblems encountered during make build\n\n### What is the expected behavior?\n\n===========> Building binary lvscare v5.0.1 for linux_amd64\r\n===========> Building binary sealctl v5.0.1 for linux_amd64\r\n# github.com/labring/sealos/cmd/sealctl\r\n/usr/local/go/pkg/tool/linux_amd64/link: running x86_64-linux-gnu-gcc failed: exit status 1\r\n/usr/bin/x86_64-linux-gnu-gcc -m64 -s -Wl,-z,now -Wl,-z,nocopyreloc -o $WORK/b001/exe/a.out -rdynamic /tmp/go-link-3363692486/go.o /tmp/go-link-3363692486/000000.o /tmp/go-link-3363692486/000001.o /tmp/go-link-3363692486/000002.o /tmp/go-link-3363692486/000003.o /tmp/go-link-3363692486/000004.o /tmp/go-link-3363692486/000005.o /tmp/go-link-3363692486/000006.o /tmp/go-link-3363692486/000007.o /tmp/go-link-3363692486/000008.o /tmp/go-link-3363692486/000009.o /tmp/go-link-3363692486/000010.o /tmp/go-link-3363692486/000011.o /tmp/go-link-3363692486/000012.o /tmp/go-link-3363692486/000013.o /tmp/go-link-3363692486/000014.o /tmp/go-link-3363692486/000015.o /tmp/go-link-3363692486/000016.o /tmp/go-link-3363692486/000017.o /tmp/go-link-3363692486/000018.o /tmp/go-link-3363692486/000019.o /tmp/go-link-3363692486/000020.o /tmp/go-link-3363692486/000021.o /tmp/go-link-3363692486/000022.o /tmp/go-link-3363692486/000023.o /tmp/go-link-3363692486/000024.o /tmp/go-link-3363692486/000025.o /tmp/go-link-3363692486/000026.o /tmp/go-link-3363692486/000027.o -O2 -g -O2 -g -lpthread -O2 -g -ldl -O2 -g -O2 -g -O2 -g -ldl\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crt1.o: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crti.o: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -lpthread: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find -lc: No such file or directory\r\n/usr/bin/x86_64-linux-gnu-ld: cannot find crtn.o: No such file or directory\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[1]: *** [scripts/make-rules/golang.mk:60: go.build.linux_amd64.sealctl] Error 1\r\nmake: *** [Makefile:58: build] Error 2\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-01T03:51:35Z",
        "number": 5185,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/5185",
        "login": "AutuSnow",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.1\n\n### How to reproduce the bug?\n\nsealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1     \r\n\r\ncilium  1.16.0 or 1.16.1  \r\n\r\n```\r\nâ„¹ï¸  Using Cilium version 1.16.0\r\n Auto-detected cluster name: kubernetes\r\n Auto-detected kube-proxy has been installed\r\n\r\nError: Unable to install Cilium: execution error at (cilium/templates/cilium-configmap.yaml:71:5): kubeProxyReplacement must be explicitly set to a valid value (true or false) to continue.\r\n\r\n```\r\n\n\n### What is the expected behavior?\n\nWhere do I need to specify kubeProxyReplacement\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-03T05:08:48Z",
        "number": 5184,
        "state": "CLOSED",
        "title": "install Cilium version 1.16.0  error",
        "url": "https://github.com/labring/sealos/issues/5184",
        "login": "dxygit1",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-03-01T01:51:34Z",
        "number": 5183,
        "state": "CLOSED",
        "title": "I want to independently deploy etcd to a host outside the cluster. Does sealos support offline deployment of etcd?",
        "url": "https://github.com/labring/sealos/issues/5183",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n2024-10-29T22:13:45 debug sync workdir: /root/.sealos/default\r\n2024-10-29T22:13:45 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 10.4.0.95:22 locally\r\nError: could not get local addresses: route ip+net: no such network interface\r\n\r\n```\r\nfunc New(inner ssh.Interface) (Interface, error) {\r\n\taddr, err := net.InterfaceAddrs()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"could not get local addresses: %v\", err)\r\n\t}\r\n\treturn &wrap{\r\n\t\tinner:          inner,\r\n\t\tlocalAddresses: sets.Set[string](netutil.AddressSet(isValid, addr)),\r\n\t}, nil\r\n}\r\n```\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:56Z",
        "number": 5180,
        "state": "CLOSED",
        "title": "BUG: can't get local addresses in single node",
        "url": "https://github.com/labring/sealos/issues/5180",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nsealos run  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17   ----\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\r\n[kubelet-check] Initial timeout of 40s passed.\r\n\r\n        Unfortunately, an error has occurred:\r\n                timed out waiting for the condition\r\n\r\n        This error is likely caused by:\r\n                - The kubelet is not running\r\n                - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\r\n\r\n        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\r\n                - 'systemctl status kubelet'\r\n                - 'journalctl -xeu kubelet'\r\n\r\n        Additionally, a control plane component may have crashed or exited when started by the container runtime.\r\n        To troubleshoot, list all containers using your preferred container runtimes CLI.\r\n\r\n        Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl:\r\n                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'\r\n                Once you have found the failing container, you can inspect its logs with:\r\n                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'\r\n\r\nerror execution phase wait-control-plane: couldn't initialize a Kubernetes cluster\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-10-29T12:21:13 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\nError: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:56Z",
        "number": 5179,
        "state": "CLOSED",
        "title": "registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17  install error",
        "url": "https://github.com/labring/sealos/issues/5179",
        "login": "biao-lvwan",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nå½“å‰åº”ç”¨å•†åº—æ¨¡æ¿è¿‡å°‘ï¼Œå¾ˆå¤šåœ°æ–¹ä¸èƒ½å¤Ÿè‡ªå®šä¹‰ï¼Œéƒ¨åˆ†åœ¨å®é™…ä½¿ç”¨æ—¶è¿˜ä¼šæœ‰bugï¼Œå¹¶ä¸”åº”ç”¨ç®¡ç†å½“ä¸­åˆ›å»ºåº”ç”¨çš„åŠŸèƒ½å¤ªä¸çµæ´»äº†ã€‚\n\n### If you have solutionï¼Œplease describe it\n\nè¯»å–ç½‘ç»œæ–‡ä»¶ï¼Œæˆ–è€…ä»¥yamlç¼–è¾‘å™¨çš„å½¢å¼ä¼ å…¥å†™å¥½çš„æ–‡ä»¶ï¼Œå’Œæ­£å¸¸åº”ç”¨å•†åº—é‡Œé¢çš„æ¨¡æ¿ä¸€æ ·è¢«éƒ¨ç½²ï¼Œå¦‚æœæœ‰å®‰å…¨è€ƒè™‘çš„è¯ï¼Œå¯ä»¥ä¸æ”¯æŒç”¨æˆ·çœ‹åˆ°å…¶ä»–ç”¨æˆ·çš„è‡ªå®šä¹‰æ¨¡æ¿ï¼Œä¸ç„¶çš„è¯ï¼Œæˆ‘è§‰å¾—åœ¨ç”¨æˆ·æœ¬äººå…è®¸çš„å‰æä¸‹ï¼Œå¯ä»¥å¼€æ”¾è®©å…¶ä»–ç”¨æˆ·è·å–åˆ°ç¬¬ä¸‰æ–¹æ¨¡æ¿ã€‚è¿™æ ·å­å¯¹å¾ˆå¤šsealosçš„æ½œåœ¨ç”¨æˆ·è€Œè¨€ï¼Œå¯ä»¥è§£å†³å¾ˆå¤šç—›ç‚¹é—®é¢˜ï¼ŒåŒæ—¶å¯¹äºå¾ˆå¤šæœ‰å®šåˆ¶åŒ–éœ€æ±‚çš„å°å›¢é˜Ÿä¹Ÿå¯ä»¥æ›´çµæ´»çš„ä½¿ç”¨ã€‚\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-27T12:45:57Z",
        "number": 5176,
        "state": "CLOSED",
        "title": "Does the official intend to develop third-party or custom application template deployment?",
        "url": "https://github.com/labring/sealos/issues/5176",
        "login": "VocabVictor",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nNone\n\n### How to reproduce the bug?\n\nI tried to add a custom domain name to my project today, but I found that it could not be added successfully, even though I had parsed it 6 hours later, as shown in the picture:\r\n\r\n![pic](https://github.com/user-attachments/assets/f32289f8-6272-49b6-9342-684d166e22a2)\r\n\r\nI have also seen many users reporting this problem. Is it possible to try without verifying the CNAME? My parsing is on CloudFlare.\r\n\r\n![image](https://github.com/user-attachments/assets/25e9ebee-f426-4d6c-a64c-9efcf6e8e98d)\n\n### What is the expected behavior?\n\ni hope to improve\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-18T13:08:46Z",
        "number": 5155,
        "state": "CLOSED",
        "title": "BUG: Problem adding custom domain name to sealos.io",
        "url": "https://github.com/labring/sealos/issues/5155",
        "login": "alltobebetter",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nDisable the verification of domain name resolution when adding custom domains\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-02-26T03:33:15Z",
        "number": 5152,
        "state": "CLOSED",
        "title": "How to disable verification for adding custom domain names",
        "url": "https://github.com/labring/sealos/issues/5152",
        "login": "clstech",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\nrootfsï¼š\r\n![QQ_1727251867263](https://github.com/user-attachments/assets/3cf27fbd-30d1-4e72-bdeb-3a4847168e08)\r\n\n\n### What is the expected behavior?\n\nsealos run è‡ªå·±æ‰“åŒ…çš„é•œåƒï¼Œå°†cni ingress-nginx é€šè¿‡helmçš„æ–¹å¼è¿›è¡Œinstallå®‰è£…ï¼Œsealos run é•œåƒ:v1.0.1 --masters 192.10.100.1 èƒ½å¤Ÿæ­£å¸¸è¿è¡Œæ•´ä¸ªé›†ç¾¤ï¼Œç½‘ç»œç»„ä»¶ ingress æ­£å¸¸å°±ç»ªï¼Œä½†æ˜¯æˆ‘é€šè¿‡sealos run é•œåƒ:v1.0.1 --masters  192.10.100.1  --nodes 192.10.100.2 å°±ä¼šæŠ¥é”™ Error: master not allow empty\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-27T05:14:44Z",
        "number": 5107,
        "state": "CLOSED",
        "title": "sealos add node Error: master not allow empty",
        "url": "https://github.com/labring/sealos/issues/5107",
        "login": "sharkat",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n![QQ_1727251467658](https://github.com/user-attachments/assets/2812456b-11fd-4af8-b4d8-8cc4a7694723)\r\n![QQ_1727251490677](https://github.com/user-attachments/assets/82097b4b-280f-4ce5-a903-670af50825fd)\r\n![QQ_1727251530883](https://github.com/user-attachments/assets/55699506-b733-4760-993b-7c38a632e726)\r\n![QQ_1727251563595](https://github.com/user-attachments/assets/b1921db2-9517-4dbe-b8fa-b28185ae4a09)\r\n![QQ_1727251595720](https://github.com/user-attachments/assets/e69c02af-7b8a-430d-8832-8bbcd2efb3af)\r\n\n\n### What is the expected behavior?\n\nsealos run images:v0.0.1 æ‰§è¡Œç›¸å…³è„šæœ¬é€šè¿‡helm installä¸­é—´ä»¶ è°ƒç”¨æ£€æŸ¥è„šæœ¬ ç­‰å¾…ç›¸å…³podå°±ç»ªï¼Œæ‰‹åŠ¨æ‰§è¡Œè„šæœ¬æ²¡é—®é¢˜ï¼Œé€šè¿‡runæ‰§è¡Œåˆ°ä¸­é€”Error: signal: killed å°±æŠ¥é”™äº†ã€‚\n\n### What do you see instead?\n\nError: signal: killed \n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.0\r\n- Docker version:\r\n- Kubernetes version:1.27.7\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T09:52:41Z",
        "number": 5106,
        "state": "CLOSED",
        "title": "sealos run Error: signal: killed",
        "url": "https://github.com/labring/sealos/issues/5106",
        "login": "sharkat",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0\r\n\r\n### How to reproduce the bug?\r\n\r\n```\r\nsealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\n```\r\n\r\n### What is the expected behavior?\r\n\r\nno warn message\r\n\r\n### What do you see instead?\r\n\r\nget warn message\r\n\r\n```\r\n2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip\r\n```\r\n```\r\nroot@ubuntu:/data/cluster-image/kubernetes-nmstate/latest# sealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\n2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip\r\nGetting image source signatures\r\nCopying blob 52b6e7f862c8 done  \r\nCopying blob 68f4756818ef done  \r\nCopying config 6a28855840 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\nGetting image source signatures\r\nCopying blob 5a8fd2560a4c done  \r\nCopying blob eac1b95df832 done  \r\nCopying blob e5f565e86761 done  \r\nCopying blob d382f544b01f done  \r\nCopying blob 47aa3ed2034c done  \r\nCopying blob 7c282516a5e0 done  \r\nCopying config 19fe8540f8 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\nGetting image source signatures\r\nCopying blob e500b42a2bb5 skipped: already exists  \r\nCopying blob 2895d6faeea8 skipped: already exists  \r\nCopying config 4268cf1312 done  \r\nWriting manifest to image destination\r\nStoring signatures\r\n2024-09-14T17:56:03 info saving images quay.io/nmstate/kubernetes-nmstate-handler:v0.82.0, quay.io/openshift/origin-kube-rbac-proxy:4.10.0, quay.io/nmstate/kubernetes-nmstate-operator:v0.82.0\r\nSTEP 1/3: FROM scratch\r\nSTEP 2/3: COPY . .\r\nSTEP 3/3: CMD [\"bash entrypoint.sh\"]\r\nCOMMIT registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\nGetting image source signatures\r\nCopying blob 46100663e36e done  \r\nCopying config b90302488f done  \r\nWriting manifest to image destination\r\nStoring signatures\r\n--> b90302488f6a\r\nSuccessfully tagged registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0\r\nb90302488f6abf1e33910c9b8638e56260e3418a1c166ebe1a8f85324022b232\r\n```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2025-01-14T03:58:24Z",
        "number": 5083,
        "state": "CLOSED",
        "title": "BUG:  warn image tar config is not exists,skip",
        "url": "https://github.com/labring/sealos/issues/5083",
        "login": "weironz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n2. ip change\r\n3. sealos reset --force\r\n4. rm -rf /usr/bin/sealos\r\n5. rm -rf /root/.sealos\r\n6. rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd /root/.kube\r\n7. sudo apt-get remove --purge docker-ce docker-ce-cli containerd.io \r\n8. sudo rm -rf /var/lib/docker /var/lib/containerd\r\n9. rm find / -name \"*sealos*\" 2>/dev/null\r\n10. sudo reboot\r\n11. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n12. ERROR: No such file or directoryscripts/init.sh:line 18:etc/sealos/.env \n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nI thought etc/sealos/.env was created by the script, but during execution it showed that there was no such file.",
        "closed": true,
        "closedAt": "2024-09-18T08:54:09Z",
        "number": 5065,
        "state": "CLOSED",
        "title": "etc/sealos/.env: No such file or directory",
        "url": "https://github.com/labring/sealos/issues/5065",
        "login": "Qiuqi777",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1. sealos reset\n\n### What is the expected behavior?\n\n[root@master0 deployment]# sealos reset --force --masters=xx.xx.xx.xx\r\n2024-09-09T14:08:17 info start to delete Cluster: master [10.105.5.233], node []\r\n2024-09-09T14:08:17 info start to reset nodes: []\r\n2024-09-09T14:08:17 info start to reset masters: [10.105.5.233:22]\r\n2024-09-09T14:08:17 info start to reset node: 10.105.5.233:22\r\n/usr/bin/kubeadm\r\n[preflight] Running pre-flight checks\r\nW0909 14:08:17.401140   12652 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\r\n[reset] Deleted contents of the etcd data directory: /var/lib/etcd\r\n[reset] Stopping the kubelet service\r\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\r\n[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\r\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\r\n\r\nThe reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\r\n\r\nThe reset process does not reset or clean up iptables rules or IPVS tables.\r\nIf you wish to reset iptables, you must do so manually by using the \"iptables\" command.\r\n\r\nIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\r\nto reset your system's IPVS tables.\r\n\r\nThe reset process does not clean your kubeconfig files and you must remove them manually.\r\nPlease, check the contents of the $HOME/.kube/config file.\r\n2024-09-09T14:08:17 info Executing pipeline Bootstrap in DeleteProcessor\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x359c0c6]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/labring/sealos/pkg/bootstrap.NewContextFrom(0xc000614240)\r\n\tgithub.com/labring/sealos/pkg/bootstrap/context.go:71 +0x2c6\r\ngithub.com/labring/sealos/pkg/bootstrap.New(0x0?)\r\n\tgithub.com/labring/sealos/pkg/bootstrap/bootstrap.go:49 +0x1d\r\ngithub.com/labring/sealos/pkg/apply/processor.(*DeleteProcessor).UndoBootstrap(0xc00059f3e0, 0xc000614240)\r\n\tgithub.com/labring/sealos/pkg/apply/processor/delete.go:83 +0x28c\r\ngithub.com/labring/sealos/pkg/apply/processor.DeleteProcessor.Execute({{0x44c6e10?, 0xc000b47350?}, {0x44bae80?, 0xc000627ba0?}}, 0xc000627ba0?)\r\n\tgithub.com/labring/sealos/pkg/apply/processor/delete.go:50 +0x35c\r\ngithub.com/labring/sealos/pkg/apply/applydrivers.(*Applier).deleteCluster(0xc000496820)\r\n\tgithub.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:265 +0x53\r\ngithub.com/labring/sealos/pkg/apply/applydrivers.(*Applier).Delete(0xc000496820)\r\n\tgithub.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:256 +0xe7\r\ngithub.com/labring/sealos/cmd/sealos/cmd.newResetCmd.func1(0xc000522c00?, {0xc00059e300?, 0x2?, 0x2?})\r\n\tgithub.com/labring/sealos/cmd/sealos/cmd/reset.go:54 +0xa3\r\ngithub.com/spf13/cobra.(*Command).execute(0xc000522c00, {0xc00059e2e0, 0x2, 0x2})\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:940 +0x862\r\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x5c6d560)\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:1068 +0x3bd\r\ngithub.com/spf13/cobra.(*Command).Execute(...)\r\n\tgithub.com/spf13/cobra@v1.7.0/command.go:992\r\ngithub.com/labring/sealos/cmd/sealos/cmd.Execute()\r\n\tgithub.com/labring/sealos/cmd/sealos/cmd/root.go:49 +0x25\r\nmain.main()\r\n\tgithub.com/labring/sealos/cmd/sealos/main.go:27 +0x29\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:v5.0.0\r\n- Docker version:\r\n- Kubernetes version:1.27.7\r\n- Operating system:openeuler 22.03\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2025-01-09T18:58:19Z",
        "number": 5054,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug panic: runtime error: invalid memory address or nil pointer dereference",
        "url": "https://github.com/labring/sealos/issues/5054",
        "login": "gitnehc",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5\n\n### How to reproduce the bug?\n\n![image](https://github.com/user-attachments/assets/b2616a1e-a7d8-48bf-ac17-9f011cf3f322)\r\nFollow the helm installation steps on the official website: https://sealos.run/docs/self-hosting/lifecycle-management/operations/build-image/build-image-helm_charts, an error occurred during run\n\n### What is the expected behavior?\n\nHope nginx can be pulled up\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-11T06:00:21Z",
        "number": 5033,
        "state": "CLOSED",
        "title": "BUG: deploy nginx Error: can't apply application type images only since RootFS type image is not applied yet",
        "url": "https://github.com/labring/sealos/issues/5033",
        "login": "biao-lvwan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0\r\n\r\n### How to reproduce the bug?\r\n\r\nsealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.12            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1            registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8            --masters 10.10.16.5,10.10.16.7,10.10.16.8            --nodes 10.10.16.11,10.10.16.12,10.10.16.14            -i '/root/.ssh/id_rsa' --output ~/Clusterfile\r\n\r\n### What is the expected behavior?\r\n```\r\napiServer:\r\n  certSANs:\r\n  - 127.0.0.1\r\n  - apiserver.cluster.local\r\n  - 10.10.16.5\r\n  - 10.10.16.7\r\n  - 10.10.16.8\r\n ```\r\n\r\n### What do you see instead?\r\n\r\n```\r\napiServer:\r\n  certSANs:\r\n  - 127.0.0.1\r\n  - apiserver.cluster.local\r\n  - 10.103.97.2\r\n  - 10.10.16.5\r\n  - 10.10.16.7\r\n  - 10.10.16.8\r\n ```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v5.0.0\r\n- Docker version: v20.10.7\r\n- Kubernetes version: v1.28.12\r\n- Operating system: Rocky 9.4\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-09-11T06:01:59Z",
        "number": 5029,
        "state": "CLOSED",
        "title": "BUG: There is a fake ip:10.103.97.2 in the Clusterfile with sealos v5.0.0 gen",
        "url": "https://github.com/labring/sealos/issues/5029",
        "login": "snakeliwei",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nsealos reset\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n![img_v3_02e7_f295c4c1-12ee-405a-aecf-79ce047fe78g](https://github.com/user-attachments/assets/a8292b3a-7bbc-4081-accd-b783b657d627)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0\r\n- Kubernetes version: v1.27.11\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-22T17:19:02Z",
        "number": 5007,
        "state": "CLOSED",
        "title": "BUG: sealos reset nil pointer",
        "url": "https://github.com/labring/sealos/issues/5007",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIs it possible to support modifying kubelet root-dir during the installation of k8s cluster. In the file /usr/lib/systemd/system/kubelet.service. d/10-kubeadm. conf, add KUBELET_EXTRA_ARGS=\"-- root dir=/data/dock/kubelet\"\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T01:27:33Z",
        "number": 4997,
        "state": "CLOSED",
        "title": "Support modifying kubelet rootdir when installing k8s cluster",
        "url": "https://github.com/labring/sealos/issues/4997",
        "login": "lllicg",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nroot@malone:~# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\r\nFlag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node\r\n2024-08-23T11:47:20 info Start to create a new cluster: master [192.168.163.23], worker [], registry 192.168.163.23\r\n2024-08-23T11:47:20 info Executing pipeline Check in CreateProcessor.\r\n2024-08-23T11:47:20 info checker:hostname [192.168.163.23:22]\r\n2024-08-23T11:47:20 info checker:timeSync [192.168.163.23:22]\r\n2024-08-23T11:47:20 info checker:containerd [192.168.163.23:22]\r\n2024-08-23T11:47:20 info Executing pipeline PreProcess in CreateProcessor.\r\n2024-08-23T11:47:20 info Executing pipeline RunConfig in CreateProcessor.\r\n2024-08-23T11:47:20 info Executing pipeline MountRootfs in CreateProcessor.\r\n2024-08-23T11:47:26 info Executing pipeline MirrorRegistry in CreateProcessor.\r\n2024-08-23T11:47:26 info trying default http mode to sync images to hosts [192.168.163.23:22]\r\n2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:22829/coredns/coredns:v1.10.1: trying to reuse blob sha256:25b7032c281a433b92d09930f3a03c0f7382c27eb69ae7f35addf2e3853dbba7 at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway\r\n2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:11009/cilium/certgen:v0.1.8: copying image 1/2 from manifest list: trying to reuse blob sha256:0342af6c6dfc37396624a2296e5e6a737e1a61a8ca5d48ba9259a86270d4bc7e at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-08-23T12:37:59Z",
        "number": 4995,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4995",
        "login": "malone231214",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nIs it possible to support k8s cluster deployments with mixed architectures (mixed x86 and arm).  I tried it and now it doesn't work. If it's not feasible, or if there's not much demand for it, forget it\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2025-01-14T03:58:23Z",
        "number": 4994,
        "state": "CLOSED",
        "title": "support install k8s cluster  with mixed architectures",
        "url": "https://github.com/labring/sealos/issues/4994",
        "login": "JACKCHEN99",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.1.3\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nè´¦æˆ·ç™»å½•é—®é¢˜ã€‚å¦‚ä½•æ‰¾å›æˆ‘çš„è´¦æˆ·ï¼Ÿ",
        "closed": true,
        "closedAt": "2024-12-23T21:37:33Z",
        "number": 4985,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4985",
        "login": "TingqiaoXu",
        "is_bot": false
      },
      {
        "body": "centos7 sealos4.2.0 4.3.0éƒ½è¯•è¿‡ï¼Œéƒ¨ç½²å•èŠ‚ç‚¹k8sï¼Œçœ‹èµ·æ¥æ˜¯kubeadm config images pull ä»æœ¬åœ°ä»“åº“æ‹‰å–kube-proxyçš„æ—¶å€™å¡ä½å¾ˆé•¿æ—¶é—´ï¼Œæœ€åå¤±è´¥ï¼Œä½†æ˜¯å…¶ä»–çš„ç»„ä»¶kube-apiserverï¼Œkube-controller-managerç­‰å¯ä»¥æ­£å¸¸ä¸‹è½½ã€‚\r\n![1723733625084](https://github.com/user-attachments/assets/263f3c46-409d-4db3-8c74-f925a82003f0)\r\ndocker pullæ‰‹åŠ¨æ‹‰å–kube-proxyä¹Ÿæ˜¯åœ¨åˆ’çº¿çš„åœ°æ–¹å¡äº†å¾ˆä¹…ï¼Œä½†æœ€åèƒ½ä¸‹è½½æˆåŠŸã€‚æ˜¯å¦èƒ½è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œæˆ–è€…æˆ‘æ‰‹åŠ¨æ‹‰å–kube-proxyä¹‹åï¼Œè¿˜éœ€è¦æ‰§è¡Œå“ªäº›è„šæœ¬ï¼Œæ‰èƒ½éƒ¨ç½²å®Œk8sï¼Ÿ\r\n![1723734061116](https://github.com/user-attachments/assets/5dbc7c82-980b-4b4d-b33d-ac05311e0170)\r\n",
        "closed": true,
        "closedAt": "2024-12-18T05:27:34Z",
        "number": 4978,
        "state": "CLOSED",
        "title": "Failed to download image during offline deployment",
        "url": "https://github.com/labring/sealos/issues/4978",
        "login": "ttsdmmn",
        "is_bot": false
      },
      {
        "body": "centos7 sealos4.2.0 4.3.0éƒ½è¯•è¿‡ï¼Œçœ‹èµ·æ¥æ˜¯kubeadm config images pull ä»æœ¬åœ°ä»“åº“æ‹‰å–kube-proxyçš„æ—¶å€™å¡ä½å¾ˆé•¿æ—¶é—´ï¼Œæœ€åå¤±è´¥ï¼Œä½†æ˜¯å…¶ä»–çš„ç»„ä»¶kube-apiserverï¼Œkube-controller-managerç­‰å¯ä»¥æ­£å¸¸ä¸‹è½½\r\n![1723733335960](https://github.com/user-attachments/assets/85163d56-d6d5-40a5-9960-2bce4e0ab333)\r\n\r\n",
        "closed": true,
        "closedAt": "2024-08-15T15:03:15Z",
        "number": 4977,
        "state": "CLOSED",
        "title": "Mirror pull failed during offline deployment",
        "url": "https://github.com/labring/sealos/issues/4977",
        "login": "ttsdmmn",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0\n\n### How to reproduce the bug?\n\n1. centos 7 with kernel 5.6.14\r\n![8d813db3-2262-4c41-a36b-f9d213ee5764](https://github.com/user-attachments/assets/a54aabde-dea0-4d5b-bbf1-bcb24f5f4844)\r\n2. curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n![5e5797fa-cf0d-4967-af8b-257530a32b7f](https://github.com/user-attachments/assets/254f1aa3-c34d-445c-82ea-5f2cb9abc8c7)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-08-16T08:31:52Z",
        "number": 4966,
        "state": "CLOSED",
        "title": "centos7 with kernel 5.6.14  install and  failed to pull image \"k8s.gcr.io/pause:3.2\"",
        "url": "https://github.com/labring/sealos/issues/4966",
        "login": "zlwzlwzlw",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv4.3.5\r\n\r\n### How to reproduce the bug?\r\n\r\n1. create a sealos app for kubevirt's virtual machine\r\n2.  locally import the sealos image \r\n3.  execute 'sealos run ' command line for multiple times to create multiple VMs with different names\r\n\r\n### What is the expected behavior?\r\n\r\nall 'sealos run' command lines completed and all VMs should be created successfully\r\n\r\n### What do you see instead?\r\n\r\none of the sealos pods failed to complete, the details as below: \r\n[root@cluster-master-10 samples]# k logs -f sealosapp-vm-jg-209-predelete-26gfz -n vm-system\r\n+ /tmp/ssh-init.sh\r\n++ cat /tmp/sealosapp-ssh/hosts\r\n+ export SEALOS_HOST=10.20.97.10\r\n+ SEALOS_HOST=10.20.97.10\r\n+ ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa root@10.20.97.10 'bash /opt/sealosapp/vm-jg-209/uninstall.sh'\r\nWarning: Permanently added '10.20.97.10' (ED25519) to the list of known hosts.\r\n+ export COMMON_NAMESPACE=vm-system\r\n+ COMMON_NAMESPACE=vm-system\r\n+ export REPOSITORY=harbor.cloud.net\r\n+ REPOSITORY=harbor.cloud.net\r\n+ export TAG=v1.2.2\r\n+ TAG=v1.2.2\r\n+ export SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-20\r\n+ SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-209\r\n+ service_name=vm-jg-209\r\n+ component=vm-app\r\n+ '[' -z vm-jg-209 ']'\r\n+ '[' -z vm-app ']'\r\n+ sealos run harbor.cloud/vm-app:v1.2.2 -e NAMESPACE=vm-system -e UNINSTALL=true -e chart=vm-app -e release=vm-jg-209 -e CPU_CORES=4 -e DATA_DISK_1_PVC_VOLUME_MODE=Block -e DATA_DISK_1_SIZE=10Gi -e DATA_DISK_1_STORAGE_CLASS=sc-jnds-data -e DATA_DISK_1_TYPE=pvc -e DISK_SIZE=20Gi -e MEMORY=48Gi -e POWER_ON=true -e STORAGE_CLASS=sc-jnds-yw -e 'TCP_PORTS=22 5432 18198 18196 18089' --cmd=./opt/uninstall.sh -f\r\n2024-08-14T09:23:51 info sync new version copy pki config: /var/lib/sealos/data/default/pki /root/.sealos/default/pki\r\n2024-08-14T09:23:51 info sync new version copy etc config: /var/lib/sealos/data/default/etc /root/.sealos/default/etc\r\n2024-08-14T09:23:51 info start to install app in this cluster\r\n2024-08-14T09:23:51 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-14T09:23:51 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-14T09:23:51 info Executing PreProcess Pipeline in InstallProcessor\r\nError: failed to mount: mounting \"default-qbfp1r97\" container \"default-qbfp1r97\": saving updated state for build container \"0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b\": saving builder state to \"/var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json\": rename /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/.tmp-buildah.json1423736918 /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json: no such file or directory\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v4.3.5\r\n- Docker version: 24.0.9\r\n- Kubernetes version: 1.26\r\n- Operating system:  openeuler 22.03\r\n- Runtime environment:\r\n- Cluster size:  standalone k3s cluster with only one node maintained\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-12-18T05:27:34Z",
        "number": 4965,
        "state": "CLOSED",
        "title": "BUG: failed to mount: mounting \"default-qbfp1r97\" container \"default-qbfp1r97\": saving updated state for build container",
        "url": "https://github.com/labring/sealos/issues/4965",
        "login": "jeven2016",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos-cloud-5.0.0\n\n### How to reproduce the bug?\n\næ­£åœ¨å®‰è£… Sealos Cloud.\r\n2024-08-12T22:11:42 info start to install app in this cluster\r\n2024-08-12T22:11:42 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-12T22:11:42 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-12T22:11:42 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-08-12T22:11:43 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env.tmpl completed\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml.tmpl completed\r\n2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml.tmpl completed\r\n2024-08-12T22:12:16 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-08-12T22:12:16 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\netc/sealos/.env: line 1: syntax error near unexpected token `newline'\r\nError: exit status 2\r\n\n\n### What is the expected behavior?\n\næ­£å¸¸å®‰è£…\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nåº”è¯¥æ˜¯æ¨¡æ¿æ²¡æœ‰æ›¿æ¢å‚æ•°ï¼Œæ‰¾ä¸åˆ°æ›¿æ¢çš„å€¼ï¼Œæ˜¾ç¤º<no value>",
        "closed": true,
        "closedAt": "2024-10-17T04:34:11Z",
        "number": 4962,
        "state": "CLOSED",
        "title": "sealos-cloud-5.0.0 install failed",
        "url": "https://github.com/labring/sealos/issues/4962",
        "login": "kinbod",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0\n\n### How to reproduce the bug?\n\nwhy still stay this step ? When executing curl, I confirmed that I pressed Enter all the way, except for entering the domain name.\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n![240810_12h34m56s_screenshot](https://github.com/user-attachments/assets/a400982a-044a-4ec1-a25c-166cf8672698)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: 5.0\r\n- Docker version: \r\n- Kubernetes version: 1.29.7\r\n- Operating system: Debian 12\r\n- Runtime environment: maybe containerd ? I use sealos cli \r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:55Z",
        "number": 4956,
        "state": "CLOSED",
        "title": "sealos 5.0,  self domain",
        "url": "https://github.com/labring/sealos/issues/4956",
        "login": "mkshift",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nsealos runæ—¶å’Œsealos apply -f Clusterfileæ—¶dockeré»˜è®¤çš„å­˜å‚¨è·¯å¾„æ˜¯/var/lib/dockerï¼Œdockerç½‘ç»œåœ°å€æ®µä¹Ÿæ˜¯é»˜è®¤çš„ï¼Œè¿™ä¸¤ä¸ªé»˜è®¤å€¼åœ¨sealosåˆå§‹åŒ–æ—¶å¦‚ä½•ä¿®æ”¹ï¼Ÿ\n\n### If you have solutionï¼Œplease describe it\n\n_No response_\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:55Z",
        "number": 4955,
        "state": "CLOSED",
        "title": "How does sealos specify docker's data storage directory and modify bip during initialization?",
        "url": "https://github.com/labring/sealos/issues/4955",
        "login": "baodelu",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nVersion: 4.3.0-7ee53f1d\n\n### How to reproduce the bug?\n\nä¸ºé›†ç¾¤æ·»åŠ nodeèŠ‚ç‚¹æ—¶æŠ¥é”™ï¼šerror Applied to cluster error: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?\r\nError: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?   \r\n\r\nè¿è¡Œsealos add ä¹‹å‰ä¹Ÿæ‰§è¡Œè¿‡export DO_NOT_CHECKSUM=true æ— æ•ˆæœä»ç„¶æŠ¥é”™ï¼Œé¢„æ·»åŠ çš„æ˜¯ç¬¬7ä¸ªèŠ‚ç‚¹ï¼Œä¹‹å‰ä¸€æ¬¡æ€§å®‰è£…6èŠ‚ç‚¹ï¼ˆ3master+3nodeï¼‰å…¨éƒ¨æ­£å¸¸è¿è¡Œï¼Œä½†å‡çº§è¿‡kubernetes ç‰ˆæœ¬åˆ°v1.28.0,æ“ä½œç³»ç»Ÿå‡ä¸ºubuntu 22.04,ç½‘ç»œä¸ºåŒç½‘æ®µï¼Œæ‰‹åŠ¨æ‹·è´æ–‡ä»¶æ²¡é—®é¢˜ï¼Œä½†æ˜¯æ— æ³•é€šè¿‡æ‰‹åŠ¨æ‹·è´è§£å†³ï¼Œaddå¤±è´¥åéœ€è¦deleteæ‰èŠ‚ç‚¹ä¸Šå·²ç»æ‹·è´çš„å…¶ä»–æ–‡ä»¶ä¼šè¢«æ¸…ç†æ‰\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-12-10T01:38:53Z",
        "number": 4951,
        "state": "CLOSED",
        "title": "ä¸ºé›†ç¾¤æ·»åŠ nodeèŠ‚ç‚¹æ—¶æŠ¥é”™ Kubefile  sha256 sum not match",
        "url": "https://github.com/labring/sealos/issues/4951",
        "login": "suntiexuan",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n```\r\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\\r\n  --cloud-version=v5.0.0 \\\r\n  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \\\r\n  --proxy-prefix=https://mirror.ghproxy.com\r\n```  \r\nprogram aborted Error: exit status 2\r\ninfracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1 pull image 403 Forbidden\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n```\r\naddon.extensions.kubeblocks.io/snapshot-controller enabled\r\nsecret/additional-scrape-configs created\r\nvmagent.operator.victoriametrics.com/victoria-metrics-k8s-stack patched\r\ndeployment.apps/vmagent-victoria-metrics-k8s-stack restarted\r\næ­£åœ¨ä¿®æ”¹ Ingress-nginx-controller çš„å®¹å¿åº¦, ä»¥å…è®¸å®ƒåœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿è¡Œ.\r\nconfigmap/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\ndaemonset.apps/ingress-nginx-controller patched\r\næ­£åœ¨å®‰è£… Sealos Cloud.\r\n2024-08-03T23:03:59 info start to install app in this cluster\r\n2024-08-03T23:03:59 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-08-03T23:03:59 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env.tmpl completed\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml.tmpl completed\r\n2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml.tmpl completed\r\n2024-08-03T23:04:03 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-08-03T23:04:03 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\netc/sealos/.env: line 1: syntax error near unexpected token `newline'\r\nError: exit status 2\r\n```\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0.\r\n- Docker version:\r\n- Kubernetes version: v1.29.7\r\n- Operating system: Rocky Linux 9.2\r\n- Runtime environment: virtual machine (vmware, 20G memory, 12 core cpu, 500G storage)\r\n- Cluster size: 1master,1node\r\n- Additional information:\n```\n\n\n### Additional information\n\n```\r\n[root@sealos-master ~]# kubectl get nodes -o wide\r\nNAME            STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                      KERNEL-VERSION                 CONTAINER-RUNTIME\r\nsealos-master   Ready    control-plane   46m   v1.29.7   192.168.50.68    <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20\r\nsealos-node     Ready    <none>          46m   v1.29.7   192.168.50.226   <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20\r\n```\r\nkb-addon-snapshot-controller \r\n```\r\n  Normal   Scheduled  49s                default-scheduler  Successfully assigned kb-system/kb-addon-snapshot-controller-7bc7cf9dbf-vgcqg to sealos-node                                                                             â”‚\r\nâ”‚   Normal   BackOff    18s (x2 over 48s)  kubelet            Back-off pulling image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\"                                                         â”‚\r\nâ”‚   Warning  Failed     18s (x2 over 48s)  kubelet            Error: ImagePullBackOff                                                                                                                                                  â”‚\r\nâ”‚   Normal   Pulling    5s (x3 over 49s)   kubelet            Pulling image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\"                                                                  â”‚\r\nâ”‚   Warning  Failed     4s (x3 over 49s)   kubelet            Failed to pull image \"infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\": failed to pull and unpack image \"infracreate-registry.cn â”‚\r\nâ”‚ -zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1\": failed to copy: httpReadSeeker: failed open: unexpected status code https://infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/v2/apecloud/snapshot-controller/b â”‚\r\nâ”‚ lobs/sha256:1ef6c138bd5f2ac45f7b4ee54db0e513efad8576909ae9829ba649fb4b067388: 403 Forbidden                                                                                                                                          â”‚\r\nâ”‚   Warning  Failed     4s (x3 over 49s)   kubelet            Error: ErrImagePull   \r\n```",
        "closed": true,
        "closedAt": "2024-12-10T01:38:54Z",
        "number": 4942,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4942",
        "login": "einnse",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\nsealos run as shown in doc\r\n\r\n sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.0 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4  registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 registry.k8s.io/etcd:3.5.6-0 --single\r\n\r\n\r\n<img width=\"768\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c3984e90-48b3-4320-b6ae-bf0b138b6eed\">\r\n\n\n### What is the expected behavior?\n\n<img width=\"768\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ffe0c822-1ba2-46d1-8beb-72cdc854a52f\">\r\n\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-29T06:37:59Z",
        "number": 4933,
        "state": "CLOSED",
        "title": " failed to pull and unpack image \\\"registry.k8s.io/etcd:3.5.6-0",
        "url": "https://github.com/labring/sealos/issues/4933",
        "login": "mingmingshiliyu",
        "is_bot": false
      },
      {
        "body": "æ–°ä¹°çš„æœåŠ¡å™¨æ˜¯armæ¶æ„çš„ï¼Œä½†æ˜¯æ—¢æœ‰é›†ç¾¤æ˜¯x86æ¶æ„ã€‚å¦‚ä½•å°†å…¶åŠ å…¥é›†ç¾¤ã€‚\nç›®å‰ç¿»é˜…äº†æ–‡æ¡£å’Œissueï¼Œè¯´æ˜¯æ”¯æŒsealos joinå‘½ä»¤ï¼Œä½†æ˜¯ä¸‹è½½äº†5.0ç‰ˆæœ¬çš„ sealosæ²¡æœ‰æ­¤å‘½ä»¤ï¼Œè¯·æ±‚å¸®åŠ©",
        "closed": true,
        "closedAt": "2024-07-24T13:56:34Z",
        "number": 4923,
        "state": "CLOSED",
        "title": "ARM nodes join x86 clusters to form heterogeneous clusters.",
        "url": "https://github.com/labring/sealos/issues/4923",
        "login": "Forfires",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\n1\n\n### What is the current documentation state?\n\n\r\nhttps://github.com/user-attachments/assets/a7b7ed5c-0e31-4158-8a76-3b161ed70a70\r\n\r\n\n\n### What is your suggestion?\n\n_No response_\n\n### Why is this change beneficial?\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-22T04:27:27Z",
        "number": 4914,
        "state": "CLOSED",
        "title": "Docs: demo video",
        "url": "https://github.com/labring/sealos/issues/4914",
        "login": "zuoFeng59556",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1ã€deploy self-hosted sealos-cloud\r\n2ã€register a non-admin user\r\n3ã€deploy any app on sealos desktop\n\n### What is the expected behavior?\n\ndeploy success\n\n### What do you see instead?\n\n![image](https://github.com/user-attachments/assets/4bba48f9-7060-446c-b702-d71b85a89ebb)\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\nI just used a node to deploy sealos, when using the admin account can deploy the application normally, using a non-admin account not, if you need to go through some configuration to make the non-admin user can use it normally, how to modify it? ",
        "closed": true,
        "closedAt": "2024-08-01T16:54:37Z",
        "number": 4913,
        "state": "CLOSED",
        "title": "BUG: non-admin user can not deploy app on v5.0.0",
        "url": "https://github.com/labring/sealos/issues/4913",
        "login": "zaunist",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0\n\n### How to reproduce the bug?\n\n1.  centos 7.8 \r\n2.  sealos-v5.0.0 deploy k8s in private cloud environment\r\n3.  SHELL:  curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0-beta5   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com\r\n\r\n4.  ERROR INFO:  \r\nW0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)\r\n[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11\r\nfailed to pull image \"k8s.gcr.io/pause:3.5\": output: E0712 03:49:52.907061   67905 remote_image.go:167] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\" image=\"k8s.gcr.io/pause:3.5\"\r\ntime=\"2024-07-12T03:49:52-04:00\" level=fatal msg=\"pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\"\r\n, error: exit status 1\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-07-12T03:49:52 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1\r\nError: failed to init masters: master pull image failed, error: exit status  1\n\n### What is the expected behavior?\n\ni think this is a bug ,  if user use this scripts to deploy k8s by sealos , this will try to get images from docker.io/xxx ;\r\n\r\nin the sealo project shoud considering  the internet is diffrent , this project  code shoud make user skip to get images form docker.io/xxx .\r\n\r\nThe code can avoid such an error, like \r\n---\r\n W0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)\r\n[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11\r\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11\r\nfailed to pull image \"k8s.gcr.io/pause:3.5\": output: E0712 03:49:52.907061   67905 remote_image.go:167] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\" image=\"k8s.gcr.io/pause:3.5\"\r\ntime=\"2024-07-12T03:49:52-04:00\" level=fatal msg=\"pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \\\"k8s.gcr.io/pause:3.5\\\": failed to resolve reference \\\"k8s.gcr.io/pause:3.5\\\": failed to do request: Head \\\"https://k8s.gcr.io/v2/pause/manifests/3.5\\\": dial tcp 108.177.97.82:443: connect: connection refused\"\r\n, error: exit status 1\r\nTo see the stack trace of this error execute with --v=5 or higher\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0(latest)\r\n- Docker version: null\r\n- Kubernetes version: v1.27.11\r\n- Operating system: centos-7.8\r\n- Runtime environment: containerd\r\n- Cluster size: 1\r\n- Additional information: mabey code shoud fix this bug , otherwise user will not success deploy k8s . \r\nThis is a sad storyï¼ï¼ï¼\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-17T03:07:09Z",
        "number": 4892,
        "state": "CLOSED",
        "title": "BUG:  The latest version ( v5.0.0 ) of the sealos privatization deployment will fail due to a public image pull issue",
        "url": "https://github.com/labring/sealos/issues/4892",
        "login": "ai-liuys",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-15T03:23:47Z",
        "number": 4891,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4891",
        "login": "ai-liuys",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\nVirtualBox:v7.0.18\r\nubuntu-20.04.5-live-server-amd64.iso\r\nhttps://old-releases.ubuntu.com/releases/20.04.5/ubuntu-20.04.5-live-server-amd64.iso\r\nUbuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)\r\nCPUï¼š4æ ¸\r\nå†…å­˜ï¼š8G\r\nIPï¼š192.168.1.11\r\n\r\nå®‰è£…Sealoså‘½ä»¤è¡Œå·¥å…·v4.3.7å¹¶å®‰è£…K8Sé›†ç¾¤\r\n```\r\necho \"deb [trusted=yes] https://apt.fury.io/labring/ /\" | tee /etc/apt/sources.list.d/labring.list\r\napt update\r\napt install -y sealos\r\n\r\nsealos run \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.14 \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 \\\r\n  registry.cn-shanghai.aliyuncs.com/labring/calico:3.27.3 \\\r\n  --masters 192.168.1.11\r\n```\r\næŸ¥çœ‹CPUå ç”¨ï¼Œå‘ç°systemd-udevdå¼‚å¸¸ï¼Œè¿›è€Œå‘ç°calicoå¼‚å¸¸\r\n```\r\n# htop\r\n\r\n# /lib/systemd/systemd-udevd -D\r\n\r\ncalico_tmp_B: /usr/lib/udev/rules.d/80-net-setup-link.rules:5 Failed to run builtin 'path_id': No such file or directory\r\n```\n\n### What is the expected behavior?\n\nå¸Œæœ›ç©ºé›†ç¾¤èŠ‚ç‚¹CPUæŒç»­å ç”¨å°äº0.5æ ¸\n\n### What do you see instead?\n\nç©ºé›†ç¾¤èŠ‚ç‚¹CPUæŒç»­å ç”¨1.5æ ¸\n\n### Operating environment\n\n```markdown\n- Sealos version:v4.3.7\r\n- Docker version:containerd:v1.7.18\r\n- Kubernetes version:v1.27.14\r\n- Operating system:Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)\r\n- Runtime environment:virtual machine (virbox, 8G memory, 4 core cpu, 100G storage)\r\n- Cluster size:1 master\r\n- Additional information:helm:v3.14.1, calico:3.27.3\n```\n\n\n### Additional information\n\nåœ¨çº¿å‡çº§Ubuntu 22.04.4 LTS (GNU/Linux 5.15.0-116-generic x86_64)åï¼ŒCPUæŒç»­å ç”¨å°äº0.5æ ¸ï¼Œ/lib/systemd/systemd-udevd -Dæ­£å¸¸",
        "closed": true,
        "closedAt": "2024-07-17T03:08:13Z",
        "number": 4881,
        "state": "CLOSED",
        "title": "The Sealos command line is used to build a K8Sv1.27.14 cluster node on Ubuntu20.04.5. The CPU continues to occupy 1.5 cores.",
        "url": "https://github.com/labring/sealos/issues/4881",
        "login": "MuJianning",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5\n\n### How to reproduce the bug?\n\næ‰§è¡Œsealos apply -f Clusterfile  ï¼ŒClusterfileå¦‚ä¸‹ï¼š xxxxå‡ä¸ºå†…ç½‘IP\r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  hosts:\r\n  - ips:\r\n    - x.x.x.x:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n  - ips:\r\n    - x.x.x.x:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n  image:\r\n  - x.x.x.x:8080/labring/kubernetes:v1.29.6\r\n  - x.x.x.x:8080/labring/helm:v3.9.4\r\n  - x.x.x.x:8080/labring/cilium:v1.14.9\r\n  ssh:\r\n    passwd: \"PASSWORD\"\r\n    pk: /root/.ssh/id_rsa\r\n    port: 22\r\nstatus: {}\r\næ‰§è¡Œå®Œæˆåï¼Œä¼šæŠ¥é”™ï¼ŒæŠ¥é”™å†…å®¹ä¸ºï¼š\r\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\r\n[kubelet-check] Initial timeout of 40s passed.\r\n\r\nUnfortunately, an error has occurred:\r\n        timed out waiting for the condition\r\n\r\nThis error is likely caused by:\r\n        - The kubelet is not running\r\n        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\r\n\r\nIf you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\r\n        - 'systemctl status kubelet'\r\n        - 'journalctl -xeu kubelet'\r\n\r\nAdditionally, a control plane component may have crashed or exited when started by the container runtime.\r\nTo troubleshoot, list all containers using your preferred container runtimes CLI.\r\nHere is one example how you may list all running Kubernetes containers by using crictl:\r\n        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'\r\n        Once you have found the failing container, you can inspect its logs with:\r\n        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'\r\nerror execution phase wait-control-plane: couldn't initialize a Kubernetes cluster\r\nTo see the stack trace of this error execute with --v=5 or higher\r\n2024-07-10T11:29:16 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\nError: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall\r\næ£€æŸ¥kubeletçŠ¶æ€ï¼Œé”™è¯¯æç¤ºï¼š\r\nJul 10 12:08:00 k8s-master-01 kubelet[24373]: E0710 12:08:00.064905   24373 kuberuntime_manager.go:1172] \"CreatePodSandbox for pod failed\" err=\"rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/a035ab8223d9d9db07cc92d89fa259d6708dc59ba4ab523e1216cd7458025bdd/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown\" pod=\"kube-system/kube-scheduler-k8s-master-01\"\r\næ£€æŸ¥containerdçŠ¶æ€ï¼Œé”™è¯¯æç¤ºï¼š\r\nJul 10 12:09:21 k8s-master-01 containerd[24036]: time=\"2024-07-10T12:09:21.072501176+08:00\" level=error msg=\"RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-k8s-master-01,Uid:0a9e7e740d78dd02274543ddd3c28942,Namespace:kube-system,Attempt:0,} failed, error\" error=\"failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/580589d578e6fa9d791ba8b2cffd221b4c1c24159015fbf34ee0891ebd427d16/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown\"\r\næœ€ç»ˆé€šè¿‡file /usr/bin/runcå‘ç°/usr/bin/runcä¸æ˜¯å¯æ‰§è¡Œç¨‹åºï¼Œ\r\n![å¾®ä¿¡å›¾ç‰‡_20240710121357](https://github.com/labring/sealos/assets/146060041/98a44917-bb12-45c5-97c1-b91e03185064)\r\n\r\nåœ¨å¦å¤–ä¸€ä¸ªèŠ‚ç‚¹æ‰§è¡Œ yum install runc -yï¼Œå®‰è£…å®Œæˆå†æŸ¥çœ‹file /usr/bin/runcä¸ºå¯æ‰§è¡Œç¨‹åºã€‚\r\n![a244443b81728790faf0f6cc606a8c8](https://github.com/labring/sealos/assets/146060041/7eaf45b3-1c37-4443-954d-555dc736f6d3)\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:5.0.0-beta5\r\n- Docker version:æ— \r\n- Kubernetes version:1.29.6\r\n- Operating system:CentOS 7.9\r\n- Runtime environment:è™šæ‹Ÿæœº 4æ ¸16Gå†…å­˜64Gå­˜å‚¨\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-10T07:47:19Z",
        "number": 4877,
        "state": "CLOSED",
        "title": "/usr/bin/runc is not an executable program",
        "url": "https://github.com/labring/sealos/issues/4877",
        "login": "linuxflow-hqukz",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos_4.3.7_linux_arm64.tar.gz\n\n### How to reproduce the bug?\n\næ“ä½œç³»ç»Ÿï¼šéº’éºŸV10ï¼ˆSP3ï¼‰/(Lance)-aarch64-Build23/20230324\r\nå†…æ ¸ç‰ˆæœ¬ï¼š4.19.90-52.22.v2207.ky10.aarch64\r\nsealosç‰ˆæœ¬ï¼š4.3.7_linux_arm64ã€å…¶ä»–ç‰ˆæœ¬çš„ä¹Ÿéƒ½è¯•è¿‡äº†\r\nk8sç‰ˆæœ¬ï¼š1.23.17ã€1.24.2ã€1.25.5ã€1.25.6ã€1.25.16ã€1.29.6éƒ½è¯•è¿‡äº†\n\n### What is the expected behavior?\n\nå¡åœ¨Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\r\né™æ€podå¦‚kube-apiserverã€kube-controller-managerã€kube-schedulerè¿™äº›éƒ½èµ·ä¸æ¥ï¼›kubeletçŠ¶æ€æ˜¯runningï¼ŒæŠ¥é”™é™æ€podå®¹å™¨æ¡ä¸èµ·æ¥ï¼Œgetting node xxx not foundï¼›containerdçŠ¶æ€runningï¼ŒæŠ¥é”™failed to get sandbox container task: no running task found\r\n\r\nç„¶åæ¯”è¾ƒå¥‡æ€ªçš„æ˜¯æˆ‘ä¹‹å‰ç”¨éº’éºŸV10(SP2)çš„ç³»ç»Ÿåˆ©ç”¨sealos4.3.7æ˜¯èƒ½å¤Ÿæ­£å¸¸æ‹‰èµ·k8sé›†ç¾¤çš„ï¼Œåªä¸è¿‡æœ‰ä¸ªcgroupçš„é—®é¢˜å¯¼è‡´corednsèµ·ä¸æ¥ï¼Œéœ€è¦æ”¹containerdçš„cgropué…ç½®ï¼Œæ”¹çš„å’Œkubeletçš„cgroupä¸ä¸€è‡´æ‰è¡Œï¼Œè¿™æ ·æˆ‘è§‰å¾—æœ‰éšæ‚£ï¼Œæ‰€ä»¥å¬å–ç½‘ä¸Šæ„è§æ›´æ¢æˆéº’éºŸSP3ç³»ç»Ÿï¼Œå…¶ä»–çš„éƒ½ä¸å˜ï¼Œç»“æœè¿é›†ç¾¤éƒ½æ‹‰ä¸èµ·æ¥äº†ï¼ï¼ï¼ï¼ï¼ï¼ æ¥ä¸ªå¤§ç¥æ•‘æˆ‘ï¼Œå¡2å¤©äº†ï¼Œéš¾å—\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:4.3.7\r\n- Docker version:\r\n- Kubernetes version:1.25.16\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:3ä¸ªmaster\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-17T03:08:30Z",
        "number": 4871,
        "state": "CLOSED",
        "title": "God help me! ! ! Kirin sp3 system arm architecture machine, deploying k8s is stuck here in kubelet startup/etc/kubernetes/manifest static pod, kubelet reports error getting node xxx not found",
        "url": "https://github.com/labring/sealos/issues/4871",
        "login": "SupRenekton",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\n5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\n1ã€run cmd sealos apply -f Clusterfile to create k8s.\r\nClusterfile:\r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  env:\r\n  - criData=/data/cri/lib\r\n  hosts:\r\n  - ips:\r\n    - 10.19.193.224:22\r\n    - 10.19.193.167:22\r\n    - 10.19.193.161:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx1\r\n      user: root\r\n      port: 22\r\n  - ips:\r\n    - 10.19.193.153:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxxx2\r\n      user: root\r\n      port: 22\r\n  image:\r\n  - 10.21.10.10:5000/labring/kubernetes:v1.27.11\r\n  - 10.21.10.10:5000/labring/helm:v3.10.3\r\n  - 10.21.10.10:5000/labring/calico:v3.26.1\r\n \r\n2ã€modify the Clusterfile and remove ip address 10.19.193.161, to delete node \r\napiVersion: apps.sealos.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: default\r\nspec:\r\n  env:\r\n  - criData=/data/cri/lib\r\n  hosts:\r\n  - ips:\r\n    - 10.19.193.224:22\r\n    - 10.19.193.167:22\r\n    roles:\r\n    - master\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx1\r\n      user: root\r\n      port: 22\r\n  - ips:\r\n    - 10.19.193.153:22\r\n    roles:\r\n    - node\r\n    - amd64\r\n    ssh:\r\n      passwd: xxxx2\r\n      user: root\r\n      port: 22\r\n  image:\r\n  - 10.21.10.10:5000/labring/kubernetes:v1.27.11\r\n  - 10.21.10.10:5000/labring/helm:v3.10.3\r\n  - 10.21.10.10:5000/labring/calico:v3.26.1\r\n\r\n3ã€run cmd sealos apply -f Clusterfile\r\n2024-07-09T11:01:12 warn delete master 10.19.193.161:22 failed cannot get node with ip address 10.19.193.161:22: cannot find host with internal ip 10.19.193.161\r\n2024-07-09T11:01:12 error failed to clean node, exec command if which kubeadm;then kubeadm reset -f  -v 0;fi && \\\r\nrm -rf /etc/kubernetes/ && \\\r\nrm -rf /etc/cni && rm -rf /opt/cni && \\\r\nrm -rf /var/lib/etcd && (ip link delete kube-ipvs0 >/dev/null 2>&1 || true)\r\n failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n2024-07-09T11:01:12 error failed to clean node, exec command rm -rf $HOME/.kube failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n2024-07-09T11:01:12 info succeeded in deleting master 10.19.193.161:22\r\n2024-07-09T11:01:12 info start to sync lvscare static pod to node: 10.19.193.153:22 master: [10.19.193.224:6443 10.19.193.167:6443]\r\n10.19.193.153:22        2024-07-09T11:01:12 info generator lvscare static pod is success\r\n2024-07-09T11:01:12 info Executing pipeline UndoBootstrap in ScaleProcessor\r\n2024-07-09T11:01:12 error Applied to cluster error: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\nError: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain\r\n\r\n### What is the expected behavior?\r\n\r\n_No response_\r\n\r\n### What do you see instead?\r\n\r\n_No response_\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:v5.0.1\r\n- containerd version:v1.7.14\r\n- Kubernetes version:1.27.11\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-09-20T12:06:40Z",
        "number": 4869,
        "state": "CLOSED",
        "title": "failed to delete node",
        "url": "https://github.com/labring/sealos/issues/4869",
        "login": "lllicg",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\ncurl -sfL https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\  \r\n--cloud-domain=<your_domain> \\   \r\n--cert-path=<your_crt> \\  \r\n--key-path=<your_key>\r\n\r\n### What is the expected behavior?\r\n\r\nFull install\r\n\r\n### What do you see instead?\r\n\r\n```\r\n...\r\n2024-07-05T19:59:06 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-07-05T19:59:06 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\nnamespace/sealos-system created\r\nnamespace/sealos created\r\ncustomresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created\r\nno mongodb uri found, create mongodb and gen mongodb uri\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...mongodb secret has been generated successfully.\r\nno cockroachdb uri found, create cockroachdb and gen cockroachdb uri\r\ncockroachdb statefulset is created.\r\nwaiting for cockroachdb ready    Error: signal: killed\r\nwaiting for cockroachdb ready ...\r\n```\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version: v5.0.0-beta5\r\n- Docker version: v20.10.7\r\n- Kubernetes version: v1.25.0\r\n- Operating system: Ubuntu 24.04\r\n- Runtime environment: 3 x 4gb / 2 cpu / 40GB server (Hetzner - CX22's)\r\n- Cluster size: 1 master, 2 nodes\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nInstall just stops with trying to start CockroachDB in the cluster. Eventually signal gets killed. Maybe not enough resources? There was an older issue like this (#4704). Just wondering if they were able to solve it. Cheers, Dave",
        "closed": true,
        "closedAt": "2024-07-06T00:37:58Z",
        "number": 4861,
        "state": "CLOSED",
        "title": "BUG: CockroachDB stops install process",
        "url": "https://github.com/labring/sealos/issues/4861",
        "login": "saashqdev",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nboth v4.3.7 and v5.0.0_beta\n\n### How to reproduce the bug?\n\n1. use sealos init install k8s (version such  v1.27.1);\r\n2. upgrade version to v1.27.5;\r\n3. add master or node\n\n### What is the expected behavior?\n\nthe newly added master or node install k8s v1.27.5\n\n### What do you see instead?\n\nthe newly added master or node install k8s v1.27.1\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\nNo matter what version of k8s you install, as long as the cluster is upgraded, when adding new masters or nodes, the new masters or nodes will be installed with the previous version.",
        "closed": true,
        "closedAt": "2024-07-09T01:12:29Z",
        "number": 4856,
        "state": "CLOSED",
        "title": "BUG: add nodes after upgrading, the version installed on the newly added nodes is incorrect",
        "url": "https://github.com/labring/sealos/issues/4856",
        "login": "yangxggo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nboth v4.3.7 and v5.0.0_beta\n\n### How to reproduce the bug?\n\nupgrade k8s from v1.26.x to v1.27.y\n\n### What is the expected behavior?\n\nall pods restart and work fine\n\n### What do you see instead?\n\nsome pods (include kube-scheduler) are pending or terminating for a long time\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-07-09T01:13:42Z",
        "number": 4848,
        "state": "CLOSED",
        "title": "BUG: upgrade from k8s v1.26.x to v1.27.y , kubelet restart failed",
        "url": "https://github.com/labring/sealos/issues/4848",
        "login": "yangxggo",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\nI've noticed that https://github.com/labring/sealos/blob/main/controllers/account/controllers/utils/email.go is using https://github.com/go-gomail/gomail for sending SMTP mails. go-mail/gomail is not actively maintained anymore and hasn't seen an update/response to an issue by the maintainer/merged PR in more than 6-7 years.\n\n### If you have solutionï¼Œplease describe it\n\nI am the maintainer of https://github.com/wneessen/go-mail which is pretty similar to go-mail/gomail but actively maintained and following modern standards and idiomatic Go. If there is any interest, I am willing to replace go-gomail/gomail in `email.go` to wneessen/go-mail.\r\n\r\nLet me know if a PR is of interest.\n\n### What alternatives have you considered?\n\nN/A",
        "closed": true,
        "closedAt": "2024-11-26T07:01:35Z",
        "number": 4840,
        "state": "CLOSED",
        "title": "Chore: Replace go-gomail/gomail with wneessen/go-mail",
        "url": "https://github.com/labring/sealos/issues/4840",
        "login": "wneessen",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nAPI Documentation\n\n### What is the current documentation state?\n\nNot Found\n\n### What is your suggestion?\n\nAdd separate section to describe Sealos APIs in detail.\n\n### Why is this change beneficial?\n\nProgrammatically control multitenant apps deployment staging and production.\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-07T00:09:41Z",
        "number": 4839,
        "state": "CLOSED",
        "title": "Request of Adding API documentation for developers",
        "url": "https://github.com/labring/sealos/issues/4839",
        "login": "mudevai",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos_4.3.7_linux_arm64.tar.gz\n\n### How to reproduce the bug?\n\nå›½äº§linuxç³»ç»Ÿï¼š4.19.90-24.4.v2101.ky10.aarch64 #1 SMP Mon May 24 14:45:37 CST 2021 aarch64 aarch64 aarch64 GNU/Linux\r\nsealos gen labring/kubernetes:v1.25.16 \\\r\n  labring/helm:v3.13.2 \\\r\n  labring/calico:v3.24.6 \\\r\n  --masters x.x.x.x,x.x.x.x,x.x.x.x --output Clusterfile\r\nClusterfileé…ç½®æ²¡æœ‰ä¿®æ”¹ï¼Œç›´æ¥applyï¼šsealos apply -f Clusterfile\r\n\r\nk8sé›†ç¾¤å»ºèµ·æ¥åï¼Œåªæœ‰corednsæ˜¾ç¤ºpodæ²¡èµ·æ¥ï¼ŒCrashLoopBackOffçŠ¶æ€\r\næŠ¥é”™ä¿¡æ¯ï¼š\r\nfailed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n2ä¸ªcoredns pod CrashLoopBackOffï¼Œå…¶ä»–podçŠ¶æ€æ­£å¸¸\r\nfailed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"\n\n### Operating environment\n\n```markdown\n- Sealos version: sealos_4.3.7_linux_arm64.tar.gz\r\n- Docker version: æ— \r\n- Kubernetes version: kubernetes:v1.25.16\r\n- Operating system: 4.19.90-24.4.v2101.ky10.aarch64\r\n- Runtime environment: \r\n- Cluster size: 3ä¸ªMasterèŠ‚ç‚¹\r\n- Additional information:\r\n  labring/helm:v3.13.2\r\n  labring/calico:v3.24.6\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-10T10:08:52Z",
        "number": 4836,
        "state": "CLOSED",
        "title": "BUG: brief descricontainer_linux.go:318: starting container process caused \"process_linux.go:281: applying cgroup configuration for process caused \\\"No such device or address\\\"\"ption of the bug",
        "url": "https://github.com/labring/sealos/issues/4836",
        "login": "SupRenekton",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\n5.0.0-beta5\r\n\r\n### How to reproduce the bug?\r\n\r\nsealos add --nodes 10.0.12.12 --debug\r\n\r\n### What is the expected behavior?\r\n\r\næ·»åŠ èŠ‚ç‚¹æˆåŠŸ\r\n\r\n### What do you see instead?\r\n\r\n<img width=\"1438\" alt=\"image\" src=\"https://github.com/labring/sealos/assets/27618687/277697e1-244f-44b7-9c55-486dbf3b0475\">\r\nå®Œæ•´çš„debugæ—¥å¿—åœ¨ä¸‹é¢é“¾æ¥ä¸­\r\n[node.log](https://github.com/user-attachments/files/16043343/node.log)\r\n\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-07-02T07:12:01Z",
        "number": 4829,
        "state": "CLOSED",
        "title": "BUG: Failed to add node node",
        "url": "https://github.com/labring/sealos/issues/4829",
        "login": "gclm",
        "is_bot": false
      },
      {
        "body": "### What is the problem this feature will solve?\n\n_No response_\n\n### If you have solutionï¼Œplease describe it\n\n![å¾®ä¿¡å›¾ç‰‡_20240629132646](https://github.com/labring/sealos/assets/22442450/7cf69ac3-211c-436e-899a-11d54a4ab05c)\r\n\n\n### What alternatives have you considered?\n\n_No response_",
        "closed": true,
        "closedAt": "2024-09-26T01:28:18Z",
        "number": 4826,
        "state": "CLOSED",
        "title": "How to install through sealos when Dockerhub cannot be accessed",
        "url": "https://github.com/labring/sealos/issues/4826",
        "login": "liuchengwen",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7-f39b2339\n\n### How to reproduce the bug?\n\n1. https://rockylinux.org/zh_CN ï¼ˆ2024-06ï¼‰æœ€æ–°ç‰ˆæœ¬\r\n2. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4\r\n3. Job for containerd.service failed because the control process exited with error code.\r\n\n\n### What is the expected behavior?\n\nå‰ç½®æ¡ä»¶æ˜¯å¹²å‡€çš„ç³»ç»Ÿï¼Œä½†æ˜¯è¿™ä¹ˆå®‰è£…ä¸ºä½•ä¸è¡Œï¼Ÿ\n\n### What do you see instead?\n\n[root@k8s-master01 ~]# systemctl status containerd.service                                                                                   â— containerd.service - containerd container runtime\r\n     Loaded: loaded (/etc/systemd/system/containerd.service; enabled; preset: disabled)\r\n     Active: activating (auto-restart) (Result: exit-code) since Sat 2024-06-29 10:25:57 CST; 3s ago\r\n       Docs: https://containerd.io\r\n    Process: 4282 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)\r\n    Process: 4283 ExecStart=/usr/bin/containerd (code=exited, status=203/EXEC)\r\n   Main PID: 4283 (code=exited, status=203/EXEC)\r\n        CPU: 4ms\r\n[root@k8s-master01 ~]# journalctl -xeu containerd.service\r\nâ–‘â–‘ Subject: Process /usr/bin/containerd could not be executed\r\nâ–‘â–‘ Defined-By: systemd\r\nâ–‘â–‘ Support: https://wiki.rockylinux.org/rocky/support\r\nâ–‘â–‘\r\nâ–‘â–‘ The process /usr/bin/containerd could not be executed and failed.\r\nâ–‘â–‘\r\nâ–‘â–‘ The error number returned by this process is ERRNO.\r\nJun 29 10:26:18 k8s-master01 systemd[4415]: containerd.service: Failed at step EXEC spawning /usr/bin/containerd: No such file or>\r\nâ–‘â–‘ Subject: Process /usr/bin/containerd could not be executed\r\nâ–‘â–‘ Defined-By: systemd\r\nâ–‘â–‘ Support: https://wiki.rockylinux.org/rocky/support\r\nâ–‘â–‘\r\nâ–‘â–‘ The process /usr/bin/containerd could not be executed and failed.\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-29T04:35:28Z",
        "number": 4825,
        "state": "CLOSED",
        "title": "rockylinux",
        "url": "https://github.com/labring/sealos/issues/4825",
        "login": "zeje",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0.0-beta5-a0b3363d9\n\n### How to reproduce the bug?\n\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh --zh  \\\r\n --cloud-version=v5.0.0-beta5 \\\r\n --image-registry=registry.cn-shanghai.aliyuncs.com \\\r\n --proxy-prefix=https://mirror.ghproxy.com \\\r\n --master-ips=192.168.0.112,192.168.0.113,192.168.0.120 \\\r\n --node-ips=192.168.0.123,192.168.0.121 \\\r\n --pod-cidr=100.64.0.0/10 \\\r\n --service-cidr=10.96.0.0/22 \\\r\n --cloud-domain=192.168.0.112.nip.io \\\r\n --cloud-port=443 \\\r\n --ssh-password=123456 \\\r\n --kubernetes-version=1.25.6\r\n\r\n![image](https://github.com/labring/sealos/assets/174031613/65457c16-089a-42b1-b862-7afd94e34ba9)\r\n\r\n![image](https://github.com/labring/sealos/assets/174031613/d63b1cb2-8eb6-4e83-ae98-5ee882cf93a0)\r\n\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\nEverything went well up until now\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0-beta5\r\n- Docker version: \r\n- Kubernetes version:  1.25.6\r\n- Operating system: ubuntu 22.04\r\n- Runtime environment: virtual machine (8G memory, 4 core cpu, 80G storage) *5\r\n- Cluster size: 3 master, 2 node\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:45Z",
        "number": 4814,
        "state": "CLOSED",
        "title": "BUG: Error: run chmod to rootfs failed: exit status 1",
        "url": "https://github.com/labring/sealos/issues/4814",
        "login": "nlqiong-me",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n5.0\n\n### How to reproduce the bug?\n\nroot@VM-32-39-ubuntu:~# kubectl get no -owide\r\nNAME              STATUS   ROLES           AGE   VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION       CONTAINER-RUNTIME\r\nvm-32-39-ubuntu   Ready    control-plane   66m   v1.26.15   172.16.32.39   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15\r\nvm-32-48-ubuntu   Ready    <none>          66m   v1.26.15   172.16.32.48   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15\r\nroot@VM-32-39-ubuntu:~# ./sealos delete --nodes 172.16.32.48\r\n2024-06-27T15:28:49 info are you sure to delete these nodes?\r\nâœ” Yes [y/yes], No [n/no]: yâ–ˆ\r\n2024-06-27T15:28:50 info start to scale this cluster\r\n2024-06-27T15:28:50 info Executing pipeline DeleteCheck in ScaleProcessor.\r\n2024-06-27T15:28:50 info checker:hostname [172.16.32.39:22]\r\n2024-06-27T15:28:50 info checker:containerd [172.16.32.39:22]\r\nError: failed to run checker: containerd is installed on 172.16.32.39:22 please uninstall it first\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n#4738 \r\nhttps://github.com/labring/sealos/blob/main/pkg/apply/processor/scale.go#L161\r\n\r\nA more precise `Phase` is needed to tell the checker whether it needs to be checked.",
        "closed": true,
        "closedAt": "2024-07-02T09:28:09Z",
        "number": 4809,
        "state": "CLOSED",
        "title": "BUG: delete node error: failed to run checker: containerd is installed on xxx please uninstall it first",
        "url": "https://github.com/labring/sealos/issues/4809",
        "login": "bxy4543",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\r\n\r\nv4.3.7\r\n\r\n### How to reproduce the bug?\r\næ‰§è¡Œå‘½ä»¤ï¼š\r\n[root@master sealos4.4.0]# sealos  registry copy reg.ide.local:5000/titanide/busybox:latest sealos.hub:5000\r\nGetting image source signatures\r\nError: trying to reuse blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa at destination: checking whether a blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa exists in sealos.hub:5000/titanide/busybox: authentication required\r\n\r\n### What is the expected behavior?\r\n\r\n_No response_\r\n\r\n### What do you see instead?\r\n\r\n_No response_\r\n\r\n### Operating environment\r\n\r\n```markdown\r\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:44Z",
        "number": 4802,
        "state": "CLOSED",
        "title": "BUG: sealos registry copyæ— æ³•é‰´æƒé€šè¿‡ï¼Œå³ä¾¿æ‰§è¡Œè¿‡sealos loginå‘½ä»¤ï¼Œä¾ç„¶ä¼šæç¤ºï¼šauthentication required",
        "url": "https://github.com/labring/sealos/issues/4802",
        "login": "fu7100",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nsealos ï¼š4.1.3\n\n### How to reproduce the bug?\n\n1ã€sealos pull  labring/kubernetes:v1.27.7,æ‰“åŒ…æˆtaråŒ…ï¼škubernetes-v1.27.7.tar\r\n2ã€mountè‡³æœ¬åœ°åä¿®æ”¹images/shim/ä¸‹DefaultImageList  å°†å…¶ä¿®æ”¹ä¸ºç§æœ‰ä»“åº“é•œåƒå¦‚ï¼š\r\n192.182.xxx.xx/registry.k8s.io/coredns:v1.9.3\r\n192.182.xxx.xx/registry.k8s.io/etcd:3.5.10-0\r\n192.182.xxx.xx/registry.k8s.io/kube-controller-manager:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/kube-scheduler:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/kube-proxy:v1.26.15\r\n192.182.xxx.xx/registry.k8s.io/pause:3.9\r\n192.182.xxx.xx/registry.k8s.io/kube-apiserver:v1.26.15\r\n3ã€åˆ é™¤registryç›®å½•ä¸‹æ–‡ä»¶\r\n4ã€ä¿®æ”¹Kubefileæ–‡ä»¶ï¼Œæ·»åŠ label\r\nFROM scratch\r\nMAINTAINER sealos\r\nLABEL init=\"init-cri.sh \\$registryDomain \\$registryPort && bash init.sh\" \\\r\n      clean=\"clean.sh && bash clean-cri.sh \\$criData\" \\\r\n      check=\"check.sh \\$registryData\" \\\r\n      init-registry=\"init-registry.sh \\$registryData \\$registryConfig\" \\\r\n      clean-registry=\"clean-registry.sh \\$registryData \\$registryConfig\" \\\r\n      sealos.io.type=\"rootfs\" sealos.io.version=\"v1beta1\" version=\"v1.26.15\"\r\nENV criData=/var/lib/containerd \\\r\n    registryData=/var/lib/registry \\\r\n    registryConfig=/etc/registry \\\r\n    registryDomain=192.168.xxx.xx \\\r\n    registryPort=5000 \\\r\n    registryUsername=admin \\\r\n    registryPassword=passw0rd \\\r\n    disableApparmor=false \\\r\n    SEALOS_SYS_CRI_ENDPOINT=/var/run/containerd/containerd.sock \\\r\n    SEALOS_SYS_IMAGE_ENDPOINT=/var/run/image-cri-shim.sock \\\r\nCOPY . .\r\n5ã€sealos build\r\n6ã€sealos run å‡ºç°éƒ¨åˆ†æŠ¥é”™ï¼š\r\n6.1:\r\n\"PullImage from image service failed\" err=\"rpc error: code = Unavailable desc = error reading from server: EOF\" image=\"192.168.xxx.xx:5000/\"\r\nFATA[0000] pulling image: rpc error: code = Unavailable desc = error reading from server: EOF\r\n6.2\r\n[preflight] Pulling images required for setting up a Kubernetes cluster\r\n[preflight] This might take a minute or two, depending on the speed of your internet connection\r\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r\nI0624 15:59:50.512988 2316864 checks.go:832] using image pull policy: IfNotPresent\r\nI0624 15:59:50.531237 2316864 checks.go:849] pulling: registry.k8s.io/kube-apiserver:v1.26.15\r\nI0624 15:59:50.625223 2316864 checks.go:849] pulling: registry.k8s.io/kube-controller-manager:v1.26.15\r\nI0624 15:59:50.713096 2316864 checks.go:849] pulling: registry.k8s.io/kube-scheduler:v1.26.15\r\nI0624 15:59:50.800815 2316864 checks.go:849] pulling: registry.k8s.io/kube-proxy:v1.26.15\r\nI0624 15:59:50.883503 2316864 checks.go:849] pulling: registry.k8s.io/pause:3.9\r\nI0624 15:59:50.970530 2316864 checks.go:849] pulling: registry.k8s.io/etcd:3.5.10-0\r\nI0624 15:59:51.059066 2316864 checks.go:849] pulling: registry.k8s.io/coredns/coredns:v1.9.3\r\n[preflight] Some fatal errors occurred:\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.26.15: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time=\"2024-06-24T15:59:50+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n, error: exit status 1\r\n        [ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.10-0: output: time=\"2024-06-24T15:59:51+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///var/run/image-cri-shim.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\\\"\"\r\n\r\nç–‘é—®ç‚¹ï¼šä¸ºä½•ä»è¿œç«¯æ‹‰å–é•œåƒè€Œä¸æ˜¯æœ¬åœ°å†…ç½®registryï¼ŒæŸ¥çœ‹containerdæœåŠ¡å‘ç°æœªè®¾ç½®point\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-11-07T00:09:42Z",
        "number": 4797,
        "state": "CLOSED",
        "title": "BUG: clusterimages build error",
        "url": "https://github.com/labring/sealos/issues/4797",
        "login": "ricardolrl",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0\n\n### How to reproduce the bug?\n\nInstall Sealos with the template frontend enabled\n\n### What is the expected behavior?\n\nNo zombies\n\n### What do you see instead?\n\nFind out one of the pids of the zombie processes:\r\n```\r\n# ps aux | grep Z\r\n1001     63841  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>\r\n1001     64043  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>\r\n1001     64183  0.0  0.0      0     0 ?        Z    Jun22   0:00 [git] <defunct>\r\n1001     64495  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>\r\n1001     64661  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>\r\n```\r\nFind out which container holds this pid:\r\n```\r\n# pstree -s -p 64661\r\nsystemd(1)â”€â”€â”€containerd-shim(43915)â”€â”€â”€node(46306)â”€â”€â”€git(64661)\r\n# pstree 46306\r\nnodeâ”€â”¬â”€108*[git]\r\n     â””â”€14*[{node}]\r\n# crictl ps -q | xargs crictl inspect --output go-template --template '{{.info.pid}}, {{.status.metadata.name}}' | grep 46306\r\n46306, template-frontend\r\n```\n\n### Operating environment\n\n_No response_\n\n### Additional information\n\nThese lines might be problematic:\r\n\r\nhttps://github.com/labring/sealos/blob/78cefc7715b6c0952f079cc6bbc1bd32e96adbcb/frontend/providers/template/src/pages/api/updateRepo.ts#L83-L87",
        "closed": true,
        "closedAt": "2024-06-25T06:18:03Z",
        "number": 4796,
        "state": "CLOSED",
        "title": "BUG: template frontend leaves zombie `git` processes",
        "url": "https://github.com/labring/sealos/issues/4796",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.0.0\n\n### How to reproduce the bug?\n\n_No response_\n\n### What is the expected behavior?\n\nç»´æŠ¤é›†ç¾¤æ·»åŠ å¤šä¸ªèŠ‚ç‚¹\n\n### What do you see instead?\n\n2024-06-23T17:31:21 debug cmd for bash in host:  /var/lib/sealos/data/default/rootfs/opt/sealctl token\r\n\r\n\r\n\r\n\r\n\r\n2024-06-23T17:33:57 debug failed to exec remote 10.20.0.3:22 shell: /var/lib/sealos/data/default/rootfs/opt/sealctl token output: &{default} error: exit status 1\r\n2024-06-23T17:33:57 error Applied to cluster error: exit status 1\r\n2024-06-23T17:33:57 debug write cluster file to local storage: /root/.sealos/default/Clusterfile\r\nError: exit status 1\n\n### Operating environment\n\n```markdown\n- Sealos version: 4.0.0\r\n- Docker version: containerd 1.6.2\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\r\ncentos 7.6\n```\n\n\n### Additional information\n\nä½ ä»¬è¿™ä¸ªå†™çš„å¤ªä¹±äº†, ä¹‹å‰çš„ç‰ˆæœ¬åˆšå¼€å§‹å®‰è£…æ˜¯å¥½çš„,æ·»åŠ èŠ‚ç‚¹éƒ½æ²¡é—®é¢˜, ç»“æœè¿è¡Œä¸€æ®µæ—¶é—´, æ·»åŠ èŠ‚ç‚¹å°±æ˜¯é—®é¢˜äº†",
        "closed": true,
        "closedAt": "2024-11-07T00:09:43Z",
        "number": 4794,
        "state": "CLOSED",
        "title": "Failed to add node",
        "url": "https://github.com/labring/sealos/issues/4794",
        "login": "he-aook",
        "is_bot": false
      },
      {
        "body": "### Documentation Section\n\nç¦»çº¿å®‰è£… K8s\n\n### What is the current documentation state?\n\n_No response_\n\n### What is your suggestion?\n\nè€å¸ˆï¼Œæ‚¨å¥½ã€‚\r\né¦–å…ˆåœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒä¸­å¯¼å‡ºé›†ç¾¤é•œåƒï¼š è¿™é‡Œé¢èƒ½å¦å¢åŠ \r\nå…·ä½“k8så’Œå„ç§ç½‘ç»œæ’ä»¶ ç»™ä¸€ä¸ªæœ€ä½³çš„æ¨è  \r\nåœ¨æ‰§è¡Œå‘½ä»¤ sealos run kubernetes.tar \r\né‡åˆ°ä»¥ä¸‹é”™è¯¯ \r\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r\n^[2^NW0621 15:16:42.324677   16367 checks.go:835] detected that the sandbox image \"sealos.hub:5000/pause:3.9\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\r\nerror execution phase preflight: [preflight] Some fatal errors occurred:\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.11: output: E0621 15:08:59.182331   16827 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\\\": dial tcp 74.125.23.82:443: i/o timeout\" image=\"registry.k8s.io/kube-apiserver:v1.28.11\"\r\ntime=\"2024-06-21T15:08:59+08:00\" level=fatal msg=\"pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-apiserver:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\\\": dial tcp 74.125.23.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.11: output: E0621 15:11:33.659002   16943 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\" image=\"registry.k8s.io/kube-controller-manager:v1.28.11\"\r\ntime=\"2024-06-21T15:11:33+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-controller-manager:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.11: output: E0621 15:14:07.264143   17021 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\" image=\"registry.k8s.io/kube-scheduler:v1.28.11\"\r\ntime=\"2024-06-21T15:14:07+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-scheduler:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\\\": dial tcp 74.125.204.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.11: output: E0621 15:16:42.295317   17286 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\\\": dial tcp 108.177.125.82:443: i/o timeout\" image=\"registry.k8s.io/kube-proxy:v1.28.11\"\r\ntime=\"2024-06-21T15:16:42+08:00\" level=fatal msg=\"pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to resolve reference \\\"registry.k8s.io/kube-proxy:v1.28.11\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\\\": dial tcp 108.177.125.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: E0621 15:19:05.165352   17615 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unavailable desc = error reading from server: EOF\" image=\"registry.k8s.io/pause:3.9\"\r\ntime=\"2024-06-21T15:19:05+08:00\" level=fatal msg=\"pulling image: rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.12-0: output: time=\"2024-06-21T15:19:05+08:00\" level=fatal msg=\"validate service connection: validate CRI v1 image API for endpoint \\\"unix:///run/containerd/containerd.sock\\\": rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\\\"\"\r\n, error: exit status 1\r\n\t[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: E0621 15:21:39.262383   18253 remote_image.go:180] \"PullImage from image service failed\" err=\"rpc error: code = Unknown desc = failed to pull and unpack image \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to resolve reference \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\\\": dial tcp 108.177.125.82:443: i/o timeout\" image=\"registry.k8s.io/coredns/coredns:v1.10.1\"\r\ntime=\"2024-06-21T15:21:39+08:00\" level=fatal msg=\"pulling image: failed to pull and unpack image \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to resolve reference \\\"registry.k8s.io/coredns/coredns:v1.10.1\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\\\": dial tcp 108.177.125.82:443: i/o timeout\"\r\n, error: exit status 1\r\n\n\n### Why is this change beneficial?\n\nå¯èƒ½å› ä¸ºç‰¹æ®ŠåŸå›  å¾ˆå¤šäººä¼šé‡åˆ°è¿™ç§æƒ…å†µ \n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:46Z",
        "number": 4793,
        "state": "CLOSED",
        "title": "Can you add more details about the offline installation of k8s?",
        "url": "https://github.com/labring/sealos/issues/4793",
        "login": "camuselliot",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv5.0.0-beta5\n\n### How to reproduce the bug?\n\nError during installation\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...Error: signal: killed\r\n\r\nInstallation steps\r\n\r\ncurl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \\  \r\n--cloud-version=v5.0.0-beta5 \\  \r\n--image-registry=registry.cn-shanghai.aliyuncs.com --zh \\  \r\n--proxy-prefix=https://mirror.ghproxy.com\r\n```\r\n\r\n\r\nMaster IPï¼š192.168.1.80\r\nNode IPï¼š192.168.1.81,192.168.1.83\r\nDomain nameï¼š192.168.1.80.nip.io\r\n\r\nMongoDB 5.0ç‰ˆæœ¬ä¾èµ–æ”¯æŒ AVX æŒ‡ä»¤é›†çš„ CPU, å½“å‰ç¯å¢ƒä¸æ”¯æŒ AVX, å·²åˆ‡æ¢ä¸º MongoDB 4.0ç‰ˆæœ¬, æ›´å¤šä¿¡æ¯æŸ¥çœ‹: https://www.mongodb.com/docs/v5.0/administration/production-notes/\r\n2024-06-20T18:56:19 info start to install app in this cluster\r\n2024-06-20T18:56:19 info Executing SyncStatusAndCheck Pipeline in InstallProcessor\r\n2024-06-20T18:56:19 info Executing ConfirmOverrideApps Pipeline in InstallProcessor\r\n2024-06-20T18:56:19 info Executing PreProcess Pipeline in InstallProcessor\r\n2024-06-20T18:56:20 info Executing pipeline MountRootfs in InstallProcessor.\r\n2024-06-20T18:58:01 info Executing pipeline MirrorRegistry in InstallProcessor.\r\n2024-06-20T18:58:01 info Executing UpgradeIfNeed Pipeline in InstallProcessor\r\nnamespace/sealos-system created\r\nnamespace/sealos created\r\ncustomresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created\r\nno mongodb uri found, create mongodb and gen mongodb uri\r\nwaiting for mongodb secret generated\r\nwaiting for mongodb ready ...Error: signal: killed\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\nkubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS    RESTARTS       AGE\r\ncert-manager                cert-manager-5f8646db6b-5nhxh                                     1/1     Running   0              14h\r\ncert-manager                cert-manager-cainjector-5cf5f57dd7-fdmp6                          1/1     Running   0              14h\r\ncert-manager                cert-manager-webhook-687b7f8b97-l5txj                             1/1     Running   0              14h\r\ncockroach-operator-system   cockroach-operator-manager-848fd7d88d-zpnk4                       1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-26dpm                                    1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-6n9mw                                    1/1     Running   0              14h\r\ningress-nginx               ingress-nginx-controller-wb5d9                                    1/1     Running   0              14h\r\nkb-system                   kb-addon-snapshot-controller-64b4dcb6bc-lzfdm                     1/1     Running   2 (14h ago)    14h\r\nkb-system                   kubeblocks-6954875d7d-rtcnh                                       1/1     Running   3 (14h ago)    14h\r\nkb-system                   kubeblocks-dataprotection-5cfdf54fcc-zh7wg                        1/1     Running   4 (14h ago)    14h\r\nkube-system                 cilium-gpgmh                                                      1/1     Running   0              14h\r\nkube-system                 cilium-gplkj                                                      1/1     Running   0              14h\r\nkube-system                 cilium-jtf27                                                      1/1     Running   0              14h\r\nkube-system                 cilium-operator-97f465f54-lrktg                                   1/1     Running   4 (14h ago)    14h\r\nkube-system                 coredns-5d78c9869d-8mxzp                                          1/1     Running   0              14h\r\nkube-system                 coredns-5d78c9869d-kppdj                                          1/1     Running   0              14h\r\nkube-system                 etcd-rofine30                                                     1/1     Running   2              14h\r\nkube-system                 kube-apiserver-rofine30                                           1/1     Running   2              14h\r\nkube-system                 kube-controller-manager-rofine30                                  1/1     Running   10 (14h ago)   14h\r\nkube-system                 kube-proxy-5mkgv                                                  1/1     Running   0              14h\r\nkube-system                 kube-proxy-82k5f                                                  1/1     Running   0              14h\r\nkube-system                 kube-proxy-kvnxl                                                  1/1     Running   0              14h\r\nkube-system                 kube-scheduler-rofine30                                           1/1     Running   10 (14h ago)   14h\r\nkube-system                 kube-sealos-lvscare-rofine31                                      1/1     Running   1              14h\r\nkube-system                 kube-sealos-lvscare-rofine32                                      1/1     Running   1              14h\r\nkube-system                 metrics-server-84c9bd846f-hb8sh                                   1/1     Running   0              14h\r\nopenebs                     openebs-localpv-provisioner-6b456b4c69-wxxr8                      1/1     Running   4 (14h ago)    14h\r\nvm                          victoria-metrics-k8s-stack-grafana-55b778cbf9-jnbsb               3/3     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-658d9b4dd5-tglth    1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-scbzm         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-sd7tw         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-vmglj         1/1     Running   0              14h\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-6b774mjx9v   1/1     Running   3 (14h ago)    14h\r\nvm                          vmagent-victoria-metrics-k8s-stack-c694d459d-vsxxm                2/2     Running   0              14h\r\nvm                          vmalert-victoria-metrics-k8s-stack-695868d7fc-89vr8               2/2     Running   0              14h\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running   0              14h\r\nvm                          vmsingle-victoria-metrics-k8s-stack-687d8cc7b4-r6c6d              1/1     Running   0              14h\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: v5.0.0-beta5\r\n- Docker version: containerd://1.7.15\r\n- Kubernetes version: v1.27.11\r\n- Operating system: Anolis OS 8.9   5.10.134-16.2.an8.x86_64\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-28T20:23:47Z",
        "number": 4790,
        "state": "CLOSED",
        "title": "Error during installation: waiting for MongoDB to be readyâ€¦ Error: signal: killed.",
        "url": "https://github.com/labring/sealos/issues/4790",
        "login": "zhubao315",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7\n\n### How to reproduce the bug?\n\n1. sealos reset\r\n2. rm ~/.sealos/ -rf \r\n3. sealos gen docker.io/labring/kubernetes:v1.27.14 --masters 192.168.x.x --pk=$HOME/.ssh/id_rsa -o ./Clusterfile\r\n4. sed -i \"s|100.64.0.0/10|100.77.0.0/10|g\" ./Clusterfile\r\n5. sed -i \"s|10.96.0.0/22|10.99.0.0/22|g\" ./Clusterfile\r\n6. sealos apply -f ./Clusterfile\r\n7. sealos run labring/helm:v3.14.1\r\n8. sealos run labring/cilium:v1.14.11 --env ExtraValues=\"ipam.mode=kubernetes\"\r\n9. kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o wide\n\n### What is the expected behavior?\n\nThe IP address assigned to the expected pod is 100.77.x.x\n\n### What do you see instead?\n\nIn fact, the IP assigned to the pod is 100.64.x.x:\r\n![image](https://github.com/labring/sealos/assets/21333000/c1030583-ce99-4f23-bb91-a71860943261)\r\nThe running configuration of the kube-controller-manager pod is as followsï¼š\r\n![image](https://github.com/labring/sealos/assets/21333000/0b2205bf-4f4b-40ec-a2f4-e5ab6f49d7b7)\r\n\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version: 4.3.7\r\n- Docker version: none\r\n- Kubernetes version: 1.27.14\r\n- Operating system: centos7\r\n- Runtime environment: 32core 128G\r\n- Cluster size: 1\r\n- Additional information: none\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-20T02:27:58Z",
        "number": 4779,
        "state": "CLOSED",
        "title": "BUG: Modifying pod_cidr via pod_subnet does not take effect",
        "url": "https://github.com/labring/sealos/issues/4779",
        "login": "cnmac",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.1.4\n\n### How to reproduce the bug?\n\n1.netstat -anpl|grep 5000\r\n<img width=\"1080\" alt=\"CleanShot 2024-06-13 at 13 57 36@2x\" src=\"https://github.com/labring/sealos/assets/16560252/4ad3b82c-eada-46f0-ae3c-158f9590d1a3\">\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version: v4.1.4\r\n- Docker version: v23.0.0\r\n- Kubernetes version: v1.25.6\r\n- Operating system: ubuntu20.04\r\n- Runtime environment: cloud(tencent, 8G memory, 2core cpu, 50G storage)\r\n- Cluster size: 1 master,1 node\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-17T07:13:00Z",
        "number": 4774,
        "state": "CLOSED",
        "title": "BUG: The local registry has a large number of network connections that will not be released",
        "url": "https://github.com/labring/sealos/issues/4774",
        "login": "xmwilldo",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\nv4.3.7 v5.0.0-beta\n\n### How to reproduce the bug?\n\nOS openeuler 24.03 LTS\r\n\r\n253 ä½œä¸ºå¤–éƒ¨æ§åˆ¶æœºï¼Œå…¶ä»–ä¸»æœºä½œä¸ºçœŸå®masterï¼Œå¦‚101\r\n```\r\nsealos gen labring/kubernetes:v1.29.5-4.3.7 labring/helm:v3.14.0 labring/metrics-server:v0.6.4 --masters 192.168.3.101,192.168.3.102,192.168.3.103  ....\r\nsealos apply -f  ...\r\nwarn failed to copy image 127.0.0.1:46879/kube-apiserver:v1.29.5: trying to reuse blob sha256:b2ce0e0660777651a7a188ae1128acc61d01aca10a035a8b1faa2cdd8bbf0785 at destination: pinging container registry 192.168.3.101:5050: received unexpected HTTP status: 502 Bad Gateway\r\n```\r\nsealos åŒæ­¥é•œåƒè‡³101ï¼ˆmaster0)çš„5050 registryï¼Œä½†æ˜¯registryå¯åŠ¨ http 5000/ debug 5001 ï¼Œæ‰€ä»¥å§‹ç»ˆæç¤º  502 Bad Gateway\r\n\r\n\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\n_No response_\n\n### Operating environment\n\n```markdown\n- Sealos version:  v4.3.7\r\n- Docker version: no\r\n- Kubernetes version:  labring/kubernetes:v1.29.5-4.3.7\r\n- Operating system: openeuler 2403\r\n- Runtime environment: no\r\n- Cluster size: no \r\n- Additional information: no\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-06-10T14:59:56Z",
        "number": 4770,
        "state": "CLOSED",
        "title": "Failed to transfer image during installation",
        "url": "https://github.com/labring/sealos/issues/4770",
        "login": "yulinor",
        "is_bot": false
      },
      {
        "body": "### Sealos Version\n\n4.3.7\n\n### How to reproduce the bug?\n\n1.ä½¿ç”¨yum å®‰è£…\r\n2.æ‰§è¡Œåˆ›å»ºå•æœºé›†ç¾¤\r\n3. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single\r\n4.centos7.9\n\n### What is the expected behavior?\n\næ­£å¸¸è¿è¡Œã€\n\n### What do you see instead?\n\n[root@k8smaster ~]# kubectl get po -A\r\nNAMESPACE     NAME                                READY   STATUS                  RESTARTS       AGE\r\nkube-system   cilium-22szr                        0/1     Init:CrashLoopBackOff   20 (24s ago)   78m\r\nkube-system   cilium-operator-86666d88cb-dnfhx    1/1     Running                 0              78m\r\nkube-system   coredns-5d78c9869d-ggk9p            0/1     Pending                 0              78m\r\nkube-system   coredns-5d78c9869d-qgd2c            0/1     Pending                 0              78m\r\nkube-system   etcd-k8smaster                      1/1     Running                 12             78m\r\nkube-system   kube-apiserver-k8smaster            1/1     Running                 12             78m\r\nkube-system   kube-controller-manager-k8smaster   1/1     Running                 12             78m\r\nkube-system   kube-proxy-pfngx                    1/1     Running                 0              78m\r\nkube-system   kube-scheduler-k8smaster            1/1     Running                 12             78m\r\n\r\n\r\næŸ¥çœ‹æ—¥å¿—ï¼š\r\n0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..\r\n\r\n\r\nkubeletï¼š\r\n\r\nI0605 20:23:00.421613    5697 server.go:415] \"Kubelet version\" kubeletVersion=\"v1.27.7\"\r\nI0605 20:23:00.421677    5697 server.go:417] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nI0605 20:23:00.421895    5697 server.go:578] \"Standalone mode, no API client\"\r\nI0605 20:23:00.422011    5697 container_manager_linux.go:802] \"CPUAccounting not enabled for process\" pid=5697\r\nI0605 20:23:00.422020    5697 container_manager_linux.go:805] \"MemoryAccounting not enabled for process\" pid=5697\r\nI0605 20:23:00.426873    5697 server.go:466] \"No api server defined - no events will be sent to API server\"\r\nI0605 20:23:00.426888    5697 server.go:662] \"--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\"\r\nI0605 20:23:00.427421    5697 container_manager_linux.go:265] \"Container manager verified user specified cgroup-root exists\" cgroupRoot=[]\r\nI0605 20:23:00.427488    5697 container_manager_linux.go:270] \"Creating Container Manager object based on Node Config\" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}\r\nI0605 20:23:00.427515    5697 topology_manager.go:136] \"Creating topology manager with policy per scope\" topologyPolicyName=\"none\" topologyScopeName=\"container\"\r\nI0605 20:23:00.427529    5697 container_manager_linux.go:301] \"Creating device plugin manager\"\r\nI0605 20:23:00.427572    5697 state_mem.go:36] \"Initialized new in-memory state store\"\r\nI0605 20:23:00.431001    5697 kubelet.go:411] \"Kubelet is running in standalone mode, will skip API server sync\"\r\nI0605 20:23:00.431434    5697 kuberuntime_manager.go:257] \"Container runtime initialized\" containerRuntime=\"containerd\" version=\"v1.7.15\" apiVersion=\"v1\"\r\nI0605 20:23:00.431651    5697 volume_host.go:75] \"KubeClient is nil. Skip initialization of CSIDriverLister\"\r\nW0605 20:23:00.432240    5697 csi_plugin.go:189] kubernetes.io/csi: kubeclient not set, assuming standalone kubelet\r\nW0605 20:23:00.432259    5697 csi_plugin.go:266] Skipping CSINode initialization, kubelet running in standalone mode\r\nE0605 20:23:00.432366    5697 safe_sysctls.go:62] \"Kernel version is too old, dropping net.ipv4.ip_local_reserved_ports from safe sysctl list\" kernelVersion=\"3.10.0\"\r\nI0605 20:23:00.432500    5697 server.go:1168] \"Started kubelet\"\r\nI0605 20:23:00.432828    5697 kubelet.go:1548] \"No API server defined - no node status update will be sent\"\r\nI0605 20:23:00.432922    5697 server.go:194] \"Starting to listen read-only\" address=\"0.0.0.0\" port=10255\r\nI0605 20:23:00.434664    5697 server.go:162] \"Starting to listen\" address=\"0.0.0.0\" port=10250\r\nI0605 20:23:00.434729    5697 ratelimit.go:65] \"Setting rate limiting for podresources endpoint\" qps=100 burstTokens=10\r\nI0605 20:23:00.434909    5697 fs_resource_analyzer.go:67] \"Starting FS ResourceAnalyzer\"\r\nE0605 20:23:00.435607    5697 server.go:794] \"Failed to start healthz server\" err=\"listen tcp 127.0.0.1:10248: bind: address already in use\"\r\nE0605 20:23:00.435846    5697 cri_stats_provider.go:455] \"Failed to get the info of the filesystem with mountpoint\" err=\"unable to find data in memory cache\" mountpoint=\"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\"\r\nI0605 20:23:00.435859    5697 server.go:461] \"Adding debug handlers to kubelet server\"\r\nE0605 20:23:00.435879    5697 kubelet.go:1400] \"Image garbage collection failed once. Stats initialization may not have completed yet\" err=\"invalid capacity 0 on image filesystem\"\r\nI0605 20:23:00.436149    5697 volume_manager.go:284] \"Starting Kubelet Volume Manager\"\r\nI0605 20:23:00.436339    5697 desired_state_of_world_populator.go:145] \"Desired state populator starts to run\"\r\nE0605 20:23:00.436476    5697 server.go:179] \"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"\r\n\r\n\r\ncrictlï¼š\r\n[root@k8smaster ~]# crictl ps\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\r\nbde21b3a0d030       9dd45d0a36c9e       About an hour ago   Running             cilium-operator           0                   b302f86dd95a2       cilium-operator-86666d88cb-dnfhx\r\na6df0160e0dde       21dd3d6f9c60d       About an hour ago   Running             kube-proxy                0                   93053c30c7029       kube-proxy-pfngx\r\nbee6716da8735       58cbecfde1998       About an hour ago   Running             kube-controller-manager   12                  e1a8c7a48c7d8       kube-controller-manager-k8smaster\r\n3a1adb4be83ec       44e520c7a8226       About an hour ago   Running             kube-apiserver            12                  40c7e5974affe       kube-apiserver-k8smaster\r\n23124a7bc7e7f       c8c40891e65bd       About an hour ago   Running             kube-scheduler            12                  d6c6d6a8e48cf       kube-scheduler-k8smaster\r\ncd68f2af5bed6       73deb9a3f7025       About an hour ago   Running             etcd                      12                  f57813da9d5f0       etcd-k8smaster\r\n\r\n\r\ncrictl imagesï¼š\r\n[root@k8smaster ~]# crictl images\r\nIMAGE                                     TAG                 IMAGE ID            SIZE\r\nsealos.hub:5000/cilium/cilium             v1.13.4             d00a7abfa71a6       174MB\r\nsealos.hub:5000/cilium/operator           v1.13.4             9dd45d0a36c9e       30.8MB\r\nsealos.hub:5000/coredns/coredns           v1.10.1             ead0a4a53df89       16.2MB\r\nsealos.hub:5000/etcd                      3.5.9-0             73deb9a3f7025       103MB\r\nsealos.hub:5000/kube-apiserver            v1.27.7             44e520c7a8226       33.5MB\r\nsealos.hub:5000/kube-controller-manager   v1.27.7             58cbecfde1998       31MB\r\nsealos.hub:5000/kube-proxy                v1.27.7             21dd3d6f9c60d       23.9MB\r\nsealos.hub:5000/kube-scheduler            v1.27.7             c8c40891e65bd       18.2MB\r\nsealos.hub:5000/pause                     3.9                 e6f1816883972       319kB\r\n\r\n\r\n\r\n\r\n\n\n### Operating environment\n\n```markdown\n- Sealos version:\r\n- Docker version:\r\n- Kubernetes version:\r\n- Operating system:\r\n- Runtime environment:\r\n- Cluster size:\r\n- Additional information:\n```\n\n\n### Additional information\n\n_No response_",
        "closed": true,
        "closedAt": "2024-10-05T01:06:00Z",
        "number": 4769,
        "state": "CLOSED",
        "title": "BUG: brief description of the bug",
        "url": "https://github.com/labring/sealos/issues/4769",
        "login": "zhengyazhao",
        "is_bot": false
      },
      {
        "body": "ç‰ˆæœ¬ï¼š `v5.0.0-beta5`\r\nåç«¯ `costcenter-frontend` æ—¥å¿—ä¸»è¦ä¿¡æ¯ï¼š\r\n\r\n```\r\nbody: {\r\n      kind: 'Status',\r\n      apiVersion: 'v1',\r\n      metadata: {},\r\n      status: 'Failure',\r\n      message: `Transfer.account.sealos.io \"5l0Lo9YkWm-1717084992510\" is invalid: metadata.name: Invalid value: \"5l0Lo9YkWm-1717084992510\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')`,\r\n      reason: 'Invalid',\r\n      details: [Object],\r\n      code: 422\r\n    },\r\n```\r\n\r\nåŸå› åˆ†æï¼š\r\nç”¨æˆ·idæœ‰å¤§å†™å­—æ¯ï¼Œå¯¼è‡´ä¸ç¬¦åˆ RFC 1123",
        "closed": true,
        "closedAt": "2024-06-27T13:42:04Z",
        "number": 4766,
        "state": "CLOSED",
        "title": "Transfer error: transfer error",
        "url": "https://github.com/labring/sealos/issues/4766",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "```bash\r\ninstall.sh \\\r\n    --zh  \\\r\n    --cloud-version=v5.0.0-beta5 \\\r\n    --kubernetes-version=1.28.10 \\\r\n    --pod-cidr=100.64.0.0/10 \\\r\n    --service-cidr=192.168.0.0/16  \\\r\n    --cloud-domain=x.com  \\\r\n    --cloud-port=8443 \\\r\n    --cert-path=/root/x.com.crt  \\\r\n    --key-path=/root/x.com.key  \\\r\n    --ssh-password=x \\\r\n    --master-ips=10.0.0.50 \\\r\n    --image-registry=registry.cn-shanghai.aliyuncs.com\r\n```\r\n\r\ncloud-port=443æ—¶ä¸€åˆ‡æ­£å¸¸,cloud-portæ¢æˆ8443å°±é”™è¯¯äº†\r\n<img width=\"1111\" alt=\"image\" src=\"https://github.com/labring/sealos/assets/84728412/a4ecf39e-8fbd-46e0-83b7-6b855623289b\">\r\n\r\nä»¥ä¸‹æ˜¯å‘ç”Ÿé”™è¯¯åè·å–çš„æ•°æ®ï¼š\r\n\r\n```\r\nkubectl get pods -A\r\nNAMESPACE                   NAME                                                              READY   STATUS                            RESTARTS      AGE\r\ncert-manager                cert-manager-6dc66985d4-s52b4                                     1/1     Running                           0             6m51s\r\ncert-manager                cert-manager-cainjector-c7d4dbdd9-hhkwq                           1/1     Running                           0             6m51s\r\ncert-manager                cert-manager-webhook-847d7676c9-6c8pc                             1/1     Running                           0             6m51s\r\ncockroach-operator-system   cockroach-operator-manager-6fd5c68d58-z5ckb                       1/1     Running                           0             6m20s\r\ningress-nginx               ingress-nginx-controller-j9mk8                                    0/1     Running                           3 (18s ago)   3m40s\r\nkb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-fzc98                     1/1     Running                           0             3m31s\r\nkb-system                   kubeblocks-85ddddddd4-bfcjk                                       1/1     Running                           0             5m28s\r\nkb-system                   kubeblocks-dataprotection-7f9c76fb8f-b5xzt                        1/1     Running                           0             5m28s\r\nkube-system                 cilium-operator-774fcb74cb-x8zxx                                  1/1     Running                           0             7m21s\r\nkube-system                 cilium-wjcv9                                                      1/1     Running                           0             7m21s\r\nkube-system                 coredns-5dd5756b68-b4qg6                                          1/1     Running                           0             7m30s\r\nkube-system                 coredns-5dd5756b68-mmrdn                                          1/1     Running                           0             7m29s\r\nkube-system                 etcd-k8s-master-1                                                 1/1     Running                           1             7m53s\r\nkube-system                 kube-apiserver-k8s-master-1                                       1/1     Running                           1             7m55s\r\nkube-system                 kube-controller-manager-k8s-master-1                              1/1     Running                           1             7m55s\r\nkube-system                 kube-proxy-2st9p                                                  1/1     Running                           0             7m30s\r\nkube-system                 kube-scheduler-k8s-master-1                                       1/1     Running                           1             7m53s\r\nkube-system                 metrics-server-68f967f7dc-rp7r2                                   1/1     Running                           0             6m24s\r\nopenebs                     openebs-localpv-provisioner-56d6489bbc-jhmnl                      1/1     Running                           0             6m25s\r\nsealos                      desktop-frontend-7667c4cb97-wjwxp                                 0/1     Init:CreateContainerConfigError   0             2m25s\r\nsealos                      sealos-cockroachdb-0                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-cockroachdb-1                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-cockroachdb-2                                              1/1     Running                           0             3m27s\r\nsealos                      sealos-mongodb-mongodb-0                                          3/3     Running                           0             3m19s\r\nvm                          victoria-metrics-k8s-stack-grafana-57c69bcd59-wp88s               3/3     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-wpnc2    1/1     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter-pcxhv         1/1     Running                           0             6m5s\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bj68sn   1/1     Running                           0             6m5s\r\nvm                          vmagent-victoria-metrics-k8s-stack-894699484-87whr                2/2     Running                           0             4m10s\r\nvm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-j68hm               2/2     Running                           0             5m57s\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running                           0             5m57s\r\nvm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-jfj56               1/1     Running                           0             5m57s\r\n```\r\n\r\n```\r\nkubectl get endpoints -A\r\nNAMESPACE                   NAME                                                   ENDPOINTS                                                             AGE\r\ncert-manager                cert-manager                                           100.64.0.198:9402                                                     10m\r\ncert-manager                cert-manager-webhook                                   100.64.0.24:10250                                                     10m\r\ncockroach-operator-system   cockroach-operator-webhook-service                     100.64.0.213:9443                                                     9m34s\r\ndefault                     kubernetes                                             10.0.0.50:6443                                                        11m\r\ningress-nginx               ingress-nginx-controller                                                                                                     8m59s\r\ningress-nginx               ingress-nginx-controller-admission                                                                                           8m59s\r\nkb-system                   kubeblocks                                             100.64.0.227:9443,100.64.0.237:9443                                   8m42s\r\nkube-system                 hubble-peer                                            10.0.0.50:4244                                                        10m\r\nkube-system                 kube-dns                                               100.64.0.106:53,100.64.0.84:53,100.64.0.106:53 + 3 more...            10m\r\nkube-system                 metrics-server                                         100.64.0.108:10250                                                    9m38s\r\nkube-system                 victoria-metrics-k8s-stack-coredns                     100.64.0.106:9153,100.64.0.84:9153                                    9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-controller-manager     10.0.0.50:10257                                                       9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-etcd                   10.0.0.50:2379                                                        9m19s\r\nkube-system                 victoria-metrics-k8s-stack-kube-scheduler              10.0.0.50:10259                                                       9m19s\r\nopenebs                     openebs.io-local                                       <none>                                                                9m37s\r\nsealos                      desktop-frontend                                                                                                             5m39s\r\nsealos                      sealos-cockroachdb                                     100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s\r\nsealos                      sealos-cockroachdb-public                              100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s\r\nsealos                      sealos-mongodb-mongodb                                 100.64.0.225:27017                                                    6m34s\r\nsealos                      sealos-mongodb-mongodb-headless                        100.64.0.225:8888,100.64.0.225:50001,100.64.0.225:9216 + 2 more...    6m34s\r\nvm                          victoria-metrics-k8s-stack-grafana                     100.64.0.219:3000                                                     9m19s\r\nvm                          victoria-metrics-k8s-stack-kube-state-metrics          100.64.0.6:8080                                                       9m19s\r\nvm                          victoria-metrics-k8s-stack-prometheus-node-exporter    10.0.0.50:9100                                                        9m19s\r\nvm                          victoria-metrics-k8s-stack-victoria-metrics-operator   100.64.0.224:9443,100.64.0.224:8080                                   9m19s\r\nvm                          vmagent-victoria-metrics-k8s-stack                     100.64.0.39:8429                                                      9m8s\r\nvm                          vmalert-victoria-metrics-k8s-stack                     100.64.0.53:8080                                                      9m10s\r\nvm                          vmalertmanager-victoria-metrics-k8s-stack              100.64.0.50:9094,100.64.0.50:9094,100.64.0.50:9093                    9m11s\r\nvm                          vmsingle-victoria-metrics-k8s-stack                    100.64.0.58:8429                                                      8m46s\r\n```\r\n\r\n```\r\nkubectl describe pod -n ingress-nginx               ingress-nginx-controller-j9mk8\r\nName:             ingress-nginx-controller-j9mk8\r\nNamespace:        ingress-nginx\r\nPriority:         0\r\nService Account:  ingress-nginx\r\nNode:             k8s-master-1/10.0.0.50\r\nStart Time:       Wed, 29 May 2024 12:32:13 -0400\r\nLabels:           app.kubernetes.io/component=controller\r\n                  app.kubernetes.io/instance=ingress-nginx\r\n                  app.kubernetes.io/managed-by=Helm\r\n                  app.kubernetes.io/name=ingress-nginx\r\n                  app.kubernetes.io/part-of=ingress-nginx\r\n                  app.kubernetes.io/version=1.9.4\r\n                  controller-revision-hash=674c9b795f\r\n                  helm.sh/chart=ingress-nginx-4.8.3\r\n                  pod-template-generation=4\r\nAnnotations:      <none>\r\nStatus:           Running\r\nIP:               10.0.0.50\r\nIPs:\r\n  IP:           10.0.0.50\r\nControlled By:  DaemonSet/ingress-nginx-controller\r\nContainers:\r\n  controller:\r\n    Container ID:  containerd://7ce271a4ac42f9c9065f6b17ebba031b05ebf43e09086c30ddd2b52944286f63\r\n    Image:         registry.k8s.io/ingress-nginx/controller:v1.9.4\r\n    Image ID:      sealos.hub:5000/ingress-nginx/controller@sha256:0115d7e01987c13e1be90b09c223c3e0d8e9a92e97c0421e712ad3577e2d78e5\r\n    Ports:         80/TCP, 443/TCP, 8443/TCP\r\n    Host Ports:    80/TCP, 443/TCP, 8443/TCP\r\n    Args:\r\n      /nginx-ingress-controller\r\n      --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\r\n      --election-id=ingress-nginx-leader\r\n      --controller-class=k8s.io/ingress-nginx\r\n      --ingress-class=nginx\r\n      --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\r\n      --validating-webhook=:8443\r\n      --validating-webhook-certificate=/usr/local/certificates/cert\r\n      --validating-webhook-key=/usr/local/certificates/key\r\n      --https-port=8443\r\n      --default-ssl-certificate=sealos-system/wildcard-cert\r\n    State:          Running\r\n      Started:      Wed, 29 May 2024 12:44:15 -0400\r\n    Last State:     Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 29 May 2024 12:40:27 -0400\r\n      Finished:     Wed, 29 May 2024 12:41:35 -0400\r\n    Ready:          False\r\n    Restart Count:  7\r\n    Requests:\r\n      cpu:      100m\r\n      memory:   90Mi\r\n    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=5\r\n    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      POD_NAME:       ingress-nginx-controller-j9mk8 (v1:metadata.name)\r\n      POD_NAMESPACE:  ingress-nginx (v1:metadata.namespace)\r\n      LD_PRELOAD:     /usr/local/lib/libmimalloc.so\r\n    Mounts:\r\n      /usr/local/certificates/ from webhook-cert (ro)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bsr9t (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  webhook-cert:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  ingress-nginx-admission\r\n    Optional:    false\r\n  kube-api-access-bsr9t:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              kubernetes.io/os=linux\r\nTolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists\r\n                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\r\n                             node.kubernetes.io/not-ready:NoExecute op=Exists\r\n                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists\r\n                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\r\nEvents:\r\n  Type     Reason     Age                     From                      Message\r\n  ----     ------     ----                    ----                      -------\r\n  Normal   Scheduled  12m                     default-scheduler         Successfully assigned ingress-nginx/ingress-nginx-controller-j9mk8 to k8s-master-1\r\n  Normal   RELOAD     12m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   Killing    11m                     kubelet                   Container controller failed liveness probe, will be restarted\r\n  Normal   Pulled     11m (x2 over 12m)       kubelet                   Container image \"registry.k8s.io/ingress-nginx/controller:v1.9.4\" already present on machine\r\n  Normal   Created    11m (x2 over 12m)       kubelet                   Created container controller\r\n  Normal   Started    11m (x2 over 12m)       kubelet                   Started container controller\r\n  Normal   RELOAD     11m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  Unhealthy  10m (x8 over 12m)       kubelet                   Liveness probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   RELOAD     10m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     8m57s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     7m47s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  Unhealthy  7m9s (x31 over 12m)     kubelet                   Readiness probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   RELOAD     6m37s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Normal   RELOAD     4m5s                    nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n  Warning  BackOff    2m17s (x13 over 5m28s)  kubelet                   Back-off restarting failed container controller in pod ingress-nginx-controller-j9mk8_ingress-nginx(f0a3d63a-6bca-4a43-b54a-120eca26069c)\r\n  Normal   RELOAD     16s                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration\r\n```",
        "closed": true,
        "closedAt": "2024-06-26T08:32:15Z",
        "number": 4764,
        "state": "CLOSED",
        "title": "cloud-port=8443æ—¶å®‰è£…cloudä¼šå‘ç”Ÿé”™è¯¯ `no endpoints available for service \"ingress-noinx-controller-admission\"`",
        "url": "https://github.com/labring/sealos/issues/4764",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L138\nhttps://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L147\nè¿™ä¸¤å¤„è·å–åˆ°çš„å€¼å¤–é¢ä¼šæœ‰ä¸€å±‚åŒå¼•å·ï¼Œå¯¼è‡´åç»­ä½¿ç”¨å˜é‡çš„æ—¶å€™é€ æˆä¸¥é‡çš„é”™è¯¯",
        "closed": true,
        "closedAt": "2024-05-29T07:01:03Z",
        "number": 4760,
        "state": "CLOSED",
        "title": "deploy/cloud/scripts/init.sh The wrong format of the variable causes the --env parameter parsing to fail.",
        "url": "https://github.com/labring/sealos/issues/4760",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "# Issue: æ–°å¢ç¹é«”ä¸­æ–‡èªè¨€æ”¯æ´\r\n\r\nç›®å‰ï¼Œæ‡‰ç”¨ç¨‹å¼ä¸æ”¯æ´ç¹é«”ä¸­æ–‡ï¼Œé€™é™åˆ¶äº†é‚£äº›æ›´ç¿’æ…£ä½¿ç”¨ç¹é«”ä¸­æ–‡çš„ç”¨æˆ¶çš„å¯ç”¨æ€§ã€‚\r\n\r\n## å¦‚æœä½ æœ‰è§£æ±ºæ–¹æ¡ˆï¼Œè«‹æè¿°ä¸€ä¸‹\r\nåŠ å…¥ç¤¾å€è‡ªé¡˜ç¿»è­¯ï¼Œæäº¤åˆ°github\r\n\r\n",
        "closed": true,
        "closedAt": "2024-09-27T06:54:11Z",
        "number": 4759,
        "state": "CLOSED",
        "title": "æ–°å¢ç¹é«”ä¸­æ–‡èªè¨€æ”¯æ´",
        "url": "https://github.com/labring/sealos/issues/4759",
        "login": "TW199501",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sealos-admin",
    "issues": [
      {
        "body": "https://github.com/labring/sealos-admin/actions/runs/17451681116/job/49558437160 ä¸»è¦è§£å†³è¿™ä¸ªé—®é¢˜",
        "closed": true,
        "closedAt": "2025-09-04T04:10:00Z",
        "number": 35,
        "state": "CLOSED",
        "title": "${{ github.sha }} å¤ªé•¿ï¼Œéœ€è¦å’Œ dockeré•œåƒä¿æŒä¸€è‡´",
        "url": "https://github.com/labring/sealos-admin/issues/35",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. æ‹†åˆ†åŒæ¶æ„ amd64 ä½¿ç”¨runneræ˜¯ buildjet-2vcpu-ubuntu-2204 è€Œ arm64ä½¿ç”¨runneræ˜¯ buildjet-4vcpu-ubuntu-2204-arm\n2. ä»…é™å®¹å™¨é•œåƒç‰ˆæœ¬ï¼Œé›†ç¾¤é•œåƒæ— éœ€ä¿®æ”¹ã€‚",
        "closed": true,
        "closedAt": "2025-09-04T03:30:56Z",
        "number": 33,
        "state": "CLOSED",
        "title": "ai: æ‹†åˆ†åŒæ¶æ„",
        "url": "https://github.com/labring/sealos-admin/issues/33",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-04T01:59:14Z",
        "number": 31,
        "state": "CLOSED",
        "title": "workflow æ·»åŠ ä¿å­˜taråŒ…çš„æµç¨‹",
        "url": "https://github.com/labring/sealos-admin/issues/31",
        "login": "cuisongliu",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sealos-pro",
    "issues": [
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 63,
        "state": "OPEN",
        "title": "æ’ä»¶åŒ–devbox sql",
        "url": "https://github.com/labring/sealos-pro/issues/63",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Error: buildx failed with: ERROR: failed to build: failed to solve: bitnami/git: failed to resolve source metadata for docker.io/bitnami/git:latest: docker.io/bitnami/git:latest: not found",
        "closed": true,
        "closedAt": "2025-10-01T04:23:59Z",
        "number": 62,
        "state": "CLOSED",
        "title": "æ¨¡ç‰ˆä»“åº“bitnamié—®é¢˜",
        "url": "https://github.com/labring/sealos-pro/issues/62",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n{\n\tname:                 \"ipv4 cidr too big\",\n\tcidr:                 *getIPnetFromCIDR(\"10.92.0.0/8\"),\n\tmaxCIDRBits:          20,\n\tcidrFlag:             \"--service-cluster-ip-range\",\n\texpectedErrorMessage: \"specified --service-cluster-ip-range is too large; for 32-bit addresses, the mask must be >= 12\",\n\texpectErrors:         true,\n},\n{\n\tname:                 \"ipv6 cidr too big\",\n\tcidr:                 *getIPnetFromCIDR(\"3000::/64\"),\n\tmaxCIDRBits:          20,\n\tcidrFlag:             \"--service-cluster-ip-range\",\n\texpectedErrorMessage: \"specified --service-cluster-ip-range is too large; for 128-bit addresses, the mask must be >= 108\",\n\texpectErrors:         true,\n},\n```",
        "closed": false,
        "closedAt": null,
        "number": 61,
        "state": "OPEN",
        "title": "æ”¯æŒ IPV6",
        "url": "https://github.com/labring/sealos-pro/issues/61",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-10-09T04:15:27Z",
        "number": 60,
        "state": "CLOSED",
        "title": "æ·±ä¿¡æœå†…æ ¸æ£€æµ‹ä¸é€šè¿‡",
        "url": "https://github.com/labring/sealos-pro/issues/60",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\netcd:\n  local:\n    dataDir: {{ default \"/var/lib/etcd\" .KUBEADM_ETCD_DATA }}\n\n```",
        "closed": true,
        "closedAt": "2025-09-29T02:05:03Z",
        "number": 58,
        "state": "CLOSED",
        "title": "etc æ”¯æŒè‡ªå®šä¹‰ç›®å½•",
        "url": "https://github.com/labring/sealos-pro/issues/58",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "ç›®å‰å›ºå®šçš„ éœ€è¦åšåˆ°éšæœºå¯†ç ",
        "closed": true,
        "closedAt": "2025-10-05T16:09:41Z",
        "number": 57,
        "state": "CLOSED",
        "title": "admin é»˜è®¤å¯†ç éšæœº",
        "url": "https://github.com/labring/sealos-pro/issues/57",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"1450\" height=\"410\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9be0e69a-c714-4bee-bddb-24027f4ea93b\" />",
        "closed": false,
        "closedAt": null,
        "number": 56,
        "state": "OPEN",
        "title": "rootfsçš„è”ç³»æ–¹å¼å˜æ›´",
        "url": "https://github.com/labring/sealos-pro/issues/56",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\ndb.properties.insertMany(\n    [{\n      \"name\": \"cpu\",\n      \"alias\": \"\",\n      \"enum\": 0,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 2.237442922,\n      \"encrypt_unit_price\": \"iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl\",\n      \"unit\": \"1m\"\n    },{\n      \"name\": \"memory\",\n      \"alias\": \"\",\n      \"enum\": 1,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 1.092501427,\n      \"encrypt_unit_price\": \"OSE6JKZaoZTdcNQsUA0GzdrObISwNrugpAkShRcq22DaOYpchmxf\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"storage\",\n      \"alias\": \"\",\n      \"enum\": 2,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 0,\n      \"encrypt_unit_price\": \"DPo+4f6FksbJ0k0g6SJifQ+PXz+rhUl/W/MahCc=\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"services.nodeports\",\n      \"alias\": \"\",\n      \"enum\": 4,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 500,\n      \"encrypt_unit_price\": \"OfHNWknr4vDHM+Amy6EfvqrSGjF+YIfZ9yBlBwp6vw==\",\n      \"unit\": \"1\",\n      \"view_price\": 500000\n    },{\n      \"name\": \"network\",\n      \"alias\": \"\",\n      \"enum\": 3,\n      \"price_type\": \"DIF\",\n      \"unit_price\": 781,\n      \"encrypt_unit_price\": \"4fGwqMywAqV/2O9ED/XB6udBdsqVKS5Y75rfzzW8pw==\",\n      \"unit\": \"1Mi\"\n    },{\n      \"name\": \"gpu-NVIDIA-GeForce-RTX-4090\",\n      \"alias\": \"RTX-4090\",\n      \"enum\": 5,\n      \"price_type\": \"AVG\",\n      \"unit_price\": 2.237442922,\n      \"encrypt_unit_price\": \"iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl\",\n      \"unit\": \"1m\"\n    }]\n)\n```\n```\n# è·å– mongodb æ•°æ®åº“çš„åå­—\nkubectl get cluster -n sealos\n# kbcli è¿æ¥ mongodb æ•°æ®åº“\nkbcli cluster connect sealos-mongodb -n sealos\n\nsealos-mongodb-mongodb [primary] admin> show dbs\nsealos-mongodb-mongodb [primary] admin> use sealos-resources\nsealos-mongodb-mongodb [primary] admin> show collections\n\n# é‡å¯ account service å’Œ account controller\nkubectl rollout restart deploy account-controller-manager -n account-system\nkubectl rollout restart deploy account-service -n account-system\n```",
        "closed": true,
        "closedAt": "2025-09-27T07:06:05Z",
        "number": 52,
        "state": "CLOSED",
        "title": "è´¹ç”¨æ”¯æŒ",
        "url": "https://github.com/labring/sealos-pro/issues/52",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. åªæœ‰ cc ç”¨åˆ°äº†,åªæ˜¯çº³ç®¡èµ·æ¥è€Œå·²\n2. æ§åˆ¶å®šæ—¶å¼€å…³æœºçš„\n\nhttps://github.com/labring/sealos/pull/5559",
        "closed": false,
        "closedAt": null,
        "number": 49,
        "state": "OPEN",
        "title": "devboxscheduler",
        "url": "https://github.com/labring/sealos-pro/issues/49",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 48,
        "state": "OPEN",
        "title": "sealos add éœ€è¦æ”¯æŒ env å‚æ•°",
        "url": "https://github.com/labring/sealos-pro/issues/48",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/LuyFw89BwiL2IpkzCcxcwsLonzg?from=from_copylink",
        "closed": false,
        "closedAt": null,
        "number": 47,
        "state": "OPEN",
        "title": "æ”¯æŒGPU",
        "url": "https://github.com/labring/sealos-pro/issues/47",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 46,
        "state": "OPEN",
        "title": "ä¿®æ”¹containerdçš„ç‰ˆæœ¬å· åŒºåˆ†ä¸Šæ¸¸ç‰ˆæœ¬å’Œè‡ªå·±æ›´æ”¹çš„ç‰ˆæœ¬",
        "url": "https://github.com/labring/sealos-pro/issues/46",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "apiserver\n```\nservice-cluster-ip-range: \"10.96.0.0/16\" # é‡è¦ï¼å¦‚æœå®‰è£…æ—¶æœªè°ƒæ•´\nmax-requests-inflight: \"1000\"\nmax-mutating-requests-inflight: \"1000\"\n```\n\n\ncontroller-manager\n\n```\nconcurrent-deployment-syncs: \"100\"\nconcurrent-endpoint-syncs: \"50\"\nconcurrent-gc-syncs: \"100\"\nconcurrent-namespace-syncs: \"100\"\nconcurrent-rc-syncs: \"100\"\nconcurrent-replicaset-syncs: \"100\"\nconcurrent-service-endpoint-syncs: \"50\"\nconcurrent-service-syncs: \"100\"\nkube-api-burst: \"100\"\nkube-api-qps: \"100\"\n```",
        "closed": true,
        "closedAt": "2025-09-24T11:30:00Z",
        "number": 45,
        "state": "CLOSED",
        "title": "å‚æ•°è°ƒä¼˜",
        "url": "https://github.com/labring/sealos-pro/issues/45",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"2332\" height=\"632\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f5e5d73b-d0b2-4b46-98fc-e7ed8b6b3c76\" />",
        "closed": true,
        "closedAt": "2025-09-24T06:51:35Z",
        "number": 40,
        "state": "CLOSED",
        "title": "labring4docker çš„ curl-kubectl é•œåƒä¸¢å¤±",
        "url": "https://github.com/labring/sealos-pro/issues/40",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cockroachdb-backup\n  namespace: cockroach-operator-system\nspec:\n  concurrencyPolicy: Forbid\n  failedJobsHistoryLimit: 2\n  jobTemplate:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: cockroachdb-backup\n        job-type: scheduled\n    spec:\n      activeDeadlineSeconds: 1200\n      backoffLimit: 1\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: cockroachdb-backup\n        spec:\n          containers:\n          - command:\n            - /bin/bash\n            - -c\n            - |\n              set -e\n              \n              echo \"=== CockroachDB æ¯æ—¥å¤‡ä»½å¼€å§‹ ===\"\n              echo \"å¤‡ä»½å¼€å§‹æ—¶é—´ï¼ˆUTCï¼‰: $(date -u)\"\n              echo \"å¤‡ä»½å¼€å§‹æ—¶é—´ï¼ˆåŒ—äº¬ï¼‰: $(TZ='Asia/Shanghai' date)\"\n              echo \"å®šæœŸå¤‡ä»½: æ¯å¤©åŒ—äº¬æ—¶é—´0ç‚¹æ‰§è¡Œ\"\n              \n              # ä»å…±äº«æ–‡ä»¶è¯»å–æ¡¶å\n              LOCAL_BUCKET=$(cat /shared/local_bucket)\n              GLOBAL_BUCKET=$(cat /shared/global_bucket)\n              BACKUP_DATE=$(cat /shared/timestamp)\n              \n              echo \"æ¯æ—¥å¤‡ä»½é…ç½®:\"\n              echo \"  æ‰§è¡Œè®¡åˆ’: æ¯å¤©åŒ—äº¬æ—¶é—´0ç‚¹ï¼ˆUTC 16ç‚¹ï¼‰\"\n              echo \"  å¤‡ä»½æ—¥æœŸ: $BACKUP_DATE\"\n              echo \"  æœ¬åœ°æ¡¶: $LOCAL_BUCKET\"\n              echo \"  å…¨å±€æ¡¶: $GLOBAL_BUCKET\"\n              echo \"  MinIOç«¯ç‚¹: $MINIO_ENDPOINT\"\n              echo \"  å¤‡ä»½ç­–ç•¥: å½“å‰æ—¶é—´ç‚¹å…¨é‡å¤‡ä»½\"\n              \n              # é¦–å…ˆæ£€æŸ¥GCé˜ˆå€¼ï¼Œä¸æŒ‡å®šå…·ä½“æ—¶é—´ç‚¹è¿›è¡Œå½“å‰æ—¶é—´å¤‡ä»½\n              echo \"=== æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ ===\"\n              \n              # å‡½æ•°ï¼šè·å–å¯ç”¨çš„å¤‡ä»½æ—¶é—´ç‚¹\n              get_safe_backup_time() {\n                local pod_name=$1\n                local certs_dir=$2\n                echo \"æ£€æŸ¥ $pod_name çš„GCé˜ˆå€¼...\"\n                \n                # æŸ¥è¯¢GCé˜ˆå€¼\n                local gc_query=\"SELECT cluster_logical_timestamp() - (25 * 60 * 60 * 1000000000)::INT8 AS safe_timestamp;\"\n                \n                kubectl exec -n cockroach-operator-system \"$pod_name\" -- \\\n                  cockroach sql --certs-dir=\"$certs_dir\" --host=localhost:26257 \\\n                  --execute=\"$gc_query\" --format=csv | tail -n +2\n              }\n              \n              # æ„å»ºå¤‡ä»½å‘½ä»¤ï¼ˆä¸ä½¿ç”¨AS OF SYSTEM TIMEï¼Œå¤‡ä»½å½“å‰æ—¶é—´ç‚¹ï¼‰\n              GLOBAL_BACKUP_CMD=\"BACKUP TO 's3://${GLOBAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';\"\n              \n              LOCAL_BACKUP_CMD=\"BACKUP TO 's3://${LOCAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';\"\n              \n              # æ‰§è¡Œå…¨å±€å¤‡ä»½\n              echo \"=== å¼€å§‹å…¨å±€å¤‡ä»½ ===\"\n              GLOBAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb-global | grep Running | head -n1 | awk '{print $1}')\n              \n              if [ -n \"$GLOBAL_POD\" ] && [ \"$GLOBAL_POD\" != \"\" ]; then\n                echo \"æ‰¾åˆ°å…¨å±€èŠ‚ç‚¹: $GLOBAL_POD\"\n                \n                # æ£€æŸ¥GCé˜ˆå€¼\n                echo \"æ£€æŸ¥å…¨å±€èŠ‚ç‚¹GCçŠ¶æ€...\"\n                GLOBAL_SAFE_TIME=$(get_safe_backup_time \"$GLOBAL_POD\" \"/cockroach/data/certs\" || echo \"\")\n                \n                if [ -n \"$GLOBAL_SAFE_TIME\" ]; then\n                  echo \"å…¨å±€èŠ‚ç‚¹å®‰å…¨æ—¶é—´æˆ³: $GLOBAL_SAFE_TIME\"\n                fi\n                \n                echo \"æ‰§è¡Œå…¨å±€å¤‡ä»½å‘½ä»¤ï¼ˆå½“å‰æ—¶é—´ç‚¹ï¼‰...\"\n                \n                if kubectl exec -n cockroach-operator-system \"$GLOBAL_POD\" -- \\\n                   cockroach sql --certs-dir=/cockroach/data/certs --host=localhost:26257 \\\n                   --execute \"$GLOBAL_BACKUP_CMD\"; then\n                  echo \"âœ“ å…¨å±€å¤‡ä»½æˆåŠŸ\"\n                else\n                  echo \"âœ— å…¨å±€å¤‡ä»½å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œæœ¬åœ°å¤‡ä»½\"\n                fi\n              else\n                echo \"âš  æœªæ‰¾åˆ°è¿è¡Œä¸­çš„å…¨å±€èŠ‚ç‚¹ï¼Œè·³è¿‡å…¨å±€å¤‡ä»½\"\n              fi\n              \n              # æ‰§è¡Œæœ¬åœ°å¤‡ä»½\n              echo \"=== å¼€å§‹æœ¬åœ°å¤‡ä»½ ===\"\n              LOCAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb | grep -v global | grep Running | head -n1 | awk '{print $1}')\n              \n              if [ -n \"$LOCAL_POD\" ] && [ \"$LOCAL_POD\" != \"\" ]; then\n                echo \"æ‰¾åˆ°æœ¬åœ°èŠ‚ç‚¹: $LOCAL_POD\"\n                \n                # æ£€æŸ¥GCé˜ˆå€¼\n                echo \"æ£€æŸ¥æœ¬åœ°èŠ‚ç‚¹GCçŠ¶æ€...\"\n                LOCAL_SAFE_TIME=$(get_safe_backup_time \"$LOCAL_POD\" \"/cockroach/cockroach-certs\" || echo \"\")\n                \n                if [ -n \"$LOCAL_SAFE_TIME\" ]; then\n                  echo \"æœ¬åœ°èŠ‚ç‚¹å®‰å…¨æ—¶é—´æˆ³: $LOCAL_SAFE_TIME\"\n                fi\n                \n                echo \"æ‰§è¡Œæœ¬åœ°å¤‡ä»½å‘½ä»¤ï¼ˆå½“å‰æ—¶é—´ç‚¹ï¼‰...\"\n                \n                # æŒ‡å®šå®¹å™¨åç§°é¿å…æ­§ä¹‰\n                if kubectl exec -n cockroach-operator-system \"$LOCAL_POD\" -c db -- \\\n                   cockroach sql --certs-dir=/cockroach/cockroach-certs --host=localhost:26257 \\\n                   --execute \"$LOCAL_BACKUP_CMD\"; then\n                  echo \"âœ“ æœ¬åœ°å¤‡ä»½æˆåŠŸ\"\n                else\n                  echo \"âœ— æœ¬åœ°å¤‡ä»½å¤±è´¥\"\n                  exit 1\n                fi\n              else\n                echo \"âœ— æœªæ‰¾åˆ°è¿è¡Œä¸­çš„æœ¬åœ°èŠ‚ç‚¹\"\n                exit 1\n              fi\n              \n              echo \"=== å¤‡ä»½éªŒè¯ ===\"\n              echo \"éªŒè¯å¤‡ä»½æ–‡ä»¶...\"\n              \n              # è®°å½•å¤‡ä»½å…ƒæ•°æ®\n              echo \"å¤‡ä»½å®Œæˆæ—¶é—´ï¼ˆUTCï¼‰: $(date -u)\" > /shared/backup_result\n              echo \"å¤‡ä»½å®Œæˆæ—¶é—´ï¼ˆåŒ—äº¬ï¼‰: $(TZ='Asia/Shanghai' date)\" >> /shared/backup_result\n              echo \"å¤‡ä»½æ—¥æœŸ: $BACKUP_DATE\" >> /shared/backup_result\n              echo \"æ‰§è¡Œè®¡åˆ’: æ¯å¤©åŒ—äº¬æ—¶é—´0ç‚¹\" >> /shared/backup_result\n              echo \"å…¨å±€å¤‡ä»½æ¡¶: $GLOBAL_BUCKET\" >> /shared/backup_result\n              echo \"æœ¬åœ°å¤‡ä»½æ¡¶: $LOCAL_BUCKET\" >> /shared/backup_result\n              echo \"å¤‡ä»½ç±»å‹: å½“å‰æ—¶é—´ç‚¹å¤‡ä»½\" >> /shared/backup_result\n              \n              echo \"=== æ¯æ—¥å¤‡ä»½æµç¨‹å®Œæˆ ===\"\n            env:\n            - name: MINIO_ENDPOINT\n              value: https://objectstorageapi.hzh.sealos.run\n            - name: MINIO_ACCESS_KEY\n              value: xxx\n            - name: MINIO_SECRET_KEY\n              value: ffff\n            image: bitnami/kubectl:latest\n            imagePullPolicy: Always\n            name: cockroachdb-backup\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: 2Gi\n              requests:\n                cpu: 500m\n                memory: 1Gi\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - mountPath: /shared\n              name: shared-data\n          dnsPolicy: ClusterFirst\n          initContainers:\n          - command:\n            - /bin/bash\n            - -c\n            - |\n              set -e\n              \n              # ç”Ÿæˆæ—¶é—´æˆ³å’Œéšæœºå­—ç¬¦ä¸²\n              TIMESTAMP=$(date +'%Y%m%d-%H%M')\n              RANDOM_STR=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 6 | head -n 1)\n              \n              export LOCAL_BUCKET=\"local-${TIMESTAMP}-${RANDOM_STR}\"\n              export GLOBAL_BUCKET=\"global-${TIMESTAMP}-${RANDOM_STR}\"\n              \n              echo \"=== æ¯æ—¥å¤‡ä»½å‡†å¤‡é˜¶æ®µ ===\"\n              echo \"å½“å‰UTCæ—¶é—´: $(date -u)\"\n              echo \"å½“å‰åŒ—äº¬æ—¶é—´: $(TZ='Asia/Shanghai' date)\"\n              echo \"å¤‡ä»½æ‰§è¡Œæ—¶é—´: åŒ—äº¬æ—¶é—´æ¯å¤©0ç‚¹\"\n              echo \"å¤‡ä»½æ—¥æœŸ: $TIMESTAMP\"\n              echo \"éšæœºæ ‡è¯†: $RANDOM_STR\"\n              echo \"æœ¬åœ°å¤‡ä»½æ¡¶: $LOCAL_BUCKET\"\n              echo \"å…¨å±€å¤‡ä»½æ¡¶: $GLOBAL_BUCKET\"\n              \n              # é…ç½® MinIO è¿æ¥\n              echo \"é…ç½® MinIO è¿æ¥...\"\n              mc alias set backup \"${MINIO_ENDPOINT}\" \"${MINIO_ACCESS_KEY}\" \"${MINIO_SECRET_KEY}\"\n              \n              # æµ‹è¯•è¿æ¥\n              echo \"æµ‹è¯• MinIO è¿æ¥...\"\n              mc admin info backup\n              \n              # åˆ›å»ºå­˜å‚¨æ¡¶\n              echo \"åˆ›å»ºå­˜å‚¨æ¡¶...\"\n              mc mb \"backup/${LOCAL_BUCKET}\" || echo \"æœ¬åœ°æ¡¶åˆ›å»ºå¤±è´¥æˆ–å·²å­˜åœ¨\"\n              mc mb \"backup/${GLOBAL_BUCKET}\" || echo \"å…¨å±€æ¡¶åˆ›å»ºå¤±è´¥æˆ–å·²å­˜åœ¨\"\n              \n              # å°†æ¡¶åå†™å…¥å…±äº«æ–‡ä»¶\n              echo \"$LOCAL_BUCKET\" > /shared/local_bucket\n              echo \"$GLOBAL_BUCKET\" > /shared/global_bucket\n              echo \"$TIMESTAMP\" > /shared/timestamp\n              \n              echo \"=== å‡†å¤‡é˜¶æ®µå®Œæˆ ===\"\n            env:\n            - name: MINIO_ENDPOINT\n              value: https://objectstorageapi.hzh.sealos.run\n            - name: MINIO_ACCESS_KEY\n              value: xxx\n            - name: MINIO_SECRET_KEY\n              value: ffff\n            image: bitnami/minio-client:latest\n            imagePullPolicy: Always\n            name: prepare-backup\n            resources:\n              limits:\n                cpu: 200m\n                memory: 256Mi\n              requests:\n                cpu: 100m\n                memory: 128Mi\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - mountPath: /shared\n              name: shared-data\n          restartPolicy: Never\n          schedulerName: default-scheduler\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n          serviceAccount: cockroachdb-backup-sa\n          serviceAccountName: cockroachdb-backup-sa\n          terminationGracePeriodSeconds: 60\n          volumes:\n          - emptyDir: {}\n            name: shared-data\n  schedule: 0 16 * * *\n  startingDeadlineSeconds: 1800\n  successfulJobsHistoryLimit: 5\n  suspend: false\nstatus:\n  lastScheduleTime: \"2025-09-23T08:00:00Z\"\n  lastSuccessfulTime: \"2025-09-23T08:01:51Z\"\n```",
        "closed": false,
        "closedAt": null,
        "number": 39,
        "state": "OPEN",
        "title": "å°å¼ºæ•°æ®åº“å¤‡ä»½",
        "url": "https://github.com/labring/sealos-pro/issues/39",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-23T14:24:54Z",
        "number": 38,
        "state": "CLOSED",
        "title": "sealos-enterprise çš„ paymentä¸å¯é‡å…¥",
        "url": "https://github.com/labring/sealos-pro/issues/38",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "kubectl edit cm -n costcenter-frontend   costcenter-frontend-config\n",
        "closed": true,
        "closedAt": "2025-09-23T14:26:15Z",
        "number": 37,
        "state": "CLOSED",
        "title": "å‰ç«¯æ²¡æœ‰å¼€å¯payment",
        "url": "https://github.com/labring/sealos-pro/issues/37",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "<img width=\"1511\" height=\"555\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/af8ae4a9-57be-49d6-a320-d60d7af7fbcc\" />\n\n\n<img width=\"1106\" height=\"92\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/63b26e4b-796f-4c08-9711-e312cf2bc8a9\" />",
        "closed": false,
        "closedAt": null,
        "number": 35,
        "state": "OPEN",
        "title": "admin çš„é›†ç¾¤ç›‘æ§",
        "url": "https://github.com/labring/sealos-pro/issues/35",
        "login": "nowinkeyy",
        "is_bot": false
      },
      {
        "body": "å¦‚æœå‘Šè­¦ä½¿ç”¨grafanaï¼Œåˆ™éœ€è¦å…³é—­alertmanagerã€‚è¯¦æƒ…å‚è§\nhttps://fael3z0zfze.feishu.cn/wiki/HAW5wmKvvioB7IkrljVc88Xennd",
        "closed": true,
        "closedAt": "2025-09-23T11:19:30Z",
        "number": 33,
        "state": "CLOSED",
        "title": "vmå…³é—­alertmanager",
        "url": "https://github.com/labring/sealos-pro/issues/33",
        "login": "bearslyricattack",
        "is_bot": false
      },
      {
        "body": "ç”±äºä»£ç ä¾èµ–è¿˜æ²¡å¤„ç†å®Œï¼Œä¸”ä¸­çŸ­æœŸå†…å¯èƒ½æœ‰ refactor çš„éœ€æ±‚ï¼Œç›®å‰æµé‡æ¨¡å—ä¸å­˜åœ¨ labring/sealos ä¸­ï¼Œè€Œæ˜¯åœ¨ https://github.com/dinoallo/sealos-nm-agent/ ï¼Œç›®å‰éƒ¨ç½²æ–¹å¼ä¸ºé›†ç¾¤é•œåƒï¼š\n\n```bash\nsealos run docker.io/dinoallo/sealos-nm-agent:v0.0.0-alpha -e \"NODE_CIDR=172.16.0.0/24\" -e \"POD_CIDR=100.64.0.0/10\" -e \"HOST_DEVS=eth0,eth1\"\n```\nå…¶ä¸­ï¼š\n- `NODE_CIDR` ï¼šä¸ºèŠ‚ç‚¹ IP æ®µ\n- `POD_CIDR` ï¼šä¸º POD IP æ®µ\n- `HOST_DEVS` ï¼šä¸ºèŠ‚ç‚¹ç½‘å¡è®¾å¤‡åç§°ï¼Œå¯ä»¥æŒ‡å®šå¤šä¸ªï¼ˆä¸å­˜åœ¨è·³è¿‡ï¼‰",
        "closed": false,
        "closedAt": null,
        "number": 32,
        "state": "OPEN",
        "title": "Cost Center æµé‡æ¨¡å—",
        "url": "https://github.com/labring/sealos-pro/issues/32",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/QDeBw7AphiQb3sk7vGzcZPkFndg",
        "closed": true,
        "closedAt": "2025-09-29T01:56:35Z",
        "number": 31,
        "state": "CLOSED",
        "title": "2025 Q3 Devbox & Containerd æ€§èƒ½ä¼˜åŒ–",
        "url": "https://github.com/labring/sealos-pro/issues/31",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf\néƒ¨ç½²å‚è€ƒï¼šhttps://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf ï¼Œç›®å‰åªæœ‰é•œåƒã€‚",
        "closed": false,
        "closedAt": null,
        "number": 30,
        "state": "OPEN",
        "title": "2025 Q2 Higress æ€§èƒ½ä¼˜åŒ–",
        "url": "https://github.com/labring/sealos-pro/issues/30",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/Uwljwf9buiQSLDkTDKxc117Oncg",
        "closed": false,
        "closedAt": null,
        "number": 29,
        "state": "OPEN",
        "title": "etcd event æ‹†åˆ†",
        "url": "https://github.com/labring/sealos-pro/issues/29",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/CBAowr3MaigLuAkrwHEcXQ9Unqx",
        "closed": false,
        "closedAt": null,
        "number": 28,
        "state": "OPEN",
        "title": "20250402 è¿æ¥å†²çªå¯¼è‡´ kubelet æ— æ³•è¿æ¥ api server",
        "url": "https://github.com/labring/sealos-pro/issues/28",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "| Controlleré¡¹ç›®åç§°                    | æ‰€å±ç‰ˆæœ¬            | è´Ÿè´£äºº |\n| ------------------------------------- | ------------------- | ------ |\n| sealos-cloud-account-controller       | all                 |        |\n| sealos-cloud-app-controller           | all                 |        |\n| sealos-cloud-db-adminer-controller    | å·²ç»åºŸå¼ƒ            |        |\n| sealos-cloud-devbox-controller        | å•†ä¸š&&å…¬æœ‰äº‘        |        |\n| sealos-cloud-job-controller           | all                 |        |\n| sealos-cloud-job-heartbeat-controller | all                 |        |\n| sealos-cloud-license-controller       | å•†ä¸šç‰ˆ                 |        |\n| sealos-cloud-node-controller          | å•†ä¸š&&å…¬æœ‰äº‘ã€GPUï¼Œæš‚æœªåŠ å…¥ã€‘ | å­æ¯…       |\n| sealos-cloud-objectstorage-controller | å•†ä¸š&&å…¬æœ‰äº‘        |        |\n| sealos-cloud-resources-controller     | all                 |        |\n| sealos-cloud-terminal-controller     | all                 |        |\n| sealos-cloud-user-controller     | all                 |        |\n---\n\n\n| Frontendé¡¹ç›®åç§°                    | æ‰€å±ç‰ˆæœ¬ | è´Ÿè´£äºº |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-desktop-frontend | all |        |\n| sealos-cloud-adminer-frontend | å·²ç»åºŸå¼ƒ |        |\n| sealos-cloud-aiproxy-frontend | å…¬æœ‰äº‘ã€æš‚æœªåŠ å…¥ã€‘ |        |\n| sealos-cloud-applaunchpad-frontend | all |        |\n| sealos-cloud-cloudserver-frontend | å·²ç»åºŸå¼ƒã€åªæœ‰ä¸€ä¸ªå¯ç”¨åŒºä½¿ç”¨ã€‘ |        |\n| sealos-cloud-costcenter-frontend | all |        |\n| sealos-cloud-cronjob-frontend | all |        |\n| sealos-cloud-dbprovider-frontend | all |        |\n| sealos-cloud-devbox-frontend | å•†ä¸š&&å…¬æœ‰äº‘ |        |\n| sealos-cloud-invite-frontend | å…¬æœ‰äº‘ã€é‚€è¯·æ³¨å†Œï¼Œæš‚æœªåŠ å…¥ã€‘ | |\n| sealos-cloud-kubepanel-frontend | å•†ä¸š&&å…¬æœ‰äº‘ | |\n| sealos-cloud-license-frontend | å•†ä¸šç‰ˆ | |\n| sealos-cloud-objectstorage-frontend | å•†ä¸š&&å…¬æœ‰äº‘ | |\n| sealos-cloud-switch-region-frontend | å…¬æœ‰äº‘ã€regionåˆ‡æ¢ï¼Œæš‚æœªåŠ å…¥ã€‘ | |\n| sealos-cloud-template-frontend | all | |\n| sealos-cloud-terminal-frontend | all | |\n| sealos-cloud-workorder-frontend | å…¬æœ‰äº‘ã€å·¥å•ï¼Œæš‚æœªåŠ å…¥ã€‘ | |\n\n\n---\n\n| Serviceé¡¹ç›®åç§°                    | æ‰€å±ç‰ˆæœ¬ | è´Ÿè´£äºº |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-account-service       | all |        |\n| sealos-cloud-database-service       | all |        |\n| sealos-cloud-devbox-service       | å•†ä¸š&&å…¬æœ‰äº‘ |        |\n| sealos-cloud-exceptionmonitor-service       | å…¬æœ‰äº‘ã€æ•°æ®åº“å‘Šè­¦ï¼Œæš‚æœªåŠ å…¥ã€‘ |        |\n| sealos-cloud-hubble-service       | å…¬æœ‰äº‘ã€æµé‡ç›¸å…³ã€‘ |        |\n| sealos-cloud-launchpad-service       | all |        |\n| sealos-cloud-license-service       | å…¬æœ‰äº‘ã€https://license.sealos.ioã€‘ |        |\n| sealos-cloud-minio-service       | å•†ä¸š&&å…¬æœ‰äº‘ |        |\n| sealos-cloud-pay-service       | æœªä¸Šçº¿ |        |\n| sealos-cloud-vlogs-service       | å•†ä¸š&&å…¬æœ‰äº‘ |        |\n\n---\n\n| Webhooké¡¹ç›®åç§°                    | æ‰€å±ç‰ˆæœ¬ | è´Ÿè´£äºº |\n| ------------------------------------- | -------- | ------ |\n| sealos-cloud-admission-webhook       | å…¬æœ‰äº‘ã€åŸŸååŠ«æŒã€‘ |  yy      |\n\n\n\n---\n| å…¶ä»–é¡¹ç›®åç§°                    | åŠŸèƒ½è¯´æ˜ | è´Ÿè´£äºº |\n| ------------------------------------- | -------- | ------ |\n| region-exporterã€user-exporterã€‘ | å¯ç”¨åŒºç›‘æ§ | ä½³è¾‰ |\n| region-exporterã€lvm-restore-exporterã€‘ | lvmç›‘æ§ + åå°ç›‘æ§pvcè·Ÿ lvm lv ä¸ä¸€è‡´çš„è¿›è¡Œæ¢å¤ | ä½³è¾‰ |\n| region-exporterã€bi-infoã€‘ | è¿è¥å¯¼å‡ºæ•°æ®åº“ | ä½³è¾‰ |\n| https://github.com/bearslyricattack/CompliK | æŒ–çŸ¿æ£€æµ‹ | é¹å®‡ |\n| [chart2db](https://fael3z0zfze.feishu.cn/docx/O7H3dUE7EoFa3Fxn2DrcjgpGnig)  |  | é‡‘è™ |\n| https://github.com/zijiren233/image-monitor | [é•œåƒç›‘æ§](https://github.com/user-attachments/files/22375128/Pod.-1758077188997.json) | ä»ªè±ª |\n| https://github.com/zijiren233/node-zombie-detector | èŠ‚ç‚¹ç›‘æ§å‘Šè­¦ | ä»ªè±ª |\n| cleanup-deleting-clusters | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| cleanup-failed-opsrequests | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| cleanup-succeeded-opsrequests | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| clear-mongo-logs | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| delete-pg-logs | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| set-pg-failsafe-mode | æ•°æ®åº“ä¿®å¤ | é‡‘è™ |\n| [sealos-admin](https://github.com/labring/sealos-admin)| ç®¡ç†å‘˜æ¨¡å— | å­æ¯… |\n| https://github.com/zijiren233/higress/tree/limit-whitelist| higress æ’ä»¶| ä»ªè±ª |\n| [deschduler](https://fael3z0zfze.feishu.cn/wiki/RnIXw0V6Ri0EDYkv4NLcooBAnYf)|deschdulerç»„ä»¶ï¼Œè§†æƒ…å†µå®š|ä»ªè±ª |\n| https://github.com/dinoallo/sealos-nm-agent/ |æµé‡æ¨¡å—|æ½˜äº‘ |\n| https://github.com/labring/aiproxy/ |aiproxy|ä»ªè±ª |\n\n\n\n",
        "closed": false,
        "closedAt": null,
        "number": 27,
        "state": "OPEN",
        "title": "sealosé¡¹ç›®ç»“æ„ç»Ÿè®¡",
        "url": "https://github.com/labring/sealos-pro/issues/27",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "![Image](https://github.com/user-attachments/assets/4ac1ca3e-810f-4fde-aa14-b1a21dd74e26)\n\n\n![Image](https://github.com/user-attachments/assets/7b23a9a5-b4ab-4bff-8d9c-cc33a4ae233a)",
        "closed": true,
        "closedAt": "2025-09-19T13:06:49Z",
        "number": 26,
        "state": "CLOSED",
        "title": "openebs é»˜è®¤çš„ä¼˜å…ˆçº§å¤ªä½äº†",
        "url": "https://github.com/labring/sealos-pro/issues/26",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: region-exporter\n  namespace: account-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: region-exporter\n  template:\n    metadata:\n      labels:\n        app: region-exporter\n    spec:\n      serviceAccountName: account-controller-manager\n      containers:\n        - name: region-exporter\n          image: [registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531](http://registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531)\n          imagePullPolicy: Always\n          ports:\n            - name: metrics\n              containerPort: 8000\n          envFrom:\n            - configMapRef:\n                name: account-manager-env\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: region-exporter\n  namespace: account-system\n  labels:\n    app: region-exporter\nspec:\n  selector:\n    app: region-exporter\n  ports:\n    - protocol: TCP\n      port: 8000\n      targetPort: metrics\n      name: metrics\n---\napiVersion: [networking.k8s.io/v1](http://networking.k8s.io/v1)\nkind: Ingress\nmetadata:\n  name: region-exporter\n  namespace: account-system\n  annotations:\n    [kubernetes.io/ingress.class](http://kubernetes.io/ingress.class): nginx\n    [nginx.ingress.kubernetes.io/proxy-body-size](http://nginx.ingress.kubernetes.io/proxy-body-size): 32m\n    [nginx.ingress.kubernetes.io/server-snippet](http://nginx.ingress.kubernetes.io/server-snippet): |\n      client_header_buffer_size 64k;\n      large_client_header_buffers 4 128k;\n    [nginx.ingress.kubernetes.io/ssl-redirect](http://nginx.ingress.kubernetes.io/ssl-redirect): 'false'\n    [nginx.ingress.kubernetes.io/backend-protocol](http://nginx.ingress.kubernetes.io/backend-protocol): HTTP\n    [nginx.ingress.kubernetes.io/rewrite-target](http://nginx.ingress.kubernetes.io/rewrite-target): /$2\n    [nginx.ingress.kubernetes.io/client-body-buffer-size](http://nginx.ingress.kubernetes.io/client-body-buffer-size): 64k\n    [nginx.ingress.kubernetes.io/proxy-buffer-size](http://nginx.ingress.kubernetes.io/proxy-buffer-size): 64k\n    [nginx.ingress.kubernetes.io/proxy-send-timeout](http://nginx.ingress.kubernetes.io/proxy-send-timeout): '300'\n    [nginx.ingress.kubernetes.io/proxy-read-timeout](http://nginx.ingress.kubernetes.io/proxy-read-timeout): '300'\n    [nginx.ingress.kubernetes.io/configuration-snippet](http://nginx.ingress.kubernetes.io/configuration-snippet): |\n      if ($request_uri ~* \\.(js|css|gif|jpe?g|png)) {\n        expires 30d;\n        add_header Cache-Control \"public\";\n      }\nspec:\n  rules:\n    - host: xxx\n      http:\n        paths:\n          - pathType: Prefix\n            path: /()(.*)\n            backend:\n              service:\n                name: region-exporter\n                port:\n                  number: 8000\n  tls:\n    - hosts:\n        - xxx\n      secretName: wildcard-cert\n---\n#apiVersion: [monitoring.coreos.com/v1](http://monitoring.coreos.com/v1)\n#kind: ServiceMonitor\n#metadata:\n#  name: region-exporter\n#  namespace: account-system\n#  labels:\n#    release: prometheus\n#spec:\n#  selector:\n#    matchLabels:\n#      app: crd-exporter\n#  endpoints:\n#    - port: metrics\n#      interval: 300s\n```",
        "closed": false,
        "closedAt": null,
        "number": 25,
        "state": "OPEN",
        "title": "å…¬æœ‰äº‘-ç›‘æ§",
        "url": "https://github.com/labring/sealos-pro/issues/25",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\napiVersion: v1\nitems:\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"extensions.higress.io/v1alpha1\",\"kind\":\"WasmPlugin\",\"metadata\":{\"annotations\":{},\"name\":\"block-vite-fs\",\"namespace\":\"higress-system\"},\"spec\":{\"defaultConfig\":{\"block_regexp_urls\":[\"^.*/@fs/.*\\\\?(?:.*\\u0026)?(?:raw|import\\u0026raw)\\\\?\\\\?.*$\"]},\"matchRules\":null,\"url\":\"oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0\"}}\n    creationTimestamp: \"2025-06-16T07:09:07Z\"\n    generation: 1\n    name: block-vite-fs\n    namespace: higress-system\n    resourceVersion: \"3870988733\"\n    uid: c2ef1fe5-0c66-47fe-bcac-1897e8c7fb76\n  spec:\n    defaultConfig:\n      block_regexp_urls:\n      - ^.*/@fs/.*\\?(?:.*&)?(?:raw|import&raw)\\?\\?.*$\n    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    creationTimestamp: \"2025-01-23T04:27:44Z\"\n    generation: 3\n    name: ip-restriction-aiproxy\n    namespace: higress-system\n    resourceVersion: \"1888196182\"\n    uid: 3edd6726-b595-452d-bb81-79676a78881c\n  spec:\n    matchRules:\n    - config:\n        deny:\n        - 52.175.17.214\n        - 209.17.118.106\n        - 52.175.21.231\n        ip_header_name: x-forwarded-for\n        ip_source_type: header\n      ingress:\n      - ns-bjwgfnik/network-eeodjtvqdkbd\n    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/ip-restriction:1.0.0\n- apiVersion: extensions.higress.io/v1alpha1\n  kind: WasmPlugin\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"extensions.higress.io/v1alpha1\",\"kind\":\"WasmPlugin\",\"metadata\":{\"annotations\":{},\"name\":\"limit\",\"namespace\":\"higress-system\"},\"spec\":{\"matchRules\":[{\"config\":{\"redis\":{\"password\":\"8bss65fz\",\"service_name\":\"redis.dns\",\"service_port\":6379,\"username\":\"default\"},\"rejected_code\":429,\"rejected_msg\":\"Too many requests, please use a custom domain name to remove the restriction\",\"rule_items\":[{\"limit_by_per_header\":\":authority\",\"limit_keys\":[{\"key\":\"regexp:^static-host.+\",\"query_per_minute\":10}]}],\"rule_name\":\"default_rule\",\"show_limit_quota_header\":true},\"domain\":[\"*.sealoshzh.site\"]}],\"priority\":600,\"url\":\"oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0\"}}\n    creationTimestamp: \"2025-03-21T13:23:25Z\"\n    generation: 3\n    name: limit\n    namespace: higress-system\n    resourceVersion: \"2650231697\"\n    uid: 4bee0edd-b021-4a7d-8fa9-f32436d9f0f1\n  spec:\n    matchRules:\n    - config:\n        redis:\n          password: *****\n          service_name: redis.dns\n          service_port: 6379\n          username: default\n        rejected_code: 429\n        rejected_msg: Too many requests, please use a custom domain name to remove\n          the restriction\n        rule_items:\n        - limit_by_per_header: :authority\n          limit_keys:\n          - key: regexp:^static-host.+\n            query_per_minute: 10\n        rule_name: default_rule\n        show_limit_quota_header: true\n      domain:\n      - '*.sealoshzh.site'\n    priority: 600\n    url: oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0\nkind: List\nmetadata:\n  resourceVersion: \"\"\n\n```",
        "closed": false,
        "closedAt": null,
        "number": 24,
        "state": "OPEN",
        "title": "å…¬æœ‰äº‘- wasmPlugins",
        "url": "https://github.com/labring/sealos-pro/issues/24",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://github.com/bearslyricattack/CompliK æŒ–å‘",
        "closed": false,
        "closedAt": null,
        "number": 23,
        "state": "OPEN",
        "title": "åˆè§„æ£€æµ‹",
        "url": "https://github.com/labring/sealos-pro/issues/23",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring/sealos/issues/5978",
        "closed": false,
        "closedAt": null,
        "number": 22,
        "state": "OPEN",
        "title": "ä¼˜åŒ–é¡¹ç›®  image-shim åŠ¨æ€åŠ è½½",
        "url": "https://github.com/labring/sealos-pro/issues/22",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. é™¤äº†é»˜è®¤çš„ç›‘æ§é¢æ¿è¿˜éœ€è¦å“ªäº›ï¼Ÿ\n2. å¯¹äºcloud æ¥è®²éœ€è¦é‚£äº›å‘Šè­¦é€šçŸ¥\n3. æœ‰æ²¡æœ‰ä¸€äº›äº‹ç›‘æ§å®šåˆ¶çš„é¡¹ç›®",
        "closed": true,
        "closedAt": "2025-09-19T13:34:42Z",
        "number": 21,
        "state": "CLOSED",
        "title": "å…¶ä»–ä»»åŠ¡ TOD å¾…åŠ",
        "url": "https://github.com/labring/sealos-pro/issues/21",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "```\n[20:23:11] [~/Workspaces/go/src/github.com/labring/sealos-pro/victoria-metrics-k8s-stack/v1.124.0] git(main) ğŸ”¥ â±â±â± sreg save --registry-dir=/tmp/registry .  --debug\nError: failed to get images in charts: template: grafana-operator/templates/rbac.yaml:14:22: executing \"grafana-operator/templates/rbac.yaml\" at <$watchNamespaces>: invalid value; expected string\nUsage:\n  sreg save [flags]\n\nExamples:\n\nsreg save --registry-dir=/tmp/registry .\nsreg save --registry-dir=/tmp/registry --images=containers-storage:docker.io/labring/coredns:v0.0.1\nsreg save --registry-dir=/tmp/registry --images=docker-daemon:docker.io/library/nginx:latest\nsreg save --registry-dir=/tmp/registry --tars=docker-archive:/root/config_main.tar@library/config_main\nsreg save --registry-dir=/tmp/registry --tars=oci-archive:/root/config_main.tar@library/config_main\nsreg save --registry-dir=/tmp/registry --images=docker.io/library/busybox:latest\n\nFlags:\n      --all                   Pull all images if SOURCE-IMAGE is a list\n      --arch string           pull images arch (default \"arm64\")\n  -h, --help                  help for save\n      --images strings        images list\n      --max-pull-procs int    maximum number of goroutines for pulling (default 5)\n      --registry-dir string   registry data dir path (default \"registry\")\n      --tars strings          tar list, eg: --tars=docker-archive:/root/config_main.tar@library/config_main\n\nGlobal Flags:\n      --debug   enable debug logger\n```",
        "closed": false,
        "closedAt": null,
        "number": 20,
        "state": "OPEN",
        "title": "æ— æ³•æ¸²æŸ“grafana-operator çš„chart",
        "url": "https://github.com/labring/sealos-pro/issues/20",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "![Image](https://github.com/user-attachments/assets/52c0c6dc-2f71-4a99-b6c9-f7b08096293c)",
        "closed": false,
        "closedAt": null,
        "number": 19,
        "state": "OPEN",
        "title": "å¯¹è±¡å­˜å‚¨ä½¿ç”¨çš„serviceæ˜¯LB æ–¹å¼ï¼Œæœ‰æ³„éœ²é£é™©",
        "url": "https://github.com/labring/sealos-pro/issues/19",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. æ‰§è¡Œçš„æ—¶å€™éœ€è¦è·å–åœ°å€ ç„¶åç»™containerdæ·»åŠ é…ç½®\n2. éœ€è¦æµ‹è¯•",
        "closed": true,
        "closedAt": "2025-09-14T10:48:43Z",
        "number": 18,
        "state": "CLOSED",
        "title": "devboxå¦‚ä½•ä¿®æ”¹é•œåƒä»“åº“åœ°å€",
        "url": "https://github.com/labring/sealos-pro/issues/18",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "å¼€å¯cilium hubbleçš„tlsä»¥å¢å¼ºè®¿é—®çš„å®‰å…¨æ€§ã€‚\nhttps://fael3z0zfze.feishu.cn/wiki/GWCywoCFBiDH8qkPS7ZcTblCnHg",
        "closed": true,
        "closedAt": "2025-09-12T04:03:31Z",
        "number": 17,
        "state": "CLOSED",
        "title": "cilium hubbleå®‰å…¨æ€§å¢å¼º",
        "url": "https://github.com/labring/sealos-pro/issues/17",
        "login": "bearslyricattack",
        "is_bot": false
      },
      {
        "body": "# ä¿®æ”¹ sync-images é€»è¾‘\n\n1. æ ¹æ®ç‰ˆæœ¬ç±»å‹åŒºåˆ†é•œåƒåå­—\n2. ä½¿ç”¨æ–°åå­— æ¯”å¦‚ sealos-pro-all\n3. tagéœ€è¦æ”¯æŒ ç‰ˆæœ¬ç±»å‹ ä»¥åŠcloudçš„ç‰ˆæœ¬\n4. éœ€è¦ä¿å­˜sealos sealctl æ–‡ä»¶ ä»sealos release ä¸‹è½½ã€‚ åœ¨ç¼“å­˜è·¯å¾„ æ–°å»ºbinç›®å½•å¹¶ä¿å­˜",
        "closed": true,
        "closedAt": "2025-09-11T14:42:17Z",
        "number": 15,
        "state": "CLOSED",
        "title": "ä¿®æ”¹é•œåƒåå­—",
        "url": "https://github.com/labring/sealos-pro/issues/15",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. ç‰ˆæœ¬åˆ†å•†ä¸šç‰ˆå’Œå…¬æœ‰äº‘ç‰ˆæœ¬\n2. å¦‚æœæ˜¯å…¬æœ‰äº‘ç‰ˆæœ¬åç¼€æœ‰ä¸ª-cloud æ¯”å¦‚ç‰ˆæœ¬ v2.1.3æ˜¯å•†ä¸šç‰ˆ v2.1.3-cloudæ˜¯å…¬æœ‰äº‘ç‰ˆæœ¬ï¼Œå¦‚æœä¸å­˜åœ¨-cloudç‰ˆæœ¬åˆ™æ˜¯ä½¿ç”¨å•†ä¸šç‰ˆä½œä¸ºcloudçš„ç‰ˆæœ¬",
        "closed": true,
        "closedAt": "2025-09-11T14:14:55Z",
        "number": 13,
        "state": "CLOSED",
        "title": "sync-images.sh å®ç°ä¸€ä¸‹featureçš„åŒºåˆ†",
        "url": "https://github.com/labring/sealos-pro/issues/13",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": true,
        "closedAt": "2025-09-11T13:57:29Z",
        "number": 11,
        "state": "CLOSED",
        "title": "sync-images åœ¨åŸæ¥çš„é€»è¾‘ä¸Šæ·»åŠ sealos-cloud é•œåƒï¼Œéœ€è¦æ–°å¢ç‰ˆæœ¬å…¥å‚",
        "url": "https://github.com/labring/sealos-pro/issues/11",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).",
        "closed": true,
        "closedAt": "2025-09-11T13:00:09Z",
        "number": 9,
        "state": "CLOSED",
        "title": "âœ¨ Set up Copilot instructions",
        "url": "https://github.com/labring/sealos-pro/issues/9",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1.  sync-images.sh è„šæœ¬ä»ç¯å¢ƒå˜é‡æ–¹å¼æ¢æˆ å‚æ•°çš„æ–¹å¼æ‰§è¡Œ\n2. ç¼“å­˜é•œåƒä¸è¦åœ¨å½“å‰ç›®å½•æ‰§è¡Œï¼Œå¯ä»¥æ¢ä¸ªç›®å½•æ‰§è¡Œ",
        "closed": true,
        "closedAt": "2025-09-11T12:36:03Z",
        "number": 7,
        "state": "CLOSED",
        "title": "è„šæœ¬é‡æ„",
        "url": "https://github.com/labring/sealos-pro/issues/7",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## æˆ‘éœ€è¦å†™ä¸€ä¸ªè„šæœ¬æ”¾åˆ°æ ¹ç›®å½•,å®ƒçš„åå­—å« sync-images.sh\n\n1. å®ƒæ”¯æŒç›®å½•æ‰«æï¼Œå¦‚æœä¸å­˜åœ¨package.sh åˆ™éœ€è¦å½“å‰çš„é•œåƒä»¥åŠç‰ˆæœ¬\n2. ä»–çš„é•œåƒè§„åˆ™æ˜¯ ghcr.io/labring/sealos-pro:${NAME}-${VERSION}\n3. éœ€è¦æ ¹æ®æ¶æ„åˆ†åˆ«ä¿å­˜\n4. éœ€è¦å•ç‹¬å†™ä¸ªKubefile åªç¼“å­˜è¿™äº›é•œåƒåˆ°registryï¼Œæ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ª images/shimç›®å½•é‡Œé¢æŠŠæˆ‘éœ€è¦çš„é•œåƒåå­—å†™åˆ°ä¸€ä¸ªtxtä¸­å»\n5. åœ¨è¿™ä¸ªç›®å½•æ‰§è¡Œsealos build å³å¯ç¼“å­˜é•œåƒ\n6. æ‰€æœ‰çš„é•œåƒéƒ½æ˜¯ç§æœ‰é•œåƒ éœ€è¦æå‰ç™»å½• ï¼Œè¿™ä¸ªé•œåƒéœ€è¦åŒ…æ‹¬sealos login",
        "closed": true,
        "closedAt": "2025-09-11T12:20:04Z",
        "number": 5,
        "state": "CLOSED",
        "title": "sync image scripts",
        "url": "https://github.com/labring/sealos-pro/issues/5",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "https://fael3z0zfze.feishu.cn/wiki/PbFIwUs3OizAg5kR78mcK4QznQd\n\n```\nsealos run docker.io/labring/cert-manager:v1.17.1\n```",
        "closed": true,
        "closedAt": "2025-09-14T11:32:11Z",
        "number": 4,
        "state": "CLOSED",
        "title": "20250804 Cloudflare DNS API å‡çº§å¯¼è‡´è¯ä¹¦æ— æ³•ç­¾å‘",
        "url": "https://github.com/labring/sealos-pro/issues/4",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. https://fael3z0zfze.feishu.cn/wiki/APYLwFZOhiQN0Xk4NSpcB7n4nxe\n\n## è§£æ³•\n\nhttps://github.com/cert-manager/cert-manager/pull/1671 --max-concurrent-challenges",
        "closed": true,
        "closedAt": "2025-09-15T04:05:22Z",
        "number": 3,
        "state": "CLOSED",
        "title": "20250729 cert-managerç”³è¯·è¯ä¹¦å¡ä½",
        "url": "https://github.com/labring/sealos-pro/issues/3",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "1. https://fael3z0zfze.feishu.cn/wiki/CmVtwPMpDiJHlPkMRp5c1Ejkn9f",
        "closed": false,
        "closedAt": null,
        "number": 2,
        "state": "OPEN",
        "title": "20250821 èŠ‚ç‚¹ç£ç›˜å‹åŠ›è¿‡å¤§å¯¼è‡´å…³é”®ç»„ä»¶è¢«é©±é™¤",
        "url": "https://github.com/labring/sealos-pro/issues/2",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "",
        "closed": false,
        "closedAt": null,
        "number": 1,
        "state": "OPEN",
        "title": "å‚æ•°è°ƒä¼˜",
        "url": "https://github.com/labring/sealos-pro/issues/1",
        "login": "cuisongliu",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring/sreg",
    "issues": []
  },
  {
    "repo": "labring-actions/cluster-image",
    "issues": [
      {
        "body": "è¯·é—®èƒ½å¸®å¿™ç»´æŠ¤ç¼–è¯‘åº”ç”¨ä»“åº“å—ï¼Ÿç›®å‰ä»…æœ‰å‡ ä¸ªå…è®¸acceptedæ ‡ç­¾å’Œç¼–è¯‘ï¼Œå¸®å¿™æŠŠå¸¸ç”¨çš„longhorn,openebsï¼Œmetallbã€calicoã€higressã€apisixã€kubeblocksæ‰“ä¸€ä¸‹acceptedæ ‡ç­¾ï¼Œè°¢è°¢ã€‚",
        "closed": false,
        "closedAt": null,
        "number": 1232,
        "state": "OPEN",
        "title": "è¯·ç»§ç»­ç»´æŠ¤è¿™ä¸ªä»“åº“çš„acceptedçš„æ ‡ç­¾ï¼Œè°¢è°¢",
        "url": "https://github.com/labring-actions/cluster-image/issues/1232",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1231,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘minio",
        "url": "https://github.com/labring-actions/cluster-image/issues/1231",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps rainbond v6.3.1",
        "closed": false,
        "closedAt": null,
        "number": 1230,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘rainbond",
        "url": "https://github.com/labring-actions/cluster-image/issues/1230",
        "login": "ilyndon",
        "is_bot": false
      },
      {
        "body": "```\n/imagebuild_apps cilium  v1.17.4\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1229,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘cilium v1.17.4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1229",
        "login": "kaneleven",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps nerdctl v2.1.2\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps nerdctl v2.1.2\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1228,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘nerdctl",
        "url": "https://github.com/labring-actions/cluster-image/issues/1228",
        "login": "LinkMaq",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": false,
        "closedAt": null,
        "number": 1227,
        "state": "OPEN",
        "title": "ã€DaylyReportã€‘ Auto build for laf 2025/6/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1227",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "Usage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps longhorn v1.8.0",
        "closed": false,
        "closedAt": null,
        "number": 1226,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘longhorn",
        "url": "https://github.com/labring-actions/cluster-image/issues/1226",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "Usage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2",
        "closed": true,
        "closedAt": "2025-06-09T01:30:36Z",
        "number": 1225,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘longhorn",
        "url": "https://github.com/labring-actions/cluster-image/issues/1225",
        "login": "macaty",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps helm v3.8.2",
        "closed": true,
        "closedAt": "2025-05-28T07:59:00Z",
        "number": 1224,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1224",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps helm v3.8.2\n",
        "closed": true,
        "closedAt": "2025-05-29T08:47:44Z",
        "number": 1221,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1221",
        "login": "dinoallo",
        "is_bot": false
      },
      {
        "body": "```\nUsage:\n   /imagebuild_apps appName appVersion\nAvailable Args:\n   appName:  current app dir\n   appVersion: current app version\n\nExample:\n   /imagebuild_apps helm v3.8.2\n```\n/imagebuild_apps elasticsearch v8.16.2",
        "closed": false,
        "closedAt": null,
        "number": 1216,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘elasticsearch",
        "url": "https://github.com/labring-actions/cluster-image/issues/1216",
        "login": "lingdie",
        "is_bot": false
      },
      {
        "body": "1",
        "closed": true,
        "closedAt": "2025-04-19T18:04:18Z",
        "number": 1214,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1214",
        "login": "kjagsdq",
        "is_bot": false
      },
      {
        "body": "/imagebuild_dockerimages abc",
        "closed": true,
        "closedAt": "2025-04-19T18:04:24Z",
        "number": 1212,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1212",
        "login": "kjagsdq",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": false,
        "closedAt": null,
        "number": 1209,
        "state": "OPEN",
        "title": "[DaylyReport] Auto build for sealos 2025/4/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1209",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-04-07T03:42:26Z",
        "number": 1208,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/4/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1208",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-04-02T17:57:28Z",
        "number": 1207,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1207",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-31T02:30:03Z",
        "number": 1206,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1206",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-28T03:52:26Z",
        "number": 1205,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1205",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-27T06:31:29Z",
        "number": 1204,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1204",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-26T09:44:54Z",
        "number": 1203,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1203",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-25T07:39:12Z",
        "number": 1202,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1202",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-24T08:58:37Z",
        "number": 1201,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1201",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-21T12:18:04Z",
        "number": 1199,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1199",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:52Z",
        "number": 1198,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1198",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:53Z",
        "number": 1197,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1197",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-07T04:35:53Z",
        "number": 1196,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1196",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:37Z",
        "number": 1195,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1195",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:37Z",
        "number": 1194,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1194",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-06T04:01:38Z",
        "number": 1193,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1193",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-05T07:03:07Z",
        "number": 1192,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1192",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-04T02:41:06Z",
        "number": 1191,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1191",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:49Z",
        "number": 1190,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1190",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:49Z",
        "number": 1189,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1189",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-03T04:18:50Z",
        "number": 1188,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/3/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1188",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-03-02T11:40:33Z",
        "number": 1187,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1187",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-28T04:52:39Z",
        "number": 1186,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1186",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-27T04:11:11Z",
        "number": 1185,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1185",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-26T02:50:12Z",
        "number": 1184,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1184",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-26T02:50:13Z",
        "number": 1183,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1183",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-25T07:24:44Z",
        "number": 1182,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1182",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-25T07:24:45Z",
        "number": 1181,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1181",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-24T10:47:03Z",
        "number": 1180,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1180",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-21T03:22:20Z",
        "number": 1179,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1179",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-20T03:00:38Z",
        "number": 1178,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1178",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-19T08:36:24Z",
        "number": 1176,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1176",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-18T04:00:40Z",
        "number": 1175,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1175",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-14T08:50:28Z",
        "number": 1174,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1174",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-13T06:20:21Z",
        "number": 1173,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1173",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-12T06:20:13Z",
        "number": 1172,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1172",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-11T07:02:02Z",
        "number": 1171,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1171",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-10T08:41:25Z",
        "number": 1170,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1170",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-07T06:25:40Z",
        "number": 1169,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/2/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1169",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-02-06T09:03:02Z",
        "number": 1168,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1168",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-27T04:27:16Z",
        "number": 1167,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1167",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-21T13:35:09Z",
        "number": 1166,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1166",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-20T03:41:03Z",
        "number": 1165,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1165",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-13T07:39:56Z",
        "number": 1164,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1164",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-13T07:39:57Z",
        "number": 1163,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1163",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps kubesphere v3.3.2\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": true,
        "closedAt": "2025-01-08T10:37:50Z",
        "number": 1162,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘kubesphere",
        "url": "https://github.com/labring-actions/cluster-image/issues/1162",
        "login": "relleo",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-10T07:47:15Z",
        "number": 1161,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1161",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-08T06:11:07Z",
        "number": 1160,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1160",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-07T07:38:54Z",
        "number": 1159,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1159",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-07T07:38:55Z",
        "number": 1158,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1158",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-06T06:37:39Z",
        "number": 1157,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1157",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-06T06:37:39Z",
        "number": 1156,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2025/1/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1156",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2025-01-03T03:54:46Z",
        "number": 1155,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1155",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-31T06:33:59Z",
        "number": 1154,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1154",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-30T03:36:43Z",
        "number": 1153,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1153",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-27T08:55:47Z",
        "number": 1152,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1152",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-27T08:55:47Z",
        "number": 1151,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1151",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-26T05:03:46Z",
        "number": 1150,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1150",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-25T06:35:16Z",
        "number": 1149,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1149",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-25T06:35:17Z",
        "number": 1148,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1148",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-24T06:14:14Z",
        "number": 1147,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1147",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-20T02:32:34Z",
        "number": 1146,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1146",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-19T08:46:51Z",
        "number": 1145,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1145",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-18T07:30:21Z",
        "number": 1144,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1144",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1143,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘oceanbase-operator",
        "url": "https://github.com/labring-actions/cluster-image/issues/1143",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-16T03:09:09Z",
        "number": 1142,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1142",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-16T03:09:09Z",
        "number": 1141,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1141",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-14T04:17:37Z",
        "number": 1140,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1140",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-13T06:37:41Z",
        "number": 1139,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1139",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-12T03:28:00Z",
        "number": 1138,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1138",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-11T02:56:40Z",
        "number": 1137,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1137",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-10T07:47:13Z",
        "number": 1136,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1136",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-09T03:47:49Z",
        "number": 1135,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1135",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-06T03:42:48Z",
        "number": 1134,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1134",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-04T04:24:33Z",
        "number": 1133,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1133",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-03T02:36:14Z",
        "number": 1132,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/12/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1132",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Description\n\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\n\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n\n### What To Do?\n\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n\n**For further information, suggestions, and guidance:**\n- ğŸ“– [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- ğŸ’¬ Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\n\nWe sincerely apologize for any inconvenience this may cause.\n\n**Thank you for your cooperation and understanding!** :pray:\n",
        "closed": false,
        "closedAt": null,
        "number": 1131,
        "state": "OPEN",
        "title": ":warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy",
        "url": "https://github.com/labring-actions/cluster-image/issues/1131",
        "login": "camilamacedo86",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-12-02T03:22:50Z",
        "number": 1130,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1130",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-29T04:11:24Z",
        "number": 1129,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1129",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-28T02:19:54Z",
        "number": 1128,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1128",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-28T02:19:54Z",
        "number": 1127,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1127",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-27T03:15:07Z",
        "number": 1126,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1126",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-26T09:21:09Z",
        "number": 1125,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1125",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-25T03:28:16Z",
        "number": 1124,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1124",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-25T03:28:17Z",
        "number": 1123,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1123",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-23T09:23:46Z",
        "number": 1122,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1122",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-22T03:06:48Z",
        "number": 1121,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1121",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-21T08:18:18Z",
        "number": 1120,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1120",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-20T07:30:09Z",
        "number": 1119,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1119",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-20T07:30:09Z",
        "number": 1118,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1118",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-19T06:46:01Z",
        "number": 1117,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1117",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-18T02:35:04Z",
        "number": 1115,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1115",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-15T03:55:10Z",
        "number": 1114,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1114",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-13T10:15:39Z",
        "number": 1113,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1113",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:48Z",
        "number": 1112,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1112",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:48Z",
        "number": 1111,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1111",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-12T08:09:49Z",
        "number": 1110,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1110",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-11T08:14:56Z",
        "number": 1109,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1109",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-08T02:50:10Z",
        "number": 1108,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1108",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-06T07:23:51Z",
        "number": 1107,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1107",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:19Z",
        "number": 1106,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1106",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:20Z",
        "number": 1105,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1105",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-05T08:54:20Z",
        "number": 1104,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/11/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1104",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-04T09:47:25Z",
        "number": 1103,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1103",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-11-04T09:47:25Z",
        "number": 1102,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1102",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-31T08:59:01Z",
        "number": 1101,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1101",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-29T02:41:56Z",
        "number": 1100,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1100",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-28T05:39:25Z",
        "number": 1099,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1099",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-25T12:42:19Z",
        "number": 1098,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1098",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-24T10:01:23Z",
        "number": 1097,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1097",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-23T08:07:42Z",
        "number": 1095,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1095",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-22T02:51:04Z",
        "number": 1094,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1094",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-21T03:06:47Z",
        "number": 1093,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/17",
        "url": "https://github.com/labring-actions/cluster-image/issues/1093",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-17T09:02:06Z",
        "number": 1092,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1092",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": true,
        "closedAt": "2025-06-10T06:19:44Z",
        "number": 1091,
        "state": "CLOSED",
        "title": "ã€DaylyReportã€‘ Auto build for laf 2024/10/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1091",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1090,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘automq-operator",
        "url": "https://github.com/labring-actions/cluster-image/issues/1090",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-16T10:22:57Z",
        "number": 1089,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1089",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-15T07:24:13Z",
        "number": 1088,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1088",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-12T02:57:27Z",
        "number": 1087,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1087",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-11T03:00:02Z",
        "number": 1086,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1086",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-10T06:26:55Z",
        "number": 1085,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1085",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-09T02:40:54Z",
        "number": 1084,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/10/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1084",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-10-08T02:49:22Z",
        "number": 1083,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1083",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-28T04:24:29Z",
        "number": 1082,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1082",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_dockerimages appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n   buildArgs:  build args, split by ','\r\n\r\nExample:\r\n   /imagebuild_dockerimages helm v3.8.2 Key1=Value1,Key2=Value2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1081,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘curl-kubectl",
        "url": "https://github.com/labring-actions/cluster-image/issues/1081",
        "login": "zijiren233",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-27T03:09:26Z",
        "number": 1080,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1080",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-26T04:08:28Z",
        "number": 1078,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/25",
        "url": "https://github.com/labring-actions/cluster-image/issues/1078",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-25T03:13:17Z",
        "number": 1077,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1077",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-24T08:18:08Z",
        "number": 1076,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1076",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps appName appVersion\r\nAvailable Args:\r\n   appName:  current app dir\r\n   appVersion: current app version\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": true,
        "closedAt": "2024-09-21T15:57:39Z",
        "number": 1075,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘metrics-server",
        "url": "https://github.com/labring-actions/cluster-image/issues/1075",
        "login": "cuisongliu",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-23T02:40:47Z",
        "number": 1074,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1074",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-20T04:48:10Z",
        "number": 1073,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/19",
        "url": "https://github.com/labring-actions/cluster-image/issues/1073",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "Longhorn v1.7.1 build failed.\r\n\r\n```\r\nError: failed to save images: save image longhornio/openshift-origin-oauth-proxy:4.15: choosing an image from manifest list docker://longhornio/openshift-origin-oauth-proxy:4.15: no image found in manifest list for architecture arm64, variant \"\", OS linux\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/0229b976-3edd-49c0-a4ca-87a5df3c16fb)\r\n\r\nThere have no arm image\r\n![image](https://github.com/user-attachments/assets/ef52bd9e-2396-4716-937b-d6e132331d76)\r\n\r\nshould support \r\n```\r\n/imagebuild_apps longhorn v1.7.1 amd64\r\n```\r\n",
        "closed": true,
        "closedAt": "2024-12-15T07:20:31Z",
        "number": 1072,
        "state": "CLOSED",
        "title": "Feature: support selecting an architecture type when building an image",
        "url": "https://github.com/labring-actions/cluster-image/issues/1072",
        "login": "weironz",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-19T03:58:03Z",
        "number": 1071,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1071",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-19T03:58:04Z",
        "number": 1070,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/18",
        "url": "https://github.com/labring-actions/cluster-image/issues/1070",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-18T07:58:37Z",
        "number": 1069,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1069",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-14T10:03:16Z",
        "number": 1068,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1068",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-13T03:27:12Z",
        "number": 1067,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1067",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-12T03:13:00Z",
        "number": 1066,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/11",
        "url": "https://github.com/labring-actions/cluster-image/issues/1066",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-11T06:55:18Z",
        "number": 1065,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/10",
        "url": "https://github.com/labring-actions/cluster-image/issues/1065",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-10T09:50:29Z",
        "number": 1064,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1064",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-09T04:11:12Z",
        "number": 1063,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1063",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-06T02:50:05Z",
        "number": 1062,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1062",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-06T02:50:05Z",
        "number": 1061,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1061",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-05T03:33:05Z",
        "number": 1060,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/4",
        "url": "https://github.com/labring-actions/cluster-image/issues/1060",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-04T04:02:32Z",
        "number": 1059,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/3",
        "url": "https://github.com/labring-actions/cluster-image/issues/1059",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-03T08:33:08Z",
        "number": 1058,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/9/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1058",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-02T05:33:49Z",
        "number": 1057,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1057",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-09-02T05:33:50Z",
        "number": 1056,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1056",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-31T07:06:14Z",
        "number": 1055,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1055",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-30T03:12:36Z",
        "number": 1054,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1054",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-30T03:12:37Z",
        "number": 1053,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1053",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-29T04:43:15Z",
        "number": 1051,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/28",
        "url": "https://github.com/labring-actions/cluster-image/issues/1051",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-28T03:03:04Z",
        "number": 1050,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/27",
        "url": "https://github.com/labring-actions/cluster-image/issues/1050",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-27T07:19:38Z",
        "number": 1049,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1049",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-27T07:19:39Z",
        "number": 1048,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1048",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-24T07:31:21Z",
        "number": 1047,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1047",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-23T03:41:01Z",
        "number": 1046,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1046",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-23T03:41:01Z",
        "number": 1045,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/22",
        "url": "https://github.com/labring-actions/cluster-image/issues/1045",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-22T03:12:46Z",
        "number": 1044,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/21",
        "url": "https://github.com/labring-actions/cluster-image/issues/1044",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-21T03:32:55Z",
        "number": 1043,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1043",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-21T03:32:55Z",
        "number": 1042,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/20",
        "url": "https://github.com/labring-actions/cluster-image/issues/1042",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-20T06:53:11Z",
        "number": 1041,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/16",
        "url": "https://github.com/labring-actions/cluster-image/issues/1041",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "\n## Intro\n\n- charts/* - Helm charts for laf cluster.\n- start.sh - Install laf cluster use helm manually. (should install openebs yourself first)\n- Kubefile - Build sealos cluster image. ex. `docker.io/lafyun/laf:latest`\n",
        "closed": true,
        "closedAt": "2024-10-16T08:24:21Z",
        "number": 1040,
        "state": "CLOSED",
        "title": "ã€DaylyReportã€‘ Auto build for laf 2024/8/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1040",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-16T06:07:44Z",
        "number": 1039,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/15",
        "url": "https://github.com/labring-actions/cluster-image/issues/1039",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-15T03:43:56Z",
        "number": 1038,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/14",
        "url": "https://github.com/labring-actions/cluster-image/issues/1038",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-14T03:36:31Z",
        "number": 1037,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/13",
        "url": "https://github.com/labring-actions/cluster-image/issues/1037",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-13T02:35:16Z",
        "number": 1036,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1036",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-13T02:35:16Z",
        "number": 1035,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/12",
        "url": "https://github.com/labring-actions/cluster-image/issues/1035",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "/imagebuild_apps cilium v1.15.7",
        "closed": true,
        "closedAt": "2024-09-21T15:57:51Z",
        "number": 1034,
        "state": "CLOSED",
        "title": "ã€Auto-buildã€‘helm",
        "url": "https://github.com/labring-actions/cluster-image/issues/1034",
        "login": "hanxianzhai",
        "is_bot": false
      },
      {
        "body": "```\r\nUsage:\r\n   /imagebuild_apps cilium v1.16.0\r\nAvailable Args:\r\n   appName:  cilium\r\n   appVersion: v.1.16.0\r\n\r\nExample:\r\n   /imagebuild_apps helm v3.8.2\r\n```\r\n",
        "closed": false,
        "closedAt": null,
        "number": 1033,
        "state": "OPEN",
        "title": "ã€Auto-buildã€‘cilium",
        "url": "https://github.com/labring-actions/cluster-image/issues/1033",
        "login": "hanxianzhai",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-12T08:51:57Z",
        "number": 1032,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/9",
        "url": "https://github.com/labring-actions/cluster-image/issues/1032",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-09T03:14:14Z",
        "number": 1031,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/8",
        "url": "https://github.com/labring-actions/cluster-image/issues/1031",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-08T03:56:22Z",
        "number": 1030,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/7",
        "url": "https://github.com/labring-actions/cluster-image/issues/1030",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-07T07:09:11Z",
        "number": 1029,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1029",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-07T07:09:11Z",
        "number": 1028,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/6",
        "url": "https://github.com/labring-actions/cluster-image/issues/1028",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-06T06:33:20Z",
        "number": 1027,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/5",
        "url": "https://github.com/labring-actions/cluster-image/issues/1027",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-05T07:09:16Z",
        "number": 1026,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/2",
        "url": "https://github.com/labring-actions/cluster-image/issues/1026",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-02T06:33:22Z",
        "number": 1025,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/8/1",
        "url": "https://github.com/labring-actions/cluster-image/issues/1025",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-01T08:49:35Z",
        "number": 1024,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1024",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-08-01T08:49:36Z",
        "number": 1023,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/31",
        "url": "https://github.com/labring-actions/cluster-image/issues/1023",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-31T11:29:39Z",
        "number": 1022,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/30",
        "url": "https://github.com/labring-actions/cluster-image/issues/1022",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-30T07:18:12Z",
        "number": 1021,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/29",
        "url": "https://github.com/labring-actions/cluster-image/issues/1021",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-29T03:54:33Z",
        "number": 1020,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/26",
        "url": "https://github.com/labring-actions/cluster-image/issues/1020",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-26T06:50:09Z",
        "number": 1019,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/24",
        "url": "https://github.com/labring-actions/cluster-image/issues/1019",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-24T02:44:36Z",
        "number": 1017,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1017",
        "login": "sealos-ci-robot",
        "is_bot": false
      },
      {
        "body": "## Sealos Image Update\n```\n/imagesync imageSourceName\n```\n",
        "closed": true,
        "closedAt": "2024-07-24T02:44:36Z",
        "number": 1018,
        "state": "CLOSED",
        "title": "[DaylyReport] Auto build for sealos 2024/7/23",
        "url": "https://github.com/labring-actions/cluster-image/issues/1018",
        "login": "sealos-ci-robot",
        "is_bot": false
      }
    ]
  },
  {
    "repo": "labring-actions/sync-aliyun",
    "issues": []
  },
  {
    "repo": "labring-actions/templates",
    "issues": [
      {
        "body": "åº”ç”¨çš„åç§°å’Œç®€ä»‹: odoo\nå®˜ç½‘ https://waline.js.org/\nGITHUBåœ°å€ï¼šhttps://github.com/walinejs/waline\næˆ‘çœ‹åº”ç”¨å•†åº—æœ‰äº†artalkï¼Œä½†Walineçš„ä½¿ç”¨äººæ•°ä¹Ÿå¾ˆå¤šã€‚æœŸå¾…å®˜æ–¹èƒ½ä¸Šæ¶Walineåº”ç”¨",
        "closed": false,
        "closedAt": null,
        "number": 479,
        "state": "OPEN",
        "title": "æœŸå¾…æ·»åŠ Walineåº”ç”¨",
        "url": "https://github.com/labring-actions/templates/issues/479",
        "login": "kok777",
        "is_bot": false
      },
      {
        "body": "ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤špræ²¡äººå®¡æ ¸å‘¢ï¼Ÿæœ€æ—©ç”šè‡³æœ‰å‰å¹´çš„ã€‚ä½†æˆ‘çœ‹ä»“åº“ä¼¼ä¹ä¸€ç›´éƒ½åœ¨æ›´æ–°ï¼Œå¦‚æœæœ‰äººåœ¨ç»´æŠ¤å°±è¯·å¤šæ¸…æ¸…prå§ï¼Œsealosç°åœ¨å°±é‚£ä¹ˆäº›ä¸ªæ¨¡æ¿ï¼Œä¸å¤Ÿç”¨",
        "closed": false,
        "closedAt": null,
        "number": 440,
        "state": "OPEN",
        "title": "ä»“åº“å¾ˆå¤špræ²¡äººå®¡æ ¸",
        "url": "https://github.com/labring-actions/templates/issues/440",
        "login": "ifzzh",
        "is_bot": false
      },
      {
        "body": "åº”ç”¨çš„åç§°å’Œç®€ä»‹: odoo\nåº”ç”¨çš„å®˜æ–¹ç½‘ç«™æˆ– GitHub ä»“åº“åœ°å€: https://github.com/odoo/odoo, https://github.com/odoo/odoo.git\nåº”ç”¨çš„éƒ¨ç½²è¦æ±‚ (å¦‚ï¼šæ‰€éœ€çš„ç¯å¢ƒå˜é‡ã€å­˜å‚¨éœ€æ±‚ç­‰): è¯´ä¸æ¸…æ¥š\nå…¶ä»–ç›¸å…³ä¿¡æ¯:  æ— ",
        "closed": false,
        "closedAt": null,
        "number": 415,
        "state": "OPEN",
        "title": "odoo_Create odoo in Sealos APP store",
        "url": "https://github.com/labring-actions/templates/issues/415",
        "login": "PythonPlayerJack",
        "is_bot": false
      },
      {
        "body": "ä¸€å¼€å§‹æ˜¯ç”¨çš„åº”ç”¨éƒ¨ç½²çš„difyå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œç„¶åæ˜¨å¤©æ™šä¸Šå°è¯•ç€æ›´æ–°åˆ°0.6.14çš„ç‰ˆæœ¬åï¼Œå‡ºç°äº†æŠ¥é”™ï¼Œç„¶åå°±é‡æ–°æ”¹ä¼šåˆ°0.6.11ï¼Œ\r\n![image](https://github.com/user-attachments/assets/4a3e6f46-c403-4327-97af-d16049688147)\r\n![0d44b7198cfc77c24e1320ae7cccad3](https://github.com/user-attachments/assets/c6beb754-4a8a-4141-b416-49a82fc3ce2f)\r\n\r\n\r\nåé¢çœ‹ä¸‹äº†difyçš„æ›´æ–°æ—¥å¿—ï¼Œ11ä¹‹åçš„dockeréƒ¨ç½²ä¸ä¸€æ ·äº†ï¼Œ å¤šäº†ä¸€ä¸ª.envæ–‡ä»¶ï¼Œæ‰€ä»¥åº”è¯¥ä¸èƒ½å‡çº§åˆ°.11ç‰ˆæœ¬åé¢ï¼Œä½†ç°åœ¨æˆ‘è¦å›æ»šåˆ°.11ä¹‹å‰çš„ç‰ˆæœ¬ä¹Ÿä¸è¡Œäº†ï¼Œä½†æ˜¯æˆ‘éƒ¨ç½²ä¸€ä¸ªæ–°çš„difyï¼Œå‘ç°ä¹Ÿè¿è¡Œä¸äº†ï¼Œä¸€ç›´æŠ¥è·¨åŸŸçš„é—®é¢˜",
        "closed": true,
        "closedAt": "2024-08-08T06:25:31Z",
        "number": 292,
        "state": "CLOSED",
        "title": "difyå¯åŠ¨ä¸äº†ï¼Œç°åœ¨è®¿é—®ä¸€ç›´æŠ¥è·¨åŸŸçš„é—®é¢˜ï¼Œåº”è¯¥apiçš„æœåŠ¡æ²¡æœ‰å¯åŠ¨",
        "url": "https://github.com/labring-actions/templates/issues/292",
        "login": "catdddmouse",
        "is_bot": false
      },
      {
        "body": "The TLS certificate does not work for the yaml config generated from the [current version of template](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/template.yaml)\r\n\r\nThe `tls.secretName` in it does not exist. According to the [Template Variables](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/example.md#built-in-system-variables), it should use **all upper case** letters `SEALOS_CERT_SECRET_NAME` instead of `SEALOS_Cert_Secret_Name`.\r\n\r\n\r\n\r\n",
        "closed": true,
        "closedAt": "2024-03-25T06:34:55Z",
        "number": 223,
        "state": "CLOSED",
        "title": "Wrong `tls.secretName` in Template Causes SSL Certificate Issue",
        "url": "https://github.com/labring-actions/templates/issues/223",
        "login": "tianshanghong",
        "is_bot": false
      },
      {
        "body": "The librechat template cannot be deployed in the beijing, guangzhou, and hangzhou availability zones of the sealos website. I checked the problem and it is that mongodb is always showing as creating. Is there a way to fix it? Thank you.\r\n\r\nthis screenshot is from https://gzg.sealos.run/\r\n\r\n![image](https://github.com/labring-actions/templates/assets/46660805/26fb20a2-81b2-4f77-9123-ebbe5f9d09c0)\r\n\r\n",
        "closed": true,
        "closedAt": "2024-03-25T06:37:55Z",
        "number": 205,
        "state": "CLOSED",
        "title": "LibreChat cannot be deployed in Guangzhou",
        "url": "https://github.com/labring-actions/templates/issues/205",
        "login": "BigTomM",
        "is_bot": false
      },
      {
        "body": "https://github.com/labring-actions/templates/blob/6b57ae8428450bfd158d0ec9c3d0ff767634c56b/template/docker-stacks.yaml#L21",
        "closed": true,
        "closedAt": "2023-10-16T08:14:28Z",
        "number": 86,
        "state": "CLOSED",
        "title": "typo error",
        "url": "https://github.com/labring-actions/templates/issues/86",
        "login": "lingdie",
        "is_bot": false
      }
    ]
  }
]