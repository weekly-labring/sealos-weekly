repo,issue_number,author,title,body,body_len,is_closed,closed_at,state,is_bot,exported_at
labring/sealos,4759,TW199501,新增繁體中文語言支援,"# Issue: 新增繁體中文語言支援

目前，應用程式不支援繁體中文，這限制了那些更習慣使用繁體中文的用戶的可用性。

## 如果你有解決方案，請描述一下
加入社區自願翻譯，提交到github

",106,true,2024-09-27 06:54:11,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-admin,31,cuisongliu,workflow 添加保存tar包的流程,"",0,true,2025-09-04 01:59:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,1,cuisongliu,参数调优,"",0,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,86,lingdie,typo error,"https://github.com/labring-actions/templates/blob/6b57ae8428450bfd158d0ec9c3d0ff767634c56b/template/docker-stacks.yaml#L21",122,true,2023-10-16 08:14:28,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4760,zijiren233,deploy/cloud/scripts/init.sh The wrong format of the variable causes the --env parameter parsing to fail.,"https://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L138
https://github.com/labring/sealos/blob/3859de964955dda9a350ddbc5cda7f9f48c2cc6a/deploy/cloud/scripts/init.sh#L147
这两处获取到的值外面会有一层双引号，导致后续使用变量的时候造成严重的错误",264,true,2024-05-29 07:01:03,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-admin,33,cuisongliu,ai: 拆分双架构,"1. 拆分双架构 amd64 使用runner是 buildjet-2vcpu-ubuntu-2204 而 arm64使用runner是 buildjet-4vcpu-ubuntu-2204-arm
2. 仅限容器镜像版本，集群镜像无需修改。",121,true,2025-09-04 03:30:56,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,2,cuisongliu,20250821 节点磁盘压力过大导致关键组件被驱除,1. https://fael3z0zfze.feishu.cn/wiki/CmVtwPMpDiJHlPkMRp5c1Ejkn9f,65,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,205,BigTomM,LibreChat cannot be deployed in Guangzhou,"The librechat template cannot be deployed in the beijing, guangzhou, and hangzhou availability zones of the sealos website. I checked the problem and it is that mongodb is always showing as creating. Is there a way to fix it? Thank you.

this screenshot is from https://gzg.sealos.run/

![image](https://github.com/labring-actions/templates/assets/46660805/26fb20a2-81b2-4f77-9123-ebbe5f9d09c0)

",402,true,2024-03-25 06:37:55,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4764,zijiren233,"cloud-port=8443时安装cloud会发生错误 `no endpoints available for service ""ingress-noinx-controller-admission""`","```bash
install.sh \
    --zh  \
    --cloud-version=v5.0.0-beta5 \
    --kubernetes-version=1.28.10 \
    --pod-cidr=100.64.0.0/10 \
    --service-cidr=192.168.0.0/16  \
    --cloud-domain=x.com  \
    --cloud-port=8443 \
    --cert-path=/root/x.com.crt  \
    --key-path=/root/x.com.key  \
    --ssh-password=x \
    --master-ips=10.0.0.50 \
    --image-registry=registry.cn-shanghai.aliyuncs.com
```

cloud-port=443时一切正常,cloud-port换成8443就错误了
<img width=""1111"" alt=""image"" src=""https://github.com/labring/sealos/assets/84728412/a4ecf39e-8fbd-46e0-83b7-6b855623289b"">

以下是发生错误后获取的数据：

```
kubectl get pods -A
NAMESPACE                   NAME                                                              READY   STATUS                            RESTARTS      AGE
cert-manager                cert-manager-6dc66985d4-s52b4                                     1/1     Running                           0             6m51s
cert-manager                cert-manager-cainjector-c7d4dbdd9-hhkwq                           1/1     Running                           0             6m51s
cert-manager                cert-manager-webhook-847d7676c9-6c8pc                             1/1     Running                           0             6m51s
cockroach-operator-system   cockroach-operator-manager-6fd5c68d58-z5ckb                       1/1     Running                           0             6m20s
ingress-nginx               ingress-nginx-controller-j9mk8                                    0/1     Running                           3 (18s ago)   3m40s
kb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-fzc98                     1/1     Running                           0             3m31s
kb-system                   kubeblocks-85ddddddd4-bfcjk                                       1/1     Running                           0             5m28s
kb-system                   kubeblocks-dataprotection-7f9c76fb8f-b5xzt                        1/1     Running                           0             5m28s
kube-system                 cilium-operator-774fcb74cb-x8zxx                                  1/1     Running                           0             7m21s
kube-system                 cilium-wjcv9                                                      1/1     Running                           0             7m21s
kube-system                 coredns-5dd5756b68-b4qg6                                          1/1     Running                           0             7m30s
kube-system                 coredns-5dd5756b68-mmrdn                                          1/1     Running                           0             7m29s
kube-system                 etcd-k8s-master-1                                                 1/1     Running                           1             7m53s
kube-system                 kube-apiserver-k8s-master-1                                       1/1     Running                           1             7m55s
kube-system                 kube-controller-manager-k8s-master-1                              1/1     Running                           1             7m55s
kube-system                 kube-proxy-2st9p                                                  1/1     Running                           0             7m30s
kube-system                 kube-scheduler-k8s-master-1                                       1/1     Running                           1             7m53s
kube-system                 metrics-server-68f967f7dc-rp7r2                                   1/1     Running                           0             6m24s
openebs                     openebs-localpv-provisioner-56d6489bbc-jhmnl                      1/1     Running                           0             6m25s
sealos                      desktop-frontend-7667c4cb97-wjwxp                                 0/1     Init:CreateContainerConfigError   0             2m25s
sealos                      sealos-cockroachdb-0                                              1/1     Running                           0             3m27s
sealos                      sealos-cockroachdb-1                                              1/1     Running                           0             3m27s
sealos                      sealos-cockroachdb-2                                              1/1     Running                           0             3m27s
sealos                      sealos-mongodb-mongodb-0                                          3/3     Running                           0             3m19s
vm                          victoria-metrics-k8s-stack-grafana-57c69bcd59-wp88s               3/3     Running                           0             6m5s
vm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-wpnc2    1/1     Running                           0             6m5s
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-pcxhv         1/1     Running                           0             6m5s
vm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bj68sn   1/1     Running                           0             6m5s
vm                          vmagent-victoria-metrics-k8s-stack-894699484-87whr                2/2     Running                           0             4m10s
vm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-j68hm               2/2     Running                           0             5m57s
vm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running                           0             5m57s
vm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-jfj56               1/1     Running                           0             5m57s
```

```
kubectl get endpoints -A
NAMESPACE                   NAME                                                   ENDPOINTS                                                             AGE
cert-manager                cert-manager                                           100.64.0.198:9402                                                     10m
cert-manager                cert-manager-webhook                                   100.64.0.24:10250                                                     10m
cockroach-operator-system   cockroach-operator-webhook-service                     100.64.0.213:9443                                                     9m34s
default                     kubernetes                                             10.0.0.50:6443                                                        11m
ingress-nginx               ingress-nginx-controller                                                                                                     8m59s
ingress-nginx               ingress-nginx-controller-admission                                                                                           8m59s
kb-system                   kubeblocks                                             100.64.0.227:9443,100.64.0.237:9443                                   8m42s
kube-system                 hubble-peer                                            10.0.0.50:4244                                                        10m
kube-system                 kube-dns                                               100.64.0.106:53,100.64.0.84:53,100.64.0.106:53 + 3 more...            10m
kube-system                 metrics-server                                         100.64.0.108:10250                                                    9m38s
kube-system                 victoria-metrics-k8s-stack-coredns                     100.64.0.106:9153,100.64.0.84:9153                                    9m19s
kube-system                 victoria-metrics-k8s-stack-kube-controller-manager     10.0.0.50:10257                                                       9m19s
kube-system                 victoria-metrics-k8s-stack-kube-etcd                   10.0.0.50:2379                                                        9m19s
kube-system                 victoria-metrics-k8s-stack-kube-scheduler              10.0.0.50:10259                                                       9m19s
openebs                     openebs.io-local                                       <none>                                                                9m37s
sealos                      desktop-frontend                                                                                                             5m39s
sealos                      sealos-cockroachdb                                     100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s
sealos                      sealos-cockroachdb-public                              100.64.0.109:26258,100.64.0.232:26258,100.64.0.31:26258 + 6 more...   6m41s
sealos                      sealos-mongodb-mongodb                                 100.64.0.225:27017                                                    6m34s
sealos                      sealos-mongodb-mongodb-headless                        100.64.0.225:8888,100.64.0.225:50001,100.64.0.225:9216 + 2 more...    6m34s
vm                          victoria-metrics-k8s-stack-grafana                     100.64.0.219:3000                                                     9m19s
vm                          victoria-metrics-k8s-stack-kube-state-metrics          100.64.0.6:8080                                                       9m19s
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter    10.0.0.50:9100                                                        9m19s
vm                          victoria-metrics-k8s-stack-victoria-metrics-operator   100.64.0.224:9443,100.64.0.224:8080                                   9m19s
vm                          vmagent-victoria-metrics-k8s-stack                     100.64.0.39:8429                                                      9m8s
vm                          vmalert-victoria-metrics-k8s-stack                     100.64.0.53:8080                                                      9m10s
vm                          vmalertmanager-victoria-metrics-k8s-stack              100.64.0.50:9094,100.64.0.50:9094,100.64.0.50:9093                    9m11s
vm                          vmsingle-victoria-metrics-k8s-stack                    100.64.0.58:8429                                                      8m46s
```

```
kubectl describe pod -n ingress-nginx               ingress-nginx-controller-j9mk8
Name:             ingress-nginx-controller-j9mk8
Namespace:        ingress-nginx
Priority:         0
Service Account:  ingress-nginx
Node:             k8s-master-1/10.0.0.50
Start Time:       Wed, 29 May 2024 12:32:13 -0400
Labels:           app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=ingress-nginx
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=ingress-nginx
                  app.kubernetes.io/part-of=ingress-nginx
                  app.kubernetes.io/version=1.9.4
                  controller-revision-hash=674c9b795f
                  helm.sh/chart=ingress-nginx-4.8.3
                  pod-template-generation=4
Annotations:      <none>
Status:           Running
IP:               10.0.0.50
IPs:
  IP:           10.0.0.50
Controlled By:  DaemonSet/ingress-nginx-controller
Containers:
  controller:
    Container ID:  containerd://7ce271a4ac42f9c9065f6b17ebba031b05ebf43e09086c30ddd2b52944286f63
    Image:         registry.k8s.io/ingress-nginx/controller:v1.9.4
    Image ID:      sealos.hub:5000/ingress-nginx/controller@sha256:0115d7e01987c13e1be90b09c223c3e0d8e9a92e97c0421e712ad3577e2d78e5
    Ports:         80/TCP, 443/TCP, 8443/TCP
    Host Ports:    80/TCP, 443/TCP, 8443/TCP
    Args:
      /nginx-ingress-controller
      --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      --election-id=ingress-nginx-leader
      --controller-class=k8s.io/ingress-nginx
      --ingress-class=nginx
      --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      --validating-webhook=:8443
      --validating-webhook-certificate=/usr/local/certificates/cert
      --validating-webhook-key=/usr/local/certificates/key
      --https-port=8443
      --default-ssl-certificate=sealos-system/wildcard-cert
    State:          Running
      Started:      Wed, 29 May 2024 12:44:15 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 29 May 2024 12:40:27 -0400
      Finished:     Wed, 29 May 2024 12:41:35 -0400
    Ready:          False
    Restart Count:  7
    Requests:
      cpu:      100m
      memory:   90Mi
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:       ingress-nginx-controller-j9mk8 (v1:metadata.name)
      POD_NAMESPACE:  ingress-nginx (v1:metadata.namespace)
      LD_PRELOAD:     /usr/local/lib/libmimalloc.so
    Mounts:
      /usr/local/certificates/ from webhook-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bsr9t (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  webhook-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  ingress-nginx-admission
    Optional:    false
  kube-api-access-bsr9t:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                     From                      Message
  ----     ------     ----                    ----                      -------
  Normal   Scheduled  12m                     default-scheduler         Successfully assigned ingress-nginx/ingress-nginx-controller-j9mk8 to k8s-master-1
  Normal   RELOAD     12m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Normal   Killing    11m                     kubelet                   Container controller failed liveness probe, will be restarted
  Normal   Pulled     11m (x2 over 12m)       kubelet                   Container image ""registry.k8s.io/ingress-nginx/controller:v1.9.4"" already present on machine
  Normal   Created    11m (x2 over 12m)       kubelet                   Created container controller
  Normal   Started    11m (x2 over 12m)       kubelet                   Started container controller
  Normal   RELOAD     11m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Warning  Unhealthy  10m (x8 over 12m)       kubelet                   Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal   RELOAD     10m                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Normal   RELOAD     8m57s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Normal   RELOAD     7m47s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Warning  Unhealthy  7m9s (x31 over 12m)     kubelet                   Readiness probe failed: HTTP probe failed with statuscode: 500
  Normal   RELOAD     6m37s                   nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Normal   RELOAD     4m5s                    nginx-ingress-controller  NGINX reload triggered due to a change in configuration
  Warning  BackOff    2m17s (x13 over 5m28s)  kubelet                   Back-off restarting failed container controller in pod ingress-nginx-controller-j9mk8_ingress-nginx(f0a3d63a-6bca-4a43-b54a-120eca26069c)
  Normal   RELOAD     16s                     nginx-ingress-controller  NGINX reload triggered due to a change in configuration
```",17059,true,2024-06-26 08:32:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-admin,35,cuisongliu,${{ github.sha }} 太长，需要和 docker镜像保持一致,https://github.com/labring/sealos-admin/actions/runs/17451681116/job/49558437160 主要解决这个问题,89,true,2025-09-04 04:10:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,3,cuisongliu,20250729 cert-manager申请证书卡住,"1. https://fael3z0zfze.feishu.cn/wiki/APYLwFZOhiQN0Xk4NSpcB7n4nxe

## 解法

https://github.com/cert-manager/cert-manager/pull/1671 --max-concurrent-challenges",156,true,2025-09-15 04:05:22,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,223,tianshanghong,Wrong `tls.secretName` in Template Causes SSL Certificate Issue,"The TLS certificate does not work for the yaml config generated from the [current version of template](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/template.yaml)

The `tls.secretName` in it does not exist. According to the [Template Variables](https://github.com/labring-actions/templates/blob/c777eda3382946c6059ff058221295a974e69fa5/example.md#built-in-system-variables), it should use **all upper case** letters `SEALOS_CERT_SECRET_NAME` instead of `SEALOS_Cert_Secret_Name`.



",535,true,2024-03-25 06:34:55,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4766,zijiren233,Transfer error: transfer error,"版本： `v5.0.0-beta5`
后端 `costcenter-frontend` 日志主要信息：

```
body: {
      kind: 'Status',
      apiVersion: 'v1',
      metadata: {},
      status: 'Failure',
      message: `Transfer.account.sealos.io ""5l0Lo9YkWm-1717084992510"" is invalid: metadata.name: Invalid value: ""5l0Lo9YkWm-1717084992510"": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')`,
      reason: 'Invalid',
      details: [Object],
      code: 422
    },
```

原因分析：
用户id有大写字母，导致不符合 RFC 1123",690,true,2024-06-27 13:42:04,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,4,cuisongliu,20250804 Cloudflare DNS API 升级导致证书无法签发,"https://fael3z0zfze.feishu.cn/wiki/PbFIwUs3OizAg5kR78mcK4QznQd

```
sealos run docker.io/labring/cert-manager:v1.17.1
```",121,true,2025-09-14 11:32:11,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,292,catdddmouse,dify启动不了，现在访问一直报跨域的问题，应该api的服务没有启动,"一开始是用的应用部署的dify可以正常使用，然后昨天晚上尝试着更新到0.6.14的版本后，出现了报错，然后就重新改会到0.6.11，
![image](https://github.com/user-attachments/assets/4a3e6f46-c403-4327-97af-d16049688147)
![0d44b7198cfc77c24e1320ae7cccad3](https://github.com/user-attachments/assets/c6beb754-4a8a-4141-b416-49a82fc3ce2f)


后面看下了dify的更新日志，11之后的docker部署不一样了， 多了一个.env文件，所以应该不能升级到.11版本后面，但现在我要回滚到.11之前的版本也不行了，但是我部署一个新的dify，发现也运行不了，一直报跨域的问题",393,true,2024-08-08 06:25:31,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4769,zhengyazhao,BUG: brief description of the bug,"### Sealos Version

4.3.7

### How to reproduce the bug?

1.使用yum 安装
2.执行创建单机集群
3. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single
4.centos7.9

### What is the expected behavior?

正常运行、

### What do you see instead?

[root@k8smaster ~]# kubectl get po -A
NAMESPACE     NAME                                READY   STATUS                  RESTARTS       AGE
kube-system   cilium-22szr                        0/1     Init:CrashLoopBackOff   20 (24s ago)   78m
kube-system   cilium-operator-86666d88cb-dnfhx    1/1     Running                 0              78m
kube-system   coredns-5d78c9869d-ggk9p            0/1     Pending                 0              78m
kube-system   coredns-5d78c9869d-qgd2c            0/1     Pending                 0              78m
kube-system   etcd-k8smaster                      1/1     Running                 12             78m
kube-system   kube-apiserver-k8smaster            1/1     Running                 12             78m
kube-system   kube-controller-manager-k8smaster   1/1     Running                 12             78m
kube-system   kube-proxy-pfngx                    1/1     Running                 0              78m
kube-system   kube-scheduler-k8smaster            1/1     Running                 12             78m


查看日志：
0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..


kubelet：

I0605 20:23:00.421613    5697 server.go:415] ""Kubelet version"" kubeletVersion=""v1.27.7""
I0605 20:23:00.421677    5697 server.go:417] ""Golang settings"" GOGC="""" GOMAXPROCS="""" GOTRACEBACK=""""
I0605 20:23:00.421895    5697 server.go:578] ""Standalone mode, no API client""
I0605 20:23:00.422011    5697 container_manager_linux.go:802] ""CPUAccounting not enabled for process"" pid=5697
I0605 20:23:00.422020    5697 container_manager_linux.go:805] ""MemoryAccounting not enabled for process"" pid=5697
I0605 20:23:00.426873    5697 server.go:466] ""No api server defined - no events will be sent to API server""
I0605 20:23:00.426888    5697 server.go:662] ""--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /""
I0605 20:23:00.427421    5697 container_manager_linux.go:265] ""Container manager verified user specified cgroup-root exists"" cgroupRoot=[]
I0605 20:23:00.427488    5697 container_manager_linux.go:270] ""Creating Container Manager object based on Node Config"" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
I0605 20:23:00.427515    5697 topology_manager.go:136] ""Creating topology manager with policy per scope"" topologyPolicyName=""none"" topologyScopeName=""container""
I0605 20:23:00.427529    5697 container_manager_linux.go:301] ""Creating device plugin manager""
I0605 20:23:00.427572    5697 state_mem.go:36] ""Initialized new in-memory state store""
I0605 20:23:00.431001    5697 kubelet.go:411] ""Kubelet is running in standalone mode, will skip API server sync""
I0605 20:23:00.431434    5697 kuberuntime_manager.go:257] ""Container runtime initialized"" containerRuntime=""containerd"" version=""v1.7.15"" apiVersion=""v1""
I0605 20:23:00.431651    5697 volume_host.go:75] ""KubeClient is nil. Skip initialization of CSIDriverLister""
W0605 20:23:00.432240    5697 csi_plugin.go:189] kubernetes.io/csi: kubeclient not set, assuming standalone kubelet
W0605 20:23:00.432259    5697 csi_plugin.go:266] Skipping CSINode initialization, kubelet running in standalone mode
E0605 20:23:00.432366    5697 safe_sysctls.go:62] ""Kernel version is too old, dropping net.ipv4.ip_local_reserved_ports from safe sysctl list"" kernelVersion=""3.10.0""
I0605 20:23:00.432500    5697 server.go:1168] ""Started kubelet""
I0605 20:23:00.432828    5697 kubelet.go:1548] ""No API server defined - no node status update will be sent""
I0605 20:23:00.432922    5697 server.go:194] ""Starting to listen read-only"" address=""0.0.0.0"" port=10255
I0605 20:23:00.434664    5697 server.go:162] ""Starting to listen"" address=""0.0.0.0"" port=10250
I0605 20:23:00.434729    5697 ratelimit.go:65] ""Setting rate limiting for podresources endpoint"" qps=100 burstTokens=10
I0605 20:23:00.434909    5697 fs_resource_analyzer.go:67] ""Starting FS ResourceAnalyzer""
E0605 20:23:00.435607    5697 server.go:794] ""Failed to start healthz server"" err=""listen tcp 127.0.0.1:10248: bind: address already in use""
E0605 20:23:00.435846    5697 cri_stats_provider.go:455] ""Failed to get the info of the filesystem with mountpoint"" err=""unable to find data in memory cache"" mountpoint=""/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs""
I0605 20:23:00.435859    5697 server.go:461] ""Adding debug handlers to kubelet server""
E0605 20:23:00.435879    5697 kubelet.go:1400] ""Image garbage collection failed once. Stats initialization may not have completed yet"" err=""invalid capacity 0 on image filesystem""
I0605 20:23:00.436149    5697 volume_manager.go:284] ""Starting Kubelet Volume Manager""
I0605 20:23:00.436339    5697 desired_state_of_world_populator.go:145] ""Desired state populator starts to run""
E0605 20:23:00.436476    5697 server.go:179] ""Failed to listen and serve"" err=""listen tcp 0.0.0.0:10250: bind: address already in use""


crictl：
[root@k8smaster ~]# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
bde21b3a0d030       9dd45d0a36c9e       About an hour ago   Running             cilium-operator           0                   b302f86dd95a2       cilium-operator-86666d88cb-dnfhx
a6df0160e0dde       21dd3d6f9c60d       About an hour ago   Running             kube-proxy                0                   93053c30c7029       kube-proxy-pfngx
bee6716da8735       58cbecfde1998       About an hour ago   Running             kube-controller-manager   12                  e1a8c7a48c7d8       kube-controller-manager-k8smaster
3a1adb4be83ec       44e520c7a8226       About an hour ago   Running             kube-apiserver            12                  40c7e5974affe       kube-apiserver-k8smaster
23124a7bc7e7f       c8c40891e65bd       About an hour ago   Running             kube-scheduler            12                  d6c6d6a8e48cf       kube-scheduler-k8smaster
cd68f2af5bed6       73deb9a3f7025       About an hour ago   Running             etcd                      12                  f57813da9d5f0       etcd-k8smaster


crictl images：
[root@k8smaster ~]# crictl images
IMAGE                                     TAG                 IMAGE ID            SIZE
sealos.hub:5000/cilium/cilium             v1.13.4             d00a7abfa71a6       174MB
sealos.hub:5000/cilium/operator           v1.13.4             9dd45d0a36c9e       30.8MB
sealos.hub:5000/coredns/coredns           v1.10.1             ead0a4a53df89       16.2MB
sealos.hub:5000/etcd                      3.5.9-0             73deb9a3f7025       103MB
sealos.hub:5000/kube-apiserver            v1.27.7             44e520c7a8226       33.5MB
sealos.hub:5000/kube-controller-manager   v1.27.7             58cbecfde1998       31MB
sealos.hub:5000/kube-proxy                v1.27.7             21dd3d6f9c60d       23.9MB
sealos.hub:5000/kube-scheduler            v1.27.7             c8c40891e65bd       18.2MB
sealos.hub:5000/pause                     3.9                 e6f1816883972       319kB






### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",8500,true,2024-10-05 01:06:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,5,cuisongliu,sync image scripts,"## 我需要写一个脚本放到根目录,它的名字叫 sync-images.sh

1. 它支持目录扫描，如果不存在package.sh 则需要当前的镜像以及版本
2. 他的镜像规则是 ghcr.io/labring/sealos-pro:${NAME}-${VERSION}
3. 需要根据架构分别保存
4. 需要单独写个Kubefile 只缓存这些镜像到registry，方法是创建一个 images/shim目录里面把我需要的镜像名字写到一个txt中去
5. 在这个目录执行sealos build 即可缓存镜像
6. 所有的镜像都是私有镜像 需要提前登录 ，这个镜像需要包括sealos login",300,true,2025-09-11 12:20:04,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,415,PythonPlayerJack,odoo_Create odoo in Sealos APP store,"应用的名称和简介: odoo
应用的官方网站或 GitHub 仓库地址: https://github.com/odoo/odoo, https://github.com/odoo/odoo.git
应用的部署要求 (如：所需的环境变量、存储需求等): 说不清楚
其他相关信息:  无",142,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4770,yulinor,Failed to transfer image during installation,"### Sealos Version

v4.3.7 v5.0.0-beta

### How to reproduce the bug?

OS openeuler 24.03 LTS

253 作为外部控制机，其他主机作为真实master，如101
```
sealos gen labring/kubernetes:v1.29.5-4.3.7 labring/helm:v3.14.0 labring/metrics-server:v0.6.4 --masters 192.168.3.101,192.168.3.102,192.168.3.103  ....
sealos apply -f  ...
warn failed to copy image 127.0.0.1:46879/kube-apiserver:v1.29.5: trying to reuse blob sha256:b2ce0e0660777651a7a188ae1128acc61d01aca10a035a8b1faa2cdd8bbf0785 at destination: pinging container registry 192.168.3.101:5050: received unexpected HTTP status: 502 Bad Gateway
```
sealos 同步镜像至101（master0)的5050 registry，但是registry启动 http 5000/ debug 5001 ，所以始终提示  502 Bad Gateway



### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:  v4.3.7
- Docker version: no
- Kubernetes version:  labring/kubernetes:v1.29.5-4.3.7
- Operating system: openeuler 2403
- Runtime environment: no
- Cluster size: no 
- Additional information: no
```


### Additional information

_No response_",1093,true,2024-06-10 14:59:56,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,7,cuisongliu,脚本重构,"1.  sync-images.sh 脚本从环境变量方式换成 参数的方式执行
2. 缓存镜像不要在当前目录执行，可以换个目录执行",64,true,2025-09-11 12:36:03,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,440,ifzzh,仓库很多pr没人审核,为什么这么多pr没人审核呢？最早甚至有前年的。但我看仓库似乎一直都在更新，如果有人在维护就请多清清pr吧，sealos现在就那么些个模板，不够用,72,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4774,xmwilldo,BUG: The local registry has a large number of network connections that will not be released,"### Sealos Version

v4.1.4

### How to reproduce the bug?

1.netstat -anpl|grep 5000
<img width=""1080"" alt=""CleanShot 2024-06-13 at 13 57 36@2x"" src=""https://github.com/labring/sealos/assets/16560252/4ad3b82c-eada-46f0-ae3c-158f9590d1a3"">


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: v4.1.4
- Docker version: v23.0.0
- Kubernetes version: v1.25.6
- Operating system: ubuntu20.04
- Runtime environment: cloud(tencent, 8G memory, 2core cpu, 50G storage)
- Cluster size: 1 master,1 node
- Additional information:
```


### Additional information

_No response_",675,true,2024-06-17 07:13:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,9,cuisongliu,✨ Set up Copilot instructions,Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).,162,true,2025-09-11 13:00:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/templates,479,kok777,期待添加Waline应用,"应用的名称和简介: odoo
官网 https://waline.js.org/
GITHUB地址：https://github.com/walinejs/waline
我看应用商店有了artalk，但Waline的使用人数也很多。期待官方能上架Waline应用",131,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4779,cnmac,BUG: Modifying pod_cidr via pod_subnet does not take effect,"### Sealos Version

v4.3.7

### How to reproduce the bug?

1. sealos reset
2. rm ~/.sealos/ -rf 
3. sealos gen docker.io/labring/kubernetes:v1.27.14 --masters 192.168.x.x --pk=$HOME/.ssh/id_rsa -o ./Clusterfile
4. sed -i ""s|100.64.0.0/10|100.77.0.0/10|g"" ./Clusterfile
5. sed -i ""s|10.96.0.0/22|10.99.0.0/22|g"" ./Clusterfile
6. sealos apply -f ./Clusterfile
7. sealos run labring/helm:v3.14.1
8. sealos run labring/cilium:v1.14.11 --env ExtraValues=""ipam.mode=kubernetes""
9. kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o wide

### What is the expected behavior?

The IP address assigned to the expected pod is 100.77.x.x

### What do you see instead?

In fact, the IP assigned to the pod is 100.64.x.x:
![image](https://github.com/labring/sealos/assets/21333000/c1030583-ce99-4f23-bb91-a71860943261)
The running configuration of the kube-controller-manager pod is as follows：
![image](https://github.com/labring/sealos/assets/21333000/0b2205bf-4f4b-40ec-a2f4-e5ab6f49d7b7)



### Operating environment

```markdown
- Sealos version: 4.3.7
- Docker version: none
- Kubernetes version: 1.27.14
- Operating system: centos7
- Runtime environment: 32core 128G
- Cluster size: 1
- Additional information: none
```


### Additional information

_No response_",1288,true,2024-06-20 02:27:58,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,11,cuisongliu,sync-images 在原来的逻辑上添加sealos-cloud 镜像，需要新增版本入参,"",0,true,2025-09-11 13:57:29,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4790,zhubao315,Error during installation: waiting for MongoDB to be ready… Error: signal: killed.,"### Sealos Version

v5.0.0-beta5

### How to reproduce the bug?

Error during installation
waiting for mongodb secret generated
waiting for mongodb ready ...Error: signal: killed

Installation steps

curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \  
--cloud-version=v5.0.0-beta5 \  
--image-registry=registry.cn-shanghai.aliyuncs.com --zh \  
--proxy-prefix=https://mirror.ghproxy.com
```


Master IP：192.168.1.80
Node IP：192.168.1.81,192.168.1.83
Domain name：192.168.1.80.nip.io

MongoDB 5.0版本依赖支持 AVX 指令集的 CPU, 当前环境不支持 AVX, 已切换为 MongoDB 4.0版本, 更多信息查看: https://www.mongodb.com/docs/v5.0/administration/production-notes/
2024-06-20T18:56:19 info start to install app in this cluster
2024-06-20T18:56:19 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2024-06-20T18:56:19 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2024-06-20T18:56:19 info Executing PreProcess Pipeline in InstallProcessor
2024-06-20T18:56:20 info Executing pipeline MountRootfs in InstallProcessor.
2024-06-20T18:58:01 info Executing pipeline MirrorRegistry in InstallProcessor.
2024-06-20T18:58:01 info Executing UpgradeIfNeed Pipeline in InstallProcessor
namespace/sealos-system created
namespace/sealos created
customresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created
no mongodb uri found, create mongodb and gen mongodb uri
waiting for mongodb secret generated
waiting for mongodb ready ...Error: signal: killed


### What is the expected behavior?

_No response_

### What do you see instead?

kubectl get pods -A
NAMESPACE                   NAME                                                              READY   STATUS    RESTARTS       AGE
cert-manager                cert-manager-5f8646db6b-5nhxh                                     1/1     Running   0              14h
cert-manager                cert-manager-cainjector-5cf5f57dd7-fdmp6                          1/1     Running   0              14h
cert-manager                cert-manager-webhook-687b7f8b97-l5txj                             1/1     Running   0              14h
cockroach-operator-system   cockroach-operator-manager-848fd7d88d-zpnk4                       1/1     Running   0              14h
ingress-nginx               ingress-nginx-controller-26dpm                                    1/1     Running   0              14h
ingress-nginx               ingress-nginx-controller-6n9mw                                    1/1     Running   0              14h
ingress-nginx               ingress-nginx-controller-wb5d9                                    1/1     Running   0              14h
kb-system                   kb-addon-snapshot-controller-64b4dcb6bc-lzfdm                     1/1     Running   2 (14h ago)    14h
kb-system                   kubeblocks-6954875d7d-rtcnh                                       1/1     Running   3 (14h ago)    14h
kb-system                   kubeblocks-dataprotection-5cfdf54fcc-zh7wg                        1/1     Running   4 (14h ago)    14h
kube-system                 cilium-gpgmh                                                      1/1     Running   0              14h
kube-system                 cilium-gplkj                                                      1/1     Running   0              14h
kube-system                 cilium-jtf27                                                      1/1     Running   0              14h
kube-system                 cilium-operator-97f465f54-lrktg                                   1/1     Running   4 (14h ago)    14h
kube-system                 coredns-5d78c9869d-8mxzp                                          1/1     Running   0              14h
kube-system                 coredns-5d78c9869d-kppdj                                          1/1     Running   0              14h
kube-system                 etcd-rofine30                                                     1/1     Running   2              14h
kube-system                 kube-apiserver-rofine30                                           1/1     Running   2              14h
kube-system                 kube-controller-manager-rofine30                                  1/1     Running   10 (14h ago)   14h
kube-system                 kube-proxy-5mkgv                                                  1/1     Running   0              14h
kube-system                 kube-proxy-82k5f                                                  1/1     Running   0              14h
kube-system                 kube-proxy-kvnxl                                                  1/1     Running   0              14h
kube-system                 kube-scheduler-rofine30                                           1/1     Running   10 (14h ago)   14h
kube-system                 kube-sealos-lvscare-rofine31                                      1/1     Running   1              14h
kube-system                 kube-sealos-lvscare-rofine32                                      1/1     Running   1              14h
kube-system                 metrics-server-84c9bd846f-hb8sh                                   1/1     Running   0              14h
openebs                     openebs-localpv-provisioner-6b456b4c69-wxxr8                      1/1     Running   4 (14h ago)    14h
vm                          victoria-metrics-k8s-stack-grafana-55b778cbf9-jnbsb               3/3     Running   0              14h
vm                          victoria-metrics-k8s-stack-kube-state-metrics-658d9b4dd5-tglth    1/1     Running   0              14h
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-scbzm         1/1     Running   0              14h
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-sd7tw         1/1     Running   0              14h
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-vmglj         1/1     Running   0              14h
vm                          victoria-metrics-k8s-stack-victoria-metrics-operator-6b774mjx9v   1/1     Running   3 (14h ago)    14h
vm                          vmagent-victoria-metrics-k8s-stack-c694d459d-vsxxm                2/2     Running   0              14h
vm                          vmalert-victoria-metrics-k8s-stack-695868d7fc-89vr8               2/2     Running   0              14h
vm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running   0              14h
vm                          vmsingle-victoria-metrics-k8s-stack-687d8cc7b4-r6c6d              1/1     Running   0              14h


### Operating environment

```markdown
- Sealos version: v5.0.0-beta5
- Docker version: containerd://1.7.15
- Kubernetes version: v1.27.11
- Operating system: Anolis OS 8.9   5.10.134-16.2.an8.x86_64
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",7045,true,2024-10-28 20:23:47,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,13,cuisongliu,sync-images.sh 实现一下feature的区分,"1. 版本分商业版和公有云版本
2. 如果是公有云版本后缀有个-cloud 比如版本 v2.1.3是商业版 v2.1.3-cloud是公有云版本，如果不存在-cloud版本则是使用商业版作为cloud的版本",103,true,2025-09-11 14:14:55,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4793,camuselliot,Can you add more details about the offline installation of k8s?,"### Documentation Section

离线安装 K8s

### What is the current documentation state?

_No response_

### What is your suggestion?

老师，您好。
首先在有网络的环境中导出集群镜像： 这里面能否增加
具体k8s和各种网络插件 给一个最佳的推荐  
在执行命令 sealos run kubernetes.tar 
遇到以下错误 
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
^[2^NW0621 15:16:42.324677   16367 checks.go:835] detected that the sandbox image ""sealos.hub:5000/pause:3.9"" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using ""registry.k8s.io/pause:3.9"" as the CRI sandbox image.
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.11: output: E0621 15:08:59.182331   16827 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \""registry.k8s.io/kube-apiserver:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-apiserver:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\"": dial tcp 74.125.23.82:443: i/o timeout"" image=""registry.k8s.io/kube-apiserver:v1.28.11""
time=""2024-06-21T15:08:59+08:00"" level=fatal msg=""pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \""registry.k8s.io/kube-apiserver:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-apiserver:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.11\"": dial tcp 74.125.23.82:443: i/o timeout""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.11: output: E0621 15:11:33.659002   16943 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""registry.k8s.io/kube-controller-manager:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-controller-manager:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\"": dial tcp 74.125.204.82:443: i/o timeout"" image=""registry.k8s.io/kube-controller-manager:v1.28.11""
time=""2024-06-21T15:11:33+08:00"" level=fatal msg=""pulling image: failed to pull and unpack image \""registry.k8s.io/kube-controller-manager:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-controller-manager:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.11\"": dial tcp 74.125.204.82:443: i/o timeout""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.11: output: E0621 15:14:07.264143   17021 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""registry.k8s.io/kube-scheduler:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-scheduler:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\"": dial tcp 74.125.204.82:443: i/o timeout"" image=""registry.k8s.io/kube-scheduler:v1.28.11""
time=""2024-06-21T15:14:07+08:00"" level=fatal msg=""pulling image: failed to pull and unpack image \""registry.k8s.io/kube-scheduler:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-scheduler:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-scheduler/manifests/v1.28.11\"": dial tcp 74.125.204.82:443: i/o timeout""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.11: output: E0621 15:16:42.295317   17286 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \""registry.k8s.io/kube-proxy:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-proxy:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\"": dial tcp 108.177.125.82:443: i/o timeout"" image=""registry.k8s.io/kube-proxy:v1.28.11""
time=""2024-06-21T15:16:42+08:00"" level=fatal msg=""pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \""registry.k8s.io/kube-proxy:v1.28.11\"": failed to resolve reference \""registry.k8s.io/kube-proxy:v1.28.11\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-proxy/manifests/v1.28.11\"": dial tcp 108.177.125.82:443: i/o timeout""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: E0621 15:19:05.165352   17615 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = Unavailable desc = error reading from server: EOF"" image=""registry.k8s.io/pause:3.9""
time=""2024-06-21T15:19:05+08:00"" level=fatal msg=""pulling image: rpc error: code = Unavailable desc = error reading from server: EOF""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.12-0: output: time=""2024-06-21T15:19:05+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///run/containerd/containerd.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""""
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: E0621 15:21:39.262383   18253 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""registry.k8s.io/coredns/coredns:v1.10.1\"": failed to resolve reference \""registry.k8s.io/coredns/coredns:v1.10.1\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\"": dial tcp 108.177.125.82:443: i/o timeout"" image=""registry.k8s.io/coredns/coredns:v1.10.1""
time=""2024-06-21T15:21:39+08:00"" level=fatal msg=""pulling image: failed to pull and unpack image \""registry.k8s.io/coredns/coredns:v1.10.1\"": failed to resolve reference \""registry.k8s.io/coredns/coredns:v1.10.1\"": failed to do request: Head \""https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/coredns/coredns/manifests/v1.10.1\"": dial tcp 108.177.125.82:443: i/o timeout""
, error: exit status 1


### Why is this change beneficial?

可能因为特殊原因 很多人会遇到这种情况 

### Additional information

_No response_",6744,true,2024-10-28 20:23:46,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,15,cuisongliu,修改镜像名字,"# 修改 sync-images 逻辑

1. 根据版本类型区分镜像名字
2. 使用新名字 比如 sealos-pro-all
3. tag需要支持 版本类型 以及cloud的版本
4. 需要保存sealos sealctl 文件 从sealos release 下载。 在缓存路径 新建bin目录并保存",152,true,2025-09-11 14:42:17,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4794,he-aook,Failed to add node,"### Sealos Version

4.0.0

### How to reproduce the bug?

_No response_

### What is the expected behavior?

维护集群添加多个节点

### What do you see instead?

2024-06-23T17:31:21 debug cmd for bash in host:  /var/lib/sealos/data/default/rootfs/opt/sealctl token





2024-06-23T17:33:57 debug failed to exec remote 10.20.0.3:22 shell: /var/lib/sealos/data/default/rootfs/opt/sealctl token output: &{default} error: exit status 1
2024-06-23T17:33:57 error Applied to cluster error: exit status 1
2024-06-23T17:33:57 debug write cluster file to local storage: /root/.sealos/default/Clusterfile
Error: exit status 1

### Operating environment

```markdown
- Sealos version: 4.0.0
- Docker version: containerd 1.6.2
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
centos 7.6
```


### Additional information

你们这个写的太乱了, 之前的版本刚开始安装是好的,添加节点都没问题, 结果运行一段时间, 添加节点就是问题了",926,true,2024-11-07 00:09:43,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,17,bearslyricattack,cilium hubble安全性增强,"开启cilium hubble的tls以增强访问的安全性。
https://fael3z0zfze.feishu.cn/wiki/GWCywoCFBiDH8qkPS7ZcTblCnHg",92,true,2025-09-12 04:03:31,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4796,dinoallo,BUG: template frontend leaves zombie `git` processes,"### Sealos Version

v5.0

### How to reproduce the bug?

Install Sealos with the template frontend enabled

### What is the expected behavior?

No zombies

### What do you see instead?

Find out one of the pids of the zombie processes:
```
# ps aux | grep Z
1001     63841  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>
1001     64043  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>
1001     64183  0.0  0.0      0     0 ?        Z    Jun22   0:00 [git] <defunct>
1001     64495  0.0  0.0      0     0 ?        Z    Jun20   0:00 [git] <defunct>
1001     64661  0.0  0.0      0     0 ?        Z    Jun21   0:00 [git] <defunct>
```
Find out which container holds this pid:
```
# pstree -s -p 64661
systemd(1)───containerd-shim(43915)───node(46306)───git(64661)
# pstree 46306
node─┬─108*[git]
     └─14*[{node}]
# crictl ps -q | xargs crictl inspect --output go-template --template '{{.info.pid}}, {{.status.metadata.name}}' | grep 46306
46306, template-frontend
```

### Operating environment

_No response_

### Additional information

These lines might be problematic:

https://github.com/labring/sealos/blob/78cefc7715b6c0952f079cc6bbc1bd32e96adbcb/frontend/providers/template/src/pages/api/updateRepo.ts#L83-L87",1271,true,2024-06-25 06:18:03,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,18,cuisongliu,devbox如何修改镜像仓库地址,"1. 执行的时候需要获取地址 然后给containerd添加配置
2. 需要测试",40,true,2025-09-14 10:48:43,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4797,ricardolrl,BUG: clusterimages build error,"### Sealos Version

sealos ：4.1.3

### How to reproduce the bug?

1、sealos pull  labring/kubernetes:v1.27.7,打包成tar包：kubernetes-v1.27.7.tar
2、mount至本地后修改images/shim/下DefaultImageList  将其修改为私有仓库镜像如：
192.182.xxx.xx/registry.k8s.io/coredns:v1.9.3
192.182.xxx.xx/registry.k8s.io/etcd:3.5.10-0
192.182.xxx.xx/registry.k8s.io/kube-controller-manager:v1.26.15
192.182.xxx.xx/registry.k8s.io/kube-scheduler:v1.26.15
192.182.xxx.xx/registry.k8s.io/kube-proxy:v1.26.15
192.182.xxx.xx/registry.k8s.io/pause:3.9
192.182.xxx.xx/registry.k8s.io/kube-apiserver:v1.26.15
3、删除registry目录下文件
4、修改Kubefile文件，添加label
FROM scratch
MAINTAINER sealos
LABEL init=""init-cri.sh \$registryDomain \$registryPort && bash init.sh"" \
      clean=""clean.sh && bash clean-cri.sh \$criData"" \
      check=""check.sh \$registryData"" \
      init-registry=""init-registry.sh \$registryData \$registryConfig"" \
      clean-registry=""clean-registry.sh \$registryData \$registryConfig"" \
      sealos.io.type=""rootfs"" sealos.io.version=""v1beta1"" version=""v1.26.15""
ENV criData=/var/lib/containerd \
    registryData=/var/lib/registry \
    registryConfig=/etc/registry \
    registryDomain=192.168.xxx.xx \
    registryPort=5000 \
    registryUsername=admin \
    registryPassword=passw0rd \
    disableApparmor=false \
    SEALOS_SYS_CRI_ENDPOINT=/var/run/containerd/containerd.sock \
    SEALOS_SYS_IMAGE_ENDPOINT=/var/run/image-cri-shim.sock \
COPY . .
5、sealos build
6、sealos run 出现部分报错：
6.1:
""PullImage from image service failed"" err=""rpc error: code = Unavailable desc = error reading from server: EOF"" image=""192.168.xxx.xx:5000/""
FATA[0000] pulling image: rpc error: code = Unavailable desc = error reading from server: EOF
6.2
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0624 15:59:50.512988 2316864 checks.go:832] using image pull policy: IfNotPresent
I0624 15:59:50.531237 2316864 checks.go:849] pulling: registry.k8s.io/kube-apiserver:v1.26.15
I0624 15:59:50.625223 2316864 checks.go:849] pulling: registry.k8s.io/kube-controller-manager:v1.26.15
I0624 15:59:50.713096 2316864 checks.go:849] pulling: registry.k8s.io/kube-scheduler:v1.26.15
I0624 15:59:50.800815 2316864 checks.go:849] pulling: registry.k8s.io/kube-proxy:v1.26.15
I0624 15:59:50.883503 2316864 checks.go:849] pulling: registry.k8s.io/pause:3.9
I0624 15:59:50.970530 2316864 checks.go:849] pulling: registry.k8s.io/etcd:3.5.10-0
I0624 15:59:51.059066 2316864 checks.go:849] pulling: registry.k8s.io/coredns/coredns:v1.9.3
[preflight] Some fatal errors occurred:
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.26.15: output: time=""2024-06-24T15:59:50+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.26.15: output: time=""2024-06-24T15:59:50+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.26.15: output: time=""2024-06-24T15:59:50+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.26.15: output: time=""2024-06-24T15:59:50+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time=""2024-06-24T15:59:50+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.10-0: output: time=""2024-06-24T15:59:51+08:00"" level=fatal msg=""validate service connection: validate CRI v1 image API for endpoint \""unix:///var/run/image-cri-shim.sock\"": rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/image-cri-shim.sock: connect: connection refused\""""

疑问点：为何从远端拉取镜像而不是本地内置registry，查看containerd服务发现未设置point

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

_No response_

### Additional information

_No response_",5639,true,2024-11-07 00:09:42,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,19,cuisongliu,对象存储使用的service是LB 方式，有泄露风险,![Image](https://github.com/user-attachments/assets/52c0c6dc-2f71-4a99-b6c9-f7b08096293c),89,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4802,fu7100,BUG: sealos registry copy无法鉴权通过，即便执行过sealos login命令，依然会提示：authentication required,"### Sealos Version

v4.3.7

### How to reproduce the bug?
执行命令：
[root@master sealos4.4.0]# sealos  registry copy reg.ide.local:5000/titanide/busybox:latest sealos.hub:5000
Getting image source signatures
Error: trying to reuse blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa at destination: checking whether a blob sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa exists in sealos.hub:5000/titanide/busybox: authentication required

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",843,true,2024-10-28 20:23:44,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,20,cuisongliu,无法渲染grafana-operator 的chart,"```
[20:23:11] [~/Workspaces/go/src/github.com/labring/sealos-pro/victoria-metrics-k8s-stack/v1.124.0] git(main) 🔥 ❱❱❱ sreg save --registry-dir=/tmp/registry .  --debug
Error: failed to get images in charts: template: grafana-operator/templates/rbac.yaml:14:22: executing ""grafana-operator/templates/rbac.yaml"" at <$watchNamespaces>: invalid value; expected string
Usage:
  sreg save [flags]

Examples:

sreg save --registry-dir=/tmp/registry .
sreg save --registry-dir=/tmp/registry --images=containers-storage:docker.io/labring/coredns:v0.0.1
sreg save --registry-dir=/tmp/registry --images=docker-daemon:docker.io/library/nginx:latest
sreg save --registry-dir=/tmp/registry --tars=docker-archive:/root/config_main.tar@library/config_main
sreg save --registry-dir=/tmp/registry --tars=oci-archive:/root/config_main.tar@library/config_main
sreg save --registry-dir=/tmp/registry --images=docker.io/library/busybox:latest

Flags:
      --all                   Pull all images if SOURCE-IMAGE is a list
      --arch string           pull images arch (default ""arm64"")
  -h, --help                  help for save
      --images strings        images list
      --max-pull-procs int    maximum number of goroutines for pulling (default 5)
      --registry-dir string   registry data dir path (default ""registry"")
      --tars strings          tar list, eg: --tars=docker-archive:/root/config_main.tar@library/config_main

Global Flags:
      --debug   enable debug logger
```",1472,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4809,bxy4543,BUG: delete node error: failed to run checker: containerd is installed on xxx please uninstall it first,"### Sealos Version

5.0

### How to reproduce the bug?

root@VM-32-39-ubuntu:~# kubectl get no -owide
NAME              STATUS   ROLES           AGE   VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION       CONTAINER-RUNTIME
vm-32-39-ubuntu   Ready    control-plane   66m   v1.26.15   172.16.32.39   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15
vm-32-48-ubuntu   Ready    <none>          66m   v1.26.15   172.16.32.48   <none>        Ubuntu 22.04 LTS   5.15.0-107-generic   containerd://1.7.15
root@VM-32-39-ubuntu:~# ./sealos delete --nodes 172.16.32.48
2024-06-27T15:28:49 info are you sure to delete these nodes?
✔ Yes [y/yes], No [n/no]: y█
2024-06-27T15:28:50 info start to scale this cluster
2024-06-27T15:28:50 info Executing pipeline DeleteCheck in ScaleProcessor.
2024-06-27T15:28:50 info checker:hostname [172.16.32.39:22]
2024-06-27T15:28:50 info checker:containerd [172.16.32.39:22]
Error: failed to run checker: containerd is installed on 172.16.32.39:22 please uninstall it first

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

#4738 
https://github.com/labring/sealos/blob/main/pkg/apply/processor/scale.go#L161

A more precise `Phase` is needed to tell the checker whether it needs to be checked.",1550,true,2024-07-02 09:28:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,21,cuisongliu,其他任务 TOD 待办,"1. 除了默认的监控面板还需要哪些？
2. 对于cloud 来讲需要那些告警通知
3. 有没有一些事监控定制的项目",57,true,2025-09-19 13:34:42,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4814,nlqiong-me,BUG: Error: run chmod to rootfs failed: exit status 1,"### Sealos Version

5.0.0-beta5-a0b3363d9

### How to reproduce the bug?

curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh --zh  \
 --cloud-version=v5.0.0-beta5 \
 --image-registry=registry.cn-shanghai.aliyuncs.com \
 --proxy-prefix=https://mirror.ghproxy.com \
 --master-ips=192.168.0.112,192.168.0.113,192.168.0.120 \
 --node-ips=192.168.0.123,192.168.0.121 \
 --pod-cidr=100.64.0.0/10 \
 --service-cidr=10.96.0.0/22 \
 --cloud-domain=192.168.0.112.nip.io \
 --cloud-port=443 \
 --ssh-password=123456 \
 --kubernetes-version=1.25.6

![image](https://github.com/labring/sealos/assets/174031613/65457c16-089a-42b1-b862-7afd94e34ba9)

![image](https://github.com/labring/sealos/assets/174031613/d63b1cb2-8eb6-4e83-ae98-5ee882cf93a0)




### What is the expected behavior?

_No response_

### What do you see instead?

Everything went well up until now

### Operating environment

```markdown
- Sealos version: v5.0.0-beta5
- Docker version: 
- Kubernetes version:  1.25.6
- Operating system: ubuntu 22.04
- Runtime environment: virtual machine (8G memory, 4 core cpu, 80G storage) *5
- Cluster size: 3 master, 2 node
- Additional information:
```


### Additional information

_No response_",1331,true,2024-10-28 20:23:45,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,22,cuisongliu,优化项目  image-shim 动态加载,https://github.com/labring/sealos/issues/5978,45,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4825,zeje,rockylinux,"### Sealos Version

4.3.7-f39b2339

### How to reproduce the bug?

1. https://rockylinux.org/zh_CN （2024-06）最新版本
2. sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4
3. Job for containerd.service failed because the control process exited with error code.


### What is the expected behavior?

前置条件是干净的系统，但是这么安装为何不行？

### What do you see instead?

[root@k8s-master01 ~]# systemctl status containerd.service                                                                                   ● containerd.service - containerd container runtime
     Loaded: loaded (/etc/systemd/system/containerd.service; enabled; preset: disabled)
     Active: activating (auto-restart) (Result: exit-code) since Sat 2024-06-29 10:25:57 CST; 3s ago
       Docs: https://containerd.io
    Process: 4282 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
    Process: 4283 ExecStart=/usr/bin/containerd (code=exited, status=203/EXEC)
   Main PID: 4283 (code=exited, status=203/EXEC)
        CPU: 4ms
[root@k8s-master01 ~]# journalctl -xeu containerd.service
░░ Subject: Process /usr/bin/containerd could not be executed
░░ Defined-By: systemd
░░ Support: https://wiki.rockylinux.org/rocky/support
░░
░░ The process /usr/bin/containerd could not be executed and failed.
░░
░░ The error number returned by this process is ERRNO.
Jun 29 10:26:18 k8s-master01 systemd[4415]: containerd.service: Failed at step EXEC spawning /usr/bin/containerd: No such file or>
░░ Subject: Process /usr/bin/containerd could not be executed
░░ Defined-By: systemd
░░ Support: https://wiki.rockylinux.org/rocky/support
░░
░░ The process /usr/bin/containerd could not be executed and failed.

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",2057,true,2024-06-29 04:35:28,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,23,cuisongliu,合规检测,https://github.com/bearslyricattack/CompliK 挖坑,46,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1033,hanxianzhai,【Auto-build】cilium,"```
Usage:
   /imagebuild_apps cilium v1.16.0
Available Args:
   appName:  cilium
   appVersion: v.1.16.0

Example:
   /imagebuild_apps helm v3.8.2
```
",162,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4826,liuchengwen,How to install through sealos when Dockerhub cannot be accessed,"### What is the problem this feature will solve?

_No response_

### If you have solution，please describe it

![微信图片_20240629132646](https://github.com/labring/sealos/assets/22442450/7cf69ac3-211c-436e-899a-11d54a4ab05c)


### What alternatives have you considered?

_No response_",281,true,2024-09-26 01:28:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,24,cuisongliu,公有云- wasmPlugins,"```
apiVersion: v1
items:
- apiVersion: extensions.higress.io/v1alpha1
  kind: WasmPlugin
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {""apiVersion"":""extensions.higress.io/v1alpha1"",""kind"":""WasmPlugin"",""metadata"":{""annotations"":{},""name"":""block-vite-fs"",""namespace"":""higress-system""},""spec"":{""defaultConfig"":{""block_regexp_urls"":[""^.*/@fs/.*\\?(?:.*\u0026)?(?:raw|import\u0026raw)\\?\\?.*$""]},""matchRules"":null,""url"":""oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0""}}
    creationTimestamp: ""2025-06-16T07:09:07Z""
    generation: 1
    name: block-vite-fs
    namespace: higress-system
    resourceVersion: ""3870988733""
    uid: c2ef1fe5-0c66-47fe-bcac-1897e8c7fb76
  spec:
    defaultConfig:
      block_regexp_urls:
      - ^.*/@fs/.*\?(?:.*&)?(?:raw|import&raw)\?\?.*$
    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/request-block:1.0.0
- apiVersion: extensions.higress.io/v1alpha1
  kind: WasmPlugin
  metadata:
    creationTimestamp: ""2025-01-23T04:27:44Z""
    generation: 3
    name: ip-restriction-aiproxy
    namespace: higress-system
    resourceVersion: ""1888196182""
    uid: 3edd6726-b595-452d-bb81-79676a78881c
  spec:
    matchRules:
    - config:
        deny:
        - 52.175.17.214
        - 209.17.118.106
        - 52.175.21.231
        ip_header_name: x-forwarded-for
        ip_source_type: header
      ingress:
      - ns-bjwgfnik/network-eeodjtvqdkbd
    url: oci://higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/ip-restriction:1.0.0
- apiVersion: extensions.higress.io/v1alpha1
  kind: WasmPlugin
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {""apiVersion"":""extensions.higress.io/v1alpha1"",""kind"":""WasmPlugin"",""metadata"":{""annotations"":{},""name"":""limit"",""namespace"":""higress-system""},""spec"":{""matchRules"":[{""config"":{""redis"":{""password"":""8bss65fz"",""service_name"":""redis.dns"",""service_port"":6379,""username"":""default""},""rejected_code"":429,""rejected_msg"":""Too many requests, please use a custom domain name to remove the restriction"",""rule_items"":[{""limit_by_per_header"":"":authority"",""limit_keys"":[{""key"":""regexp:^static-host.+"",""query_per_minute"":10}]}],""rule_name"":""default_rule"",""show_limit_quota_header"":true},""domain"":[""*.sealoshzh.site""]}],""priority"":600,""url"":""oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0""}}
    creationTimestamp: ""2025-03-21T13:23:25Z""
    generation: 3
    name: limit
    namespace: higress-system
    resourceVersion: ""2650231697""
    uid: 4bee0edd-b021-4a7d-8fa9-f32436d9f0f1
  spec:
    matchRules:
    - config:
        redis:
          password: *****
          service_name: redis.dns
          service_port: 6379
          username: default
        rejected_code: 429
        rejected_msg: Too many requests, please use a custom domain name to remove
          the restriction
        rule_items:
        - limit_by_per_header: :authority
          limit_keys:
          - key: regexp:^static-host.+
            query_per_minute: 10
        rule_name: default_rule
        show_limit_quota_header: true
      domain:
      - '*.sealoshzh.site'
    priority: 600
    url: oci://docker.pyhdxy.com/zijiren/cluster-key-rate-limit:1.0.0
kind: List
metadata:
  resourceVersion: """"

```",3309,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1034,hanxianzhai,【Auto-build】helm,/imagebuild_apps cilium v1.15.7,31,true,2024-09-21 15:57:51,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4829,gclm,BUG: Failed to add node node,"### Sealos Version

5.0.0-beta5

### How to reproduce the bug?

sealos add --nodes 10.0.12.12 --debug

### What is the expected behavior?

添加节点成功

### What do you see instead?

<img width=""1438"" alt=""image"" src=""https://github.com/labring/sealos/assets/27618687/277697e1-244f-44b7-9c55-486dbf3b0475"">
完整的debug日志在下面链接中
[node.log](https://github.com/user-attachments/files/16043343/node.log)


### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",655,true,2024-07-02 07:12:01,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,25,cuisongliu,公有云-监控,"```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: region-exporter
  namespace: account-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: region-exporter
  template:
    metadata:
      labels:
        app: region-exporter
    spec:
      serviceAccountName: account-controller-manager
      containers:
        - name: region-exporter
          image: [registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531](http://registry.cn-hangzhou.aliyuncs.com/bxy4543/user-exporter:2025.0716.1531)
          imagePullPolicy: Always
          ports:
            - name: metrics
              containerPort: 8000
          envFrom:
            - configMapRef:
                name: account-manager-env
---
apiVersion: v1
kind: Service
metadata:
  name: region-exporter
  namespace: account-system
  labels:
    app: region-exporter
spec:
  selector:
    app: region-exporter
  ports:
    - protocol: TCP
      port: 8000
      targetPort: metrics
      name: metrics
---
apiVersion: [networking.k8s.io/v1](http://networking.k8s.io/v1)
kind: Ingress
metadata:
  name: region-exporter
  namespace: account-system
  annotations:
    [kubernetes.io/ingress.class](http://kubernetes.io/ingress.class): nginx
    [nginx.ingress.kubernetes.io/proxy-body-size](http://nginx.ingress.kubernetes.io/proxy-body-size): 32m
    [nginx.ingress.kubernetes.io/server-snippet](http://nginx.ingress.kubernetes.io/server-snippet): |
      client_header_buffer_size 64k;
      large_client_header_buffers 4 128k;
    [nginx.ingress.kubernetes.io/ssl-redirect](http://nginx.ingress.kubernetes.io/ssl-redirect): 'false'
    [nginx.ingress.kubernetes.io/backend-protocol](http://nginx.ingress.kubernetes.io/backend-protocol): HTTP
    [nginx.ingress.kubernetes.io/rewrite-target](http://nginx.ingress.kubernetes.io/rewrite-target): /$2
    [nginx.ingress.kubernetes.io/client-body-buffer-size](http://nginx.ingress.kubernetes.io/client-body-buffer-size): 64k
    [nginx.ingress.kubernetes.io/proxy-buffer-size](http://nginx.ingress.kubernetes.io/proxy-buffer-size): 64k
    [nginx.ingress.kubernetes.io/proxy-send-timeout](http://nginx.ingress.kubernetes.io/proxy-send-timeout): '300'
    [nginx.ingress.kubernetes.io/proxy-read-timeout](http://nginx.ingress.kubernetes.io/proxy-read-timeout): '300'
    [nginx.ingress.kubernetes.io/configuration-snippet](http://nginx.ingress.kubernetes.io/configuration-snippet): |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 30d;
        add_header Cache-Control ""public"";
      }
spec:
  rules:
    - host: xxx
      http:
        paths:
          - pathType: Prefix
            path: /()(.*)
            backend:
              service:
                name: region-exporter
                port:
                  number: 8000
  tls:
    - hosts:
        - xxx
      secretName: wildcard-cert
---
#apiVersion: [monitoring.coreos.com/v1](http://monitoring.coreos.com/v1)
#kind: ServiceMonitor
#metadata:
#  name: region-exporter
#  namespace: account-system
#  labels:
#    release: prometheus
#spec:
#  selector:
#    matchLabels:
#      app: crd-exporter
#  endpoints:
#    - port: metrics
#      interval: 300s
```",3186,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4836,SupRenekton,"BUG: brief descricontainer_linux.go:318: starting container process caused ""process_linux.go:281: applying cgroup configuration for process caused \""No such device or address\""""ption of the bug","### Sealos Version

sealos_4.3.7_linux_arm64.tar.gz

### How to reproduce the bug?

国产linux系统：4.19.90-24.4.v2101.ky10.aarch64 #1 SMP Mon May 24 14:45:37 CST 2021 aarch64 aarch64 aarch64 GNU/Linux
sealos gen labring/kubernetes:v1.25.16 \
  labring/helm:v3.13.2 \
  labring/calico:v3.24.6 \
  --masters x.x.x.x,x.x.x.x,x.x.x.x --output Clusterfile
Clusterfile配置没有修改，直接apply：sealos apply -f Clusterfile

k8s集群建起来后，只有coredns显示pod没起来，CrashLoopBackOff状态
报错信息：
failed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused ""process_linux.go:281: applying cgroup configuration for process caused \""No such device or address\""""


### What is the expected behavior?

_No response_

### What do you see instead?

2个coredns pod CrashLoopBackOff，其他pod状态正常
failed to create containerd task: failed to create shim task : OCI runtime create failed: container_linux.go:318: starting container process caused ""process_linux.go:281: applying cgroup configuration for process caused \""No such device or address\""""

### Operating environment

```markdown
- Sealos version: sealos_4.3.7_linux_arm64.tar.gz
- Docker version: 无
- Kubernetes version: kubernetes:v1.25.16
- Operating system: 4.19.90-24.4.v2101.ky10.aarch64
- Runtime environment: 
- Cluster size: 3个Master节点
- Additional information:
  labring/helm:v3.13.2
  labring/calico:v3.24.6
```


### Additional information

_No response_",1475,true,2024-11-10 10:08:52,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,26,cuisongliu,openebs 默认的优先级太低了,"![Image](https://github.com/user-attachments/assets/4ac1ca3e-810f-4fde-aa14-b1a21dd74e26)


![Image](https://github.com/user-attachments/assets/7b23a9a5-b4ab-4bff-8d9c-cc33a4ae233a)",181,true,2025-09-19 13:06:49,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4839,mudevai,Request of Adding API documentation for developers,"### Documentation Section

API Documentation

### What is the current documentation state?

Not Found

### What is your suggestion?

Add separate section to describe Sealos APIs in detail.

### Why is this change beneficial?

Programmatically control multitenant apps deployment staging and production.

### Additional information

_No response_",345,true,2024-11-07 00:09:41,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,27,cuisongliu,sealos项目结构统计,"| Controller项目名称                    | 所属版本            | 负责人 |
| ------------------------------------- | ------------------- | ------ |
| sealos-cloud-account-controller       | all                 |        |
| sealos-cloud-app-controller           | all                 |        |
| sealos-cloud-db-adminer-controller    | 已经废弃            |        |
| sealos-cloud-devbox-controller        | 商业&&公有云        |        |
| sealos-cloud-job-controller           | all                 |        |
| sealos-cloud-license-controller       | all                 |        |
| sealos-cloud-node-controller          | 商业&&公有云【GPU，暂未加入】 | 子毅       |
| sealos-cloud-objectstorage-controller | 商业&&公有云        |        |
| sealos-cloud-resources-controller     | all                 |        |

---


| Frontend项目名称                    | 所属版本 | 负责人 |
| ------------------------------------- | -------- | ------ |
| sealos-cloud-desktop-frontend | all |        |
| sealos-cloud-adminer-frontend | 已经废弃 |        |
| sealos-cloud-aiproxy-frontend | 公有云【暂未加入】 |        |
| sealos-cloud-applaunchpad-frontend | all |        |
| sealos-cloud-cloudserver-frontend | 已经废弃【只有一个可用区使用】 |        |
| sealos-cloud-costcenter-frontend | all |        |
| sealos-cloud-cronjob-frontend | all |        |
| sealos-cloud-dbprovider-frontend | all |        |
| sealos-cloud-devbox-frontend | 商业&&公有云 |        |
| sealos-cloud-invite-frontend | 公有云【邀请注册，暂未加入】 | |
| sealos-cloud-kubepanel-frontend | 商业&&公有云 | |
| sealos-cloud-license-frontend | all | |
| sealos-cloud-objectstorage-frontend | 商业&&公有云 | |
| sealos-cloud-switch-region-frontend | 公有云【region切换，暂未加入】 | |
| sealos-cloud-template-frontend | all | |
| sealos-cloud-terminal-frontend | all | |
| sealos-cloud-workorder-frontend | 公有云【工单，暂未加入】 | |


---

| Service项目名称                    | 所属版本 | 负责人 |
| ------------------------------------- | -------- | ------ |
| sealos-cloud-account-service       | all |        |
| sealos-cloud-database-service       | all |        |
| sealos-cloud-devbox-service       | 商业&&公有云 |        |
| sealos-cloud-exceptionmonitor-service       | 公有云【数据库告警，暂未加入】 |        |
| sealos-cloud-hubble-service       | 公有云【流量相关】 |        |
| sealos-cloud-launchpad-service       | all |        |
| sealos-cloud-license-service       | 公有云【https://license.sealos.io】 |        |
| sealos-cloud-minio-service       | 商业&&公有云 |        |
| sealos-cloud-pay-service       | 未上线 |        |
| sealos-cloud-vlogs-service       | 商业&&公有云 |        |

---

| Webhook项目名称                    | 所属版本 | 负责人 |
| ------------------------------------- | -------- | ------ |
| sealos-cloud-admission-webhook       | 公有云【域名劫持】 |  yy      |



---
| 其他项目名称                    | 功能说明 | 负责人 |
| ------------------------------------- | -------- | ------ |
| region-exporter【user-exporter】 | 可用区监控 | 佳辉 |
| region-exporter【lvm-restore-exporter】 | lvm监控 + 后台监控pvc跟 lvm lv 不一致的进行恢复 | 佳辉 |
| region-exporter【bi-info】 | 运营导出数据库 | 佳辉 |
| https://github.com/bearslyricattack/CompliK | 挖矿检测 | 鹏宇 |
| [chart2db](https://fael3z0zfze.feishu.cn/docx/O7H3dUE7EoFa3Fxn2DrcjgpGnig)  |  | 金虎 |
| https://github.com/zijiren233/image-monitor | [镜像监控](https://github.com/user-attachments/files/22375128/Pod.-1758077188997.json) | 仪豪 |
| https://github.com/zijiren233/node-zombie-detector | 节点监控告警 | 仪豪 |
| cleanup-deleting-clusters | 数据库修复 | 金虎 |
| cleanup-failed-opsrequests | 数据库修复 | 金虎 |
| cleanup-succeeded-opsrequests | 数据库修复 | 金虎 |
| clear-mongo-logs | 数据库修复 | 金虎 |
| delete-pg-logs | 数据库修复 | 金虎 |
| set-pg-failsafe-mode | 数据库修复 | 金虎 |
| [sealos-admin](https://github.com/labring/sealos-admin)| 管理员模块 | 子毅 |
| https://github.com/zijiren233/higress/tree/limit-whitelist| higress 插件| 仪豪 |
| [deschduler](https://fael3z0zfze.feishu.cn/wiki/RnIXw0V6Ri0EDYkv4NLcooBAnYf)|deschduler组件，视情况定|仪豪 |



",3801,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4840,wneessen,Chore: Replace go-gomail/gomail with wneessen/go-mail,"### What is the problem this feature will solve?

I've noticed that https://github.com/labring/sealos/blob/main/controllers/account/controllers/utils/email.go is using https://github.com/go-gomail/gomail for sending SMTP mails. go-mail/gomail is not actively maintained anymore and hasn't seen an update/response to an issue by the maintainer/merged PR in more than 6-7 years.

### If you have solution，please describe it

I am the maintainer of https://github.com/wneessen/go-mail which is pretty similar to go-mail/gomail but actively maintained and following modern standards and idiomatic Go. If there is any interest, I am willing to replace go-gomail/gomail in `email.go` to wneessen/go-mail.

Let me know if a PR is of interest.

### What alternatives have you considered?

N/A",786,true,2024-11-26 07:01:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,28,cuisongliu,20250402 连接冲突导致 kubelet 无法连接 api server,https://fael3z0zfze.feishu.cn/wiki/CBAowr3MaigLuAkrwHEcXQ9Unqx,62,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4848,yangxggo,"BUG: upgrade from k8s v1.26.x to v1.27.y , kubelet restart failed","### Sealos Version

both v4.3.7 and v5.0.0_beta

### How to reproduce the bug?

upgrade k8s from v1.26.x to v1.27.y

### What is the expected behavior?

all pods restart and work fine

### What do you see instead?

some pods (include kube-scheduler) are pending or terminating for a long time

### Operating environment

_No response_

### Additional information

_No response_",377,true,2024-07-09 01:13:42,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,29,cuisongliu,etcd event 拆分,https://fael3z0zfze.feishu.cn/wiki/Uwljwf9buiQSLDkTDKxc117Oncg,62,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4856,yangxggo,"BUG: add nodes after upgrading, the version installed on the newly added nodes is incorrect","### Sealos Version

both v4.3.7 and v5.0.0_beta

### How to reproduce the bug?

1. use sealos init install k8s (version such  v1.27.1);
2. upgrade version to v1.27.5;
3. add master or node

### What is the expected behavior?

the newly added master or node install k8s v1.27.5

### What do you see instead?

the newly added master or node install k8s v1.27.1

### Operating environment

_No response_

### Additional information

No matter what version of k8s you install, as long as the cluster is upgraded, when adding new masters or nodes, the new masters or nodes will be installed with the previous version.",614,true,2024-07-09 01:12:29,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,30,dinoallo,2025 Q2 Higress 性能优化,"https://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf
部署参考：https://fael3z0zfze.feishu.cn/docx/Ss2Ld9GbNo8qZJxnU8TcfWainbf ，目前只有镜像。",139,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4861,saashqdev,BUG: CockroachDB stops install process,"### Sealos Version

v5.0.0-beta5

### How to reproduce the bug?

curl -sfL https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \  
--cloud-domain=<your_domain> \   
--cert-path=<your_crt> \  
--key-path=<your_key>

### What is the expected behavior?

Full install

### What do you see instead?

```
...
2024-07-05T19:59:06 info Executing pipeline MirrorRegistry in InstallProcessor.
2024-07-05T19:59:06 info Executing UpgradeIfNeed Pipeline in InstallProcessor
namespace/sealos-system created
namespace/sealos created
customresourcedefinition.apiextensions.k8s.io/notifications.notification.sealos.io created
no mongodb uri found, create mongodb and gen mongodb uri
waiting for mongodb secret generated
waiting for mongodb ready ...mongodb secret has been generated successfully.
no cockroachdb uri found, create cockroachdb and gen cockroachdb uri
cockroachdb statefulset is created.
waiting for cockroachdb ready    Error: signal: killed
waiting for cockroachdb ready ...
```

### Operating environment

```markdown
- Sealos version: v5.0.0-beta5
- Docker version: v20.10.7
- Kubernetes version: v1.25.0
- Operating system: Ubuntu 24.04
- Runtime environment: 3 x 4gb / 2 cpu / 40GB server (Hetzner - CX22's)
- Cluster size: 1 master, 2 nodes
- Additional information:
```


### Additional information

Install just stops with trying to start CockroachDB in the cluster. Eventually signal gets killed. Maybe not enough resources? There was an older issue like this (#4704). Just wondering if they were able to solve it. Cheers, Dave",1660,true,2024-07-06 00:37:58,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,31,dinoallo,2025 Q3 Devbox & Containerd 性能优化,https://fael3z0zfze.feishu.cn/wiki/QDeBw7AphiQb3sk7vGzcZPkFndg,62,true,2025-09-29 01:56:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4869,lllicg,failed to delete node,"### Sealos Version

5.0.0-beta5

### How to reproduce the bug?

1、run cmd sealos apply -f Clusterfile to create k8s.
Clusterfile:
apiVersion: apps.sealos.io/v1beta1
kind: Cluster
metadata:
  name: default
spec:
  env:
  - criData=/data/cri/lib
  hosts:
  - ips:
    - 10.19.193.224:22
    - 10.19.193.167:22
    - 10.19.193.161:22
    roles:
    - master
    - amd64
    ssh:
      passwd: xxxx1
      user: root
      port: 22
  - ips:
    - 10.19.193.153:22
    roles:
    - node
    - amd64
    ssh:
      passwd: xxxxx2
      user: root
      port: 22
  image:
  - 10.21.10.10:5000/labring/kubernetes:v1.27.11
  - 10.21.10.10:5000/labring/helm:v3.10.3
  - 10.21.10.10:5000/labring/calico:v3.26.1
 
2、modify the Clusterfile and remove ip address 10.19.193.161, to delete node 
apiVersion: apps.sealos.io/v1beta1
kind: Cluster
metadata:
  name: default
spec:
  env:
  - criData=/data/cri/lib
  hosts:
  - ips:
    - 10.19.193.224:22
    - 10.19.193.167:22
    roles:
    - master
    - amd64
    ssh:
      passwd: xxxx1
      user: root
      port: 22
  - ips:
    - 10.19.193.153:22
    roles:
    - node
    - amd64
    ssh:
      passwd: xxxx2
      user: root
      port: 22
  image:
  - 10.21.10.10:5000/labring/kubernetes:v1.27.11
  - 10.21.10.10:5000/labring/helm:v3.10.3
  - 10.21.10.10:5000/labring/calico:v3.26.1

3、run cmd sealos apply -f Clusterfile
2024-07-09T11:01:12 warn delete master 10.19.193.161:22 failed cannot get node with ip address 10.19.193.161:22: cannot find host with internal ip 10.19.193.161
2024-07-09T11:01:12 error failed to clean node, exec command if which kubeadm;then kubeadm reset -f  -v 0;fi && \
rm -rf /etc/kubernetes/ && \
rm -rf /etc/cni && rm -rf /opt/cni && \
rm -rf /var/lib/etcd && (ip link delete kube-ipvs0 >/dev/null 2>&1 || true)
 failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain
2024-07-09T11:01:12 error failed to clean node, exec command rm -rf $HOME/.kube failed, connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain
2024-07-09T11:01:12 info succeeded in deleting master 10.19.193.161:22
2024-07-09T11:01:12 info start to sync lvscare static pod to node: 10.19.193.153:22 master: [10.19.193.224:6443 10.19.193.167:6443]
10.19.193.153:22        2024-07-09T11:01:12 info generator lvscare static pod is success
2024-07-09T11:01:12 info Executing pipeline UndoBootstrap in ScaleProcessor
2024-07-09T11:01:12 error Applied to cluster error: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain
Error: failed to execute remote command `/var/lib/sealos/data/default/rootfs/opt/sealctl hosts delete  --domain sealos.hub`: connect error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:v5.0.1
- containerd version:v1.7.14
- Kubernetes version:1.27.11
```


### Additional information

_No response_",3402,true,2024-09-20 12:06:40,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,32,dinoallo,Cost Center 流量模块,"由于代码依赖还没处理完，且中短期内可能有 refactor 的需求，目前流量模块不存在 labring/sealos 中，而是在 https://github.com/dinoallo/sealos-nm-agent/ ，目前部署方式为集群镜像：

```bash
sealos run docker.io/dinoallo/sealos-nm-agent:v0.0.0-alpha -e ""NODE_CIDR=172.16.0.0/24"" -e ""POD_CIDR=100.64.0.0/10"" -e ""HOST_DEVS=eth0,eth1""
```
其中：
- `NODE_CIDR` ：为节点 IP 段
- `POD_CIDR` ：为 POD IP 段
- `HOST_DEVS` ：为节点网卡设备名称，可以指定多个（不存在跳过）",369,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4871,SupRenekton,"God help me! ! ! Kirin sp3 system arm architecture machine, deploying k8s is stuck here in kubelet startup/etc/kubernetes/manifest static pod, kubelet reports error getting node xxx not found","### Sealos Version

sealos_4.3.7_linux_arm64.tar.gz

### How to reproduce the bug?

操作系统：麒麟V10（SP3）/(Lance)-aarch64-Build23/20230324
内核版本：4.19.90-52.22.v2207.ky10.aarch64
sealos版本：4.3.7_linux_arm64、其他版本的也都试过了
k8s版本：1.23.17、1.24.2、1.25.5、1.25.6、1.25.16、1.29.6都试过了

### What is the expected behavior?

卡在Waiting for the kubelet to boot up the control plane as static Pods from directory ""/etc/kubernetes/manifests""
静态pod如kube-apiserver、kube-controller-manager、kube-scheduler这些都起不来；kubelet状态是running，报错静态pod容器捡不起来，getting node xxx not found；containerd状态running，报错failed to get sandbox container task: no running task found

然后比较奇怪的是我之前用麒麟V10(SP2)的系统利用sealos4.3.7是能够正常拉起k8s集群的，只不过有个cgroup的问题导致coredns起不来，需要改containerd的cgropu配置，改的和kubelet的cgroup不一致才行，这样我觉得有隐患，所以听取网上意见更换成麒麟SP3系统，其他的都不变，结果连集群都拉不起来了！！！！！！ 来个大神救我，卡2天了，难受

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:4.3.7
- Docker version:
- Kubernetes version:1.25.16
- Operating system:
- Runtime environment:
- Cluster size:3个master
- Additional information:
```


### Additional information

_No response_",1121,true,2024-07-17 03:08:30,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,33,bearslyricattack,vm关闭alertmanager,"如果告警使用grafana，则需要关闭alertmanager。详情参见
https://fael3z0zfze.feishu.cn/wiki/HAW5wmKvvioB7IkrljVc88Xennd",99,true,2025-09-23 11:19:30,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4877,linuxflow-hqukz,/usr/bin/runc is not an executable program,"### Sealos Version

5.0.0-beta5

### How to reproduce the bug?

执行sealos apply -f Clusterfile  ，Clusterfile如下： xxxx均为内网IP
apiVersion: apps.sealos.io/v1beta1
kind: Cluster
metadata:
  name: default
spec:
  hosts:
  - ips:
    - x.x.x.x:22
    roles:
    - master
    - amd64
  - ips:
    - x.x.x.x:22
    roles:
    - node
    - amd64
  image:
  - x.x.x.x:8080/labring/kubernetes:v1.29.6
  - x.x.x.x:8080/labring/helm:v3.9.4
  - x.x.x.x:8080/labring/cilium:v1.14.9
  ssh:
    passwd: ""PASSWORD""
    pk: /root/.ssh/id_rsa
    port: 22
status: {}
执行完成后，会报错，报错内容为：
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory ""/etc/kubernetes/manifests"". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
2024-07-10T11:29:16 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall
Error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall
检查kubelet状态，错误提示：
Jul 10 12:08:00 k8s-master-01 kubelet[24373]: E0710 12:08:00.064905   24373 kuberuntime_manager.go:1172] ""CreatePodSandbox for pod failed"" err=""rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/a035ab8223d9d9db07cc92d89fa259d6708dc59ba4ab523e1216cd7458025bdd/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown"" pod=""kube-system/kube-scheduler-k8s-master-01""
检查containerd状态，错误提示：
Jul 10 12:09:21 k8s-master-01 containerd[24036]: time=""2024-07-10T12:09:21.072501176+08:00"" level=error msg=""RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-k8s-master-01,Uid:0a9e7e740d78dd02274543ddd3c28942,Namespace:kube-system,Attempt:0,} failed, error"" error=""failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/580589d578e6fa9d791ba8b2cffd221b4c1c24159015fbf34ee0891ebd427d16/log.json: no such file or directory): fork/exec /usr/bin/runc: exec format error: unknown""
最终通过file /usr/bin/runc发现/usr/bin/runc不是可执行程序，
![微信图片_20240710121357](https://github.com/labring/sealos/assets/146060041/98a44917-bb12-45c5-97c1-b91e03185064)

在另外一个节点执行 yum install runc -y，安装完成再查看file /usr/bin/runc为可执行程序。
![a244443b81728790faf0f6cc606a8c8](https://github.com/labring/sealos/assets/146060041/7eaf45b3-1c37-4443-954d-555dc736f6d3)


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:5.0.0-beta5
- Docker version:无
- Kubernetes version:1.29.6
- Operating system:CentOS 7.9
- Runtime environment:虚拟机 4核16G内存64G存储
- Cluster size:
- Additional information:
```


### Additional information

_No response_",4175,true,2024-07-10 07:47:19,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,35,nowinkeyy,admin 的集群监控,"<img width=""1511"" height=""555"" alt=""Image"" src=""https://github.com/user-attachments/assets/af8ae4a9-57be-49d6-a320-d60d7af7fbcc"" />


<img width=""1106"" height=""92"" alt=""Image"" src=""https://github.com/user-attachments/assets/63b26e4b-796f-4c08-9711-e312cf2bc8a9"" />",264,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4881,MuJianning,The Sealos command line is used to build a K8Sv1.27.14 cluster node on Ubuntu20.04.5. The CPU continues to occupy 1.5 cores.,"### Sealos Version

v4.3.7

### How to reproduce the bug?

VirtualBox:v7.0.18
ubuntu-20.04.5-live-server-amd64.iso
https://old-releases.ubuntu.com/releases/20.04.5/ubuntu-20.04.5-live-server-amd64.iso
Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)
CPU：4核
内存：8G
IP：192.168.1.11

安装Sealos命令行工具v4.3.7并安装K8S集群
```
echo ""deb [trusted=yes] https://apt.fury.io/labring/ /"" | tee /etc/apt/sources.list.d/labring.list
apt update
apt install -y sealos

sealos run \
  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.14 \
  registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 \
  registry.cn-shanghai.aliyuncs.com/labring/calico:3.27.3 \
  --masters 192.168.1.11
```
查看CPU占用，发现systemd-udevd异常，进而发现calico异常
```
# htop

# /lib/systemd/systemd-udevd -D

calico_tmp_B: /usr/lib/udev/rules.d/80-net-setup-link.rules:5 Failed to run builtin 'path_id': No such file or directory
```

### What is the expected behavior?

希望空集群节点CPU持续占用小于0.5核

### What do you see instead?

空集群节点CPU持续占用1.5核

### Operating environment

```markdown
- Sealos version:v4.3.7
- Docker version:containerd:v1.7.18
- Kubernetes version:v1.27.14
- Operating system:Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-187-generic x86_64)
- Runtime environment:virtual machine (virbox, 8G memory, 4 core cpu, 100G storage)
- Cluster size:1 master
- Additional information:helm:v3.14.1, calico:3.27.3
```


### Additional information

在线升级Ubuntu 22.04.4 LTS (GNU/Linux 5.15.0-116-generic x86_64)后，CPU持续占用小于0.5核，/lib/systemd/systemd-udevd -D正常",1534,true,2024-07-17 03:08:13,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,37,cuisongliu,前端没有开启payment,"kubectl edit cm -n costcenter-frontend   costcenter-frontend-config
",68,true,2025-09-23 14:26:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4891,ai-liuys,BUG: brief description of the bug,"### Sealos Version

v

### How to reproduce the bug?

_No response_

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",400,true,2024-07-15 03:23:47,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,38,cuisongliu,sealos-enterprise 的 payment不可重入,"",0,true,2025-09-23 14:24:54,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4892,ai-liuys,BUG:  The latest version ( v5.0.0 ) of the sealos privatization deployment will fail due to a public image pull issue,"### Sealos Version

v5.0.0

### How to reproduce the bug?

1.  centos 7.8 
2.  sealos-v5.0.0 deploy k8s in private cloud environment
3.  SHELL:  curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0-beta5/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0-beta5   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com

4.  ERROR INFO:  
W0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11
failed to pull image ""k8s.gcr.io/pause:3.5"": output: E0712 03:49:52.907061   67905 remote_image.go:167] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""k8s.gcr.io/pause:3.5\"": failed to resolve reference \""k8s.gcr.io/pause:3.5\"": failed to do request: Head \""https://k8s.gcr.io/v2/pause/manifests/3.5\"": dial tcp 108.177.97.82:443: connect: connection refused"" image=""k8s.gcr.io/pause:3.5""
time=""2024-07-12T03:49:52-04:00"" level=fatal msg=""pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \""k8s.gcr.io/pause:3.5\"": failed to resolve reference \""k8s.gcr.io/pause:3.5\"": failed to do request: Head \""https://k8s.gcr.io/v2/pause/manifests/3.5\"": dial tcp 108.177.97.82:443: connect: connection refused""
, error: exit status 1
To see the stack trace of this error execute with --v=5 or higher
2024-07-12T03:49:52 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
Error: failed to init masters: master pull image failed, error: exit status  1

### What is the expected behavior?

i think this is a bug ,  if user use this scripts to deploy k8s by sealos , this will try to get images from docker.io/xxx ;

in the sealo project shoud considering  the internet is diffrent , this project  code shoud make user skip to get images form docker.io/xxx .

The code can avoid such an error, like 
---
 W0712 03:47:59.829652   67789 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.11, falling back to the nearest etcd version (3.5.0-0)
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.27.11
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.27.11
failed to pull image ""k8s.gcr.io/pause:3.5"": output: E0712 03:49:52.907061   67905 remote_image.go:167] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""k8s.gcr.io/pause:3.5\"": failed to resolve reference \""k8s.gcr.io/pause:3.5\"": failed to do request: Head \""https://k8s.gcr.io/v2/pause/manifests/3.5\"": dial tcp 108.177.97.82:443: connect: connection refused"" image=""k8s.gcr.io/pause:3.5""
time=""2024-07-12T03:49:52-04:00"" level=fatal msg=""pulling image: rpc error: code = Unknown desc = failed to pull and unpack image \""k8s.gcr.io/pause:3.5\"": failed to resolve reference \""k8s.gcr.io/pause:3.5\"": failed to do request: Head \""https://k8s.gcr.io/v2/pause/manifests/3.5\"": dial tcp 108.177.97.82:443: connect: connection refused""
, error: exit status 1
To see the stack trace of this error execute with --v=5 or higher

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: v5.0.0(latest)
- Docker version: null
- Kubernetes version: v1.27.11
- Operating system: centos-7.8
- Runtime environment: containerd
- Cluster size: 1
- Additional information: mabey code shoud fix this bug , otherwise user will not success deploy k8s . 
This is a sad story！！！
```


### Additional information

_No response_",4064,true,2024-07-17 03:07:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,39,cuisongliu,小强数据库备份,"```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cockroachdb-backup
  namespace: cockroach-operator-system
spec:
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 2
  jobTemplate:
    metadata:
      creationTimestamp: null
      labels:
        app: cockroachdb-backup
        job-type: scheduled
    spec:
      activeDeadlineSeconds: 1200
      backoffLimit: 1
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: cockroachdb-backup
        spec:
          containers:
          - command:
            - /bin/bash
            - -c
            - |
              set -e
              
              echo ""=== CockroachDB 每日备份开始 ===""
              echo ""备份开始时间（UTC）: $(date -u)""
              echo ""备份开始时间（北京）: $(TZ='Asia/Shanghai' date)""
              echo ""定期备份: 每天北京时间0点执行""
              
              # 从共享文件读取桶名
              LOCAL_BUCKET=$(cat /shared/local_bucket)
              GLOBAL_BUCKET=$(cat /shared/global_bucket)
              BACKUP_DATE=$(cat /shared/timestamp)
              
              echo ""每日备份配置:""
              echo ""  执行计划: 每天北京时间0点（UTC 16点）""
              echo ""  备份日期: $BACKUP_DATE""
              echo ""  本地桶: $LOCAL_BUCKET""
              echo ""  全局桶: $GLOBAL_BUCKET""
              echo ""  MinIO端点: $MINIO_ENDPOINT""
              echo ""  备份策略: 当前时间点全量备份""
              
              # 首先检查GC阈值，不指定具体时间点进行当前时间备份
              echo ""=== 检查数据库状态 ===""
              
              # 函数：获取可用的备份时间点
              get_safe_backup_time() {
                local pod_name=$1
                local certs_dir=$2
                echo ""检查 $pod_name 的GC阈值...""
                
                # 查询GC阈值
                local gc_query=""SELECT cluster_logical_timestamp() - (25 * 60 * 60 * 1000000000)::INT8 AS safe_timestamp;""
                
                kubectl exec -n cockroach-operator-system ""$pod_name"" -- \
                  cockroach sql --certs-dir=""$certs_dir"" --host=localhost:26257 \
                  --execute=""$gc_query"" --format=csv | tail -n +2
              }
              
              # 构建备份命令（不使用AS OF SYSTEM TIME，备份当前时间点）
              GLOBAL_BACKUP_CMD=""BACKUP TO 's3://${GLOBAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';""
              
              LOCAL_BACKUP_CMD=""BACKUP TO 's3://${LOCAL_BUCKET}?AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}&AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}&AWS_ENDPOINT=${MINIO_ENDPOINT}';""
              
              # 执行全局备份
              echo ""=== 开始全局备份 ===""
              GLOBAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb-global | grep Running | head -n1 | awk '{print $1}')
              
              if [ -n ""$GLOBAL_POD"" ] && [ ""$GLOBAL_POD"" != """" ]; then
                echo ""找到全局节点: $GLOBAL_POD""
                
                # 检查GC阈值
                echo ""检查全局节点GC状态...""
                GLOBAL_SAFE_TIME=$(get_safe_backup_time ""$GLOBAL_POD"" ""/cockroach/data/certs"" || echo """")
                
                if [ -n ""$GLOBAL_SAFE_TIME"" ]; then
                  echo ""全局节点安全时间戳: $GLOBAL_SAFE_TIME""
                fi
                
                echo ""执行全局备份命令（当前时间点）...""
                
                if kubectl exec -n cockroach-operator-system ""$GLOBAL_POD"" -- \
                   cockroach sql --certs-dir=/cockroach/data/certs --host=localhost:26257 \
                   --execute ""$GLOBAL_BACKUP_CMD""; then
                  echo ""✓ 全局备份成功""
                else
                  echo ""✗ 全局备份失败，但继续执行本地备份""
                fi
              else
                echo ""⚠ 未找到运行中的全局节点，跳过全局备份""
              fi
              
              # 执行本地备份
              echo ""=== 开始本地备份 ===""
              LOCAL_POD=$(kubectl get pods -n cockroach-operator-system --no-headers | grep cockroachdb | grep -v global | grep Running | head -n1 | awk '{print $1}')
              
              if [ -n ""$LOCAL_POD"" ] && [ ""$LOCAL_POD"" != """" ]; then
                echo ""找到本地节点: $LOCAL_POD""
                
                # 检查GC阈值
                echo ""检查本地节点GC状态...""
                LOCAL_SAFE_TIME=$(get_safe_backup_time ""$LOCAL_POD"" ""/cockroach/cockroach-certs"" || echo """")
                
                if [ -n ""$LOCAL_SAFE_TIME"" ]; then
                  echo ""本地节点安全时间戳: $LOCAL_SAFE_TIME""
                fi
                
                echo ""执行本地备份命令（当前时间点）...""
                
                # 指定容器名称避免歧义
                if kubectl exec -n cockroach-operator-system ""$LOCAL_POD"" -c db -- \
                   cockroach sql --certs-dir=/cockroach/cockroach-certs --host=localhost:26257 \
                   --execute ""$LOCAL_BACKUP_CMD""; then
                  echo ""✓ 本地备份成功""
                else
                  echo ""✗ 本地备份失败""
                  exit 1
                fi
              else
                echo ""✗ 未找到运行中的本地节点""
                exit 1
              fi
              
              echo ""=== 备份验证 ===""
              echo ""验证备份文件...""
              
              # 记录备份元数据
              echo ""备份完成时间（UTC）: $(date -u)"" > /shared/backup_result
              echo ""备份完成时间（北京）: $(TZ='Asia/Shanghai' date)"" >> /shared/backup_result
              echo ""备份日期: $BACKUP_DATE"" >> /shared/backup_result
              echo ""执行计划: 每天北京时间0点"" >> /shared/backup_result
              echo ""全局备份桶: $GLOBAL_BUCKET"" >> /shared/backup_result
              echo ""本地备份桶: $LOCAL_BUCKET"" >> /shared/backup_result
              echo ""备份类型: 当前时间点备份"" >> /shared/backup_result
              
              echo ""=== 每日备份流程完成 ===""
            env:
            - name: MINIO_ENDPOINT
              value: https://objectstorageapi.hzh.sealos.run
            - name: MINIO_ACCESS_KEY
              value: xxx
            - name: MINIO_SECRET_KEY
              value: ffff
            image: bitnami/kubectl:latest
            imagePullPolicy: Always
            name: cockroachdb-backup
            resources:
              limits:
                cpu: ""1""
                memory: 2Gi
              requests:
                cpu: 500m
                memory: 1Gi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /shared
              name: shared-data
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - /bin/bash
            - -c
            - |
              set -e
              
              # 生成时间戳和随机字符串
              TIMESTAMP=$(date +'%Y%m%d-%H%M')
              RANDOM_STR=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 6 | head -n 1)
              
              export LOCAL_BUCKET=""local-${TIMESTAMP}-${RANDOM_STR}""
              export GLOBAL_BUCKET=""global-${TIMESTAMP}-${RANDOM_STR}""
              
              echo ""=== 每日备份准备阶段 ===""
              echo ""当前UTC时间: $(date -u)""
              echo ""当前北京时间: $(TZ='Asia/Shanghai' date)""
              echo ""备份执行时间: 北京时间每天0点""
              echo ""备份日期: $TIMESTAMP""
              echo ""随机标识: $RANDOM_STR""
              echo ""本地备份桶: $LOCAL_BUCKET""
              echo ""全局备份桶: $GLOBAL_BUCKET""
              
              # 配置 MinIO 连接
              echo ""配置 MinIO 连接...""
              mc alias set backup ""${MINIO_ENDPOINT}"" ""${MINIO_ACCESS_KEY}"" ""${MINIO_SECRET_KEY}""
              
              # 测试连接
              echo ""测试 MinIO 连接...""
              mc admin info backup
              
              # 创建存储桶
              echo ""创建存储桶...""
              mc mb ""backup/${LOCAL_BUCKET}"" || echo ""本地桶创建失败或已存在""
              mc mb ""backup/${GLOBAL_BUCKET}"" || echo ""全局桶创建失败或已存在""
              
              # 将桶名写入共享文件
              echo ""$LOCAL_BUCKET"" > /shared/local_bucket
              echo ""$GLOBAL_BUCKET"" > /shared/global_bucket
              echo ""$TIMESTAMP"" > /shared/timestamp
              
              echo ""=== 准备阶段完成 ===""
            env:
            - name: MINIO_ENDPOINT
              value: https://objectstorageapi.hzh.sealos.run
            - name: MINIO_ACCESS_KEY
              value: xxx
            - name: MINIO_SECRET_KEY
              value: ffff
            image: bitnami/minio-client:latest
            imagePullPolicy: Always
            name: prepare-backup
            resources:
              limits:
                cpu: 200m
                memory: 256Mi
              requests:
                cpu: 100m
                memory: 128Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /shared
              name: shared-data
          restartPolicy: Never
          schedulerName: default-scheduler
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          serviceAccount: cockroachdb-backup-sa
          serviceAccountName: cockroachdb-backup-sa
          terminationGracePeriodSeconds: 60
          volumes:
          - emptyDir: {}
            name: shared-data
  schedule: 0 16 * * *
  startingDeadlineSeconds: 1800
  successfulJobsHistoryLimit: 5
  suspend: false
status:
  lastScheduleTime: ""2025-09-23T08:00:00Z""
  lastSuccessfulTime: ""2025-09-23T08:01:51Z""
```",9221,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4913,zaunist,BUG: non-admin user can not deploy app on v5.0.0,"### Sealos Version

v5.0.0

### How to reproduce the bug?

1、deploy self-hosted sealos-cloud
2、register a non-admin user
3、deploy any app on sealos desktop

### What is the expected behavior?

deploy success

### What do you see instead?

![image](https://github.com/user-attachments/assets/4bba48f9-7060-446c-b702-d71b85a89ebb)


### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

I just used a node to deploy sealos, when using the admin account can deploy the application normally, using a non-admin account not, if you need to go through some configuration to make the non-admin user can use it normally, how to modify it? ",801,true,2024-08-01 16:54:37,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,40,cuisongliu,labring4docker 的 curl-kubectl 镜像丢失,"<img width=""2332"" height=""632"" alt=""Image"" src=""https://github.com/user-attachments/assets/f5e5d73b-d0b2-4b46-98fc-e7ed8b6b3c76"" />",131,true,2025-09-24 06:51:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4914,zuoFeng59556,Docs: demo video,"### Documentation Section

1

### What is the current documentation state?


https://github.com/user-attachments/assets/a7b7ed5c-0e31-4158-8a76-3b161ed70a70



### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",300,true,2024-07-22 04:27:27,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,45,cuisongliu,参数调优,"apiserver
```
service-cluster-ip-range: ""10.96.0.0/16"" # 重要！如果安装时未调整
max-requests-inflight: ""1000""
max-mutating-requests-inflight: ""1000""
```


controller-manager

```
concurrent-deployment-syncs: ""100""
concurrent-endpoint-syncs: ""50""
concurrent-gc-syncs: ""100""
concurrent-namespace-syncs: ""100""
concurrent-rc-syncs: ""100""
concurrent-replicaset-syncs: ""100""
concurrent-service-endpoint-syncs: ""50""
concurrent-service-syncs: ""100""
kube-api-burst: ""100""
kube-api-qps: ""100""
```",475,true,2025-09-24 11:30:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4923,Forfires,ARM nodes join x86 clusters to form heterogeneous clusters.,"新买的服务器是arm架构的，但是既有集群是x86架构。如何将其加入集群。
目前翻阅了文档和issue，说是支持sealos join命令，但是下载了5.0版本的 sealos没有此命令，请求帮助",97,true,2024-07-24 13:56:34,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,46,cuisongliu,修改containerd的版本号 区分上游版本和自己更改的版本,"",0,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4933,mingmingshiliyu," failed to pull and unpack image \""registry.k8s.io/etcd:3.5.6-0","### Sealos Version

v5.0.0

### How to reproduce the bug?

sealos run as shown in doc

 sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.0 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4  registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 registry.k8s.io/etcd:3.5.6-0 --single


<img width=""768"" alt=""image"" src=""https://github.com/user-attachments/assets/c3984e90-48b3-4320-b6ae-bf0b138b6eed"">


### What is the expected behavior?

<img width=""768"" alt=""image"" src=""https://github.com/user-attachments/assets/ffe0c822-1ba2-46d1-8beb-72cdc854a52f"">


### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",871,true,2024-11-29 06:37:59,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,47,cuisongliu,支持GPU,https://fael3z0zfze.feishu.cn/wiki/LuyFw89BwiL2IpkzCcxcwsLonzg?from=from_copylink,81,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4942,einnse,BUG: brief description of the bug,"### Sealos Version

v5.0.0

### How to reproduce the bug?

```
curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh \
  --cloud-version=v5.0.0 \
  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \
  --proxy-prefix=https://mirror.ghproxy.com
```  
program aborted Error: exit status 2
infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1 pull image 403 Forbidden

### What is the expected behavior?

_No response_

### What do you see instead?

```
addon.extensions.kubeblocks.io/snapshot-controller enabled
secret/additional-scrape-configs created
vmagent.operator.victoriametrics.com/victoria-metrics-k8s-stack patched
deployment.apps/vmagent-victoria-metrics-k8s-stack restarted
正在修改 Ingress-nginx-controller 的容忍度, 以允许它在主节点上运行.
configmap/ingress-nginx-controller patched
daemonset.apps/ingress-nginx-controller patched
daemonset.apps/ingress-nginx-controller patched
daemonset.apps/ingress-nginx-controller patched
正在安装 Sealos Cloud.
2024-08-03T23:03:59 info start to install app in this cluster
2024-08-03T23:03:59 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2024-08-03T23:03:59 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2024-08-03T23:03:59 info Executing PreProcess Pipeline in InstallProcessor
2024-08-03T23:03:59 info Executing pipeline MountRootfs in InstallProcessor.
2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/etc/sealos/.env.tmpl completed
2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mock-cert.yaml.tmpl completed
2024-08-03T23:04:03 info render /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-hyll1ffo/workdir/manifests/mongodb.yaml.tmpl completed
2024-08-03T23:04:03 info Executing pipeline MirrorRegistry in InstallProcessor.
2024-08-03T23:04:03 info Executing UpgradeIfNeed Pipeline in InstallProcessor
etc/sealos/.env: line 1: syntax error near unexpected token `newline'
Error: exit status 2
```

### Operating environment

```markdown
- Sealos version: v5.0.0.
- Docker version:
- Kubernetes version: v1.29.7
- Operating system: Rocky Linux 9.2
- Runtime environment: virtual machine (vmware, 20G memory, 12 core cpu, 500G storage)
- Cluster size: 1master,1node
- Additional information:
```


### Additional information

```
[root@sealos-master ~]# kubectl get nodes -o wide
NAME            STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                      KERNEL-VERSION                 CONTAINER-RUNTIME
sealos-master   Ready    control-plane   46m   v1.29.7   192.168.50.68    <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20
sealos-node     Ready    <none>          46m   v1.29.7   192.168.50.226   <none>        Rocky Linux 9.2 (Blue Onyx)   5.14.0-284.11.1.el9_2.x86_64   containerd://1.7.20
```
kb-addon-snapshot-controller 
```
  Normal   Scheduled  49s                default-scheduler  Successfully assigned kb-system/kb-addon-snapshot-controller-7bc7cf9dbf-vgcqg to sealos-node                                                                             │
│   Normal   BackOff    18s (x2 over 48s)  kubelet            Back-off pulling image ""infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1""                                                         │
│   Warning  Failed     18s (x2 over 48s)  kubelet            Error: ImagePullBackOff                                                                                                                                                  │
│   Normal   Pulling    5s (x3 over 49s)   kubelet            Pulling image ""infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1""                                                                  │
│   Warning  Failed     4s (x3 over 49s)   kubelet            Failed to pull image ""infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1"": failed to pull and unpack image ""infracreate-registry.cn │
│ -zhangjiakou.cr.aliyuncs.com/apecloud/snapshot-controller:v6.2.1"": failed to copy: httpReadSeeker: failed open: unexpected status code https://infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/v2/apecloud/snapshot-controller/b │
│ lobs/sha256:1ef6c138bd5f2ac45f7b4ee54db0e513efad8576909ae9829ba649fb4b067388: 403 Forbidden                                                                                                                                          │
│   Warning  Failed     4s (x3 over 49s)   kubelet            Error: ErrImagePull   
```",5100,true,2024-12-10 01:38:54,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,48,cuisongliu,sealos add 需要支持 env 参数,"",0,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4951,suntiexuan,为集群添加node节点时报错 Kubefile  sha256 sum not match,"### Sealos Version

Version: 4.3.0-7ee53f1d

### How to reproduce the bug?

为集群添加node节点时报错：error Applied to cluster error: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?
Error: failed to copy rootfs default-0zj6yydb: failed to copy entry /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 172.18.8.47:22: sha256 sum not match /var/lib/containers/storage/overlay/5ca40be347ce0e5591de636436f645560a5818580a13c92619551d5a53d25180/merged/Kubefile(19da1b0c0a48328df462baf55b5aa382ffdba6e960d8c513286c9bb3d580285b) != /var/lib/sealos/data/default/rootfs/Kubefile(403075df8b2315228d2275b8a1332a0959550cfb5812d00b2efc2c5397c25733), maybe network corruption?   

运行sealos add 之前也执行过export DO_NOT_CHECKSUM=true 无效果仍然报错，预添加的是第7个节点，之前一次性安装6节点（3master+3node）全部正常运行，但升级过kubernetes 版本到v1.28.0,操作系统均为ubuntu 22.04,网络为同网段，手动拷贝文件没问题，但是无法通过手动拷贝解决，add失败后需要delete掉节点上已经拷贝的其他文件会被清理掉

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",1855,true,2024-12-10 01:38:53,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,49,cuisongliu,devboxscheduler,"1. 只有 cc 用到了,只是纳管起来而已
2. 控制定时开关机的

https://github.com/labring/sealos/pull/5559",78,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4955,baodelu,How does sealos specify docker's data storage directory and modify bip during initialization?,"### What is the problem this feature will solve?

sealos run时和sealos apply -f Clusterfile时docker默认的存储路径是/var/lib/docker，docker网络地址段也是默认的，这两个默认值在sealos初始化时如何修改？

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",278,true,2024-12-10 01:38:55,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,52,cuisongliu,费用支持,"```
db.properties.insertMany(
    [{
      ""name"": ""cpu"",
      ""alias"": """",
      ""enum"": 0,
      ""price_type"": ""AVG"",
      ""unit_price"": 2.237442922,
      ""encrypt_unit_price"": ""iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl"",
      ""unit"": ""1m""
    },{
      ""name"": ""memory"",
      ""alias"": """",
      ""enum"": 1,
      ""price_type"": ""AVG"",
      ""unit_price"": 1.092501427,
      ""encrypt_unit_price"": ""OSE6JKZaoZTdcNQsUA0GzdrObISwNrugpAkShRcq22DaOYpchmxf"",
      ""unit"": ""1Mi""
    },{
      ""name"": ""storage"",
      ""alias"": """",
      ""enum"": 2,
      ""price_type"": ""AVG"",
      ""unit_price"": 0,
      ""encrypt_unit_price"": ""DPo+4f6FksbJ0k0g6SJifQ+PXz+rhUl/W/MahCc="",
      ""unit"": ""1Mi""
    },{
      ""name"": ""services.nodeports"",
      ""alias"": """",
      ""enum"": 4,
      ""price_type"": ""AVG"",
      ""unit_price"": 500,
      ""encrypt_unit_price"": ""OfHNWknr4vDHM+Amy6EfvqrSGjF+YIfZ9yBlBwp6vw=="",
      ""unit"": ""1"",
      ""view_price"": 500000
    },{
      ""name"": ""network"",
      ""alias"": """",
      ""enum"": 3,
      ""price_type"": ""DIF"",
      ""unit_price"": 781,
      ""encrypt_unit_price"": ""4fGwqMywAqV/2O9ED/XB6udBdsqVKS5Y75rfzzW8pw=="",
      ""unit"": ""1Mi""
    },{
      ""name"": ""gpu-NVIDIA-GeForce-RTX-4090"",
      ""alias"": ""RTX-4090"",
      ""enum"": 5,
      ""price_type"": ""AVG"",
      ""unit_price"": 2.237442922,
      ""encrypt_unit_price"": ""iziacrMkifa+OfA0GCRV/yNQ/tS35VOzkj1Kb3bxUuPq9wgH5Bnl"",
      ""unit"": ""1m""
    }]
)
```
```
# 获取 mongodb 数据库的名字
kubectl get cluster -n sealos
# kbcli 连接 mongodb 数据库
kbcli cluster connect sealos-mongodb -n sealos

sealos-mongodb-mongodb [primary] admin> show dbs
sealos-mongodb-mongodb [primary] admin> use sealos-resources
sealos-mongodb-mongodb [primary] admin> show collections

# 重启 account service 和 account controller
kubectl rollout restart deploy account-controller-manager -n account-system
kubectl rollout restart deploy account-service -n account-system
```",1923,true,2025-09-27 07:06:05,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4956,mkshift,"sealos 5.0,  self domain","### Sealos Version

v5.0

### How to reproduce the bug?

why still stay this step ? When executing curl, I confirmed that I pressed Enter all the way, except for entering the domain name.

### What is the expected behavior?

_No response_

### What do you see instead?

![240810_12h34m56s_screenshot](https://github.com/user-attachments/assets/a400982a-044a-4ec1-a25c-166cf8672698)


### Operating environment

```markdown
- Sealos version: 5.0
- Docker version: 
- Kubernetes version: 1.29.7
- Operating system: Debian 12
- Runtime environment: maybe containerd ? I use sealos cli 
- Cluster size:
- Additional information:
```


### Additional information

_No response_",679,true,2024-12-10 01:38:55,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,56,cuisongliu,rootfs的联系方式变更,"<img width=""1450"" height=""410"" alt=""Image"" src=""https://github.com/user-attachments/assets/9be0e69a-c714-4bee-bddb-24027f4ea93b"" />",131,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4962,kinbod,sealos-cloud-5.0.0 install failed,"### Sealos Version

sealos-cloud-5.0.0

### How to reproduce the bug?

正在安装 Sealos Cloud.
2024-08-12T22:11:42 info start to install app in this cluster
2024-08-12T22:11:42 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2024-08-12T22:11:42 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2024-08-12T22:11:42 info Executing PreProcess Pipeline in InstallProcessor
2024-08-12T22:11:43 info Executing pipeline MountRootfs in InstallProcessor.
2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/etc/sealos/.env.tmpl completed
2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mock-cert.yaml.tmpl completed
2024-08-12T22:12:16 info render /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml from /var/lib/sealos/data/default/applications/default-og08w1i2/workdir/manifests/mongodb.yaml.tmpl completed
2024-08-12T22:12:16 info Executing pipeline MirrorRegistry in InstallProcessor.
2024-08-12T22:12:16 info Executing UpgradeIfNeed Pipeline in InstallProcessor
etc/sealos/.env: line 1: syntax error near unexpected token `newline'
Error: exit status 2


### What is the expected behavior?

正常安装

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

应该是模板没有替换参数，找不到替换的值，显示<no value>",1762,true,2024-10-17 04:34:11,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,57,cuisongliu,admin 默认密码随机,目前固定的 需要做到随机密码,14,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4965,jeven2016,"BUG: failed to mount: mounting ""default-qbfp1r97"" container ""default-qbfp1r97"": saving updated state for build container","### Sealos Version

v4.3.5

### How to reproduce the bug?

1. create a sealos app for kubevirt's virtual machine
2.  locally import the sealos image 
3.  execute 'sealos run ' command line for multiple times to create multiple VMs with different names

### What is the expected behavior?

all 'sealos run' command lines completed and all VMs should be created successfully

### What do you see instead?

one of the sealos pods failed to complete, the details as below: 
[root@cluster-master-10 samples]# k logs -f sealosapp-vm-jg-209-predelete-26gfz -n vm-system
+ /tmp/ssh-init.sh
++ cat /tmp/sealosapp-ssh/hosts
+ export SEALOS_HOST=10.20.97.10
+ SEALOS_HOST=10.20.97.10
+ ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa root@10.20.97.10 'bash /opt/sealosapp/vm-jg-209/uninstall.sh'
Warning: Permanently added '10.20.97.10' (ED25519) to the list of known hosts.
+ export COMMON_NAMESPACE=vm-system
+ COMMON_NAMESPACE=vm-system
+ export REPOSITORY=harbor.cloud.net
+ REPOSITORY=harbor.cloud.net
+ export TAG=v1.2.2
+ TAG=v1.2.2
+ export SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-20
+ SEALOS_APP_LABELS=app.kubesphere.io/instance:sealosapp-vm-jg-209
+ service_name=vm-jg-209
+ component=vm-app
+ '[' -z vm-jg-209 ']'
+ '[' -z vm-app ']'
+ sealos run harbor.cloud/vm-app:v1.2.2 -e NAMESPACE=vm-system -e UNINSTALL=true -e chart=vm-app -e release=vm-jg-209 -e CPU_CORES=4 -e DATA_DISK_1_PVC_VOLUME_MODE=Block -e DATA_DISK_1_SIZE=10Gi -e DATA_DISK_1_STORAGE_CLASS=sc-jnds-data -e DATA_DISK_1_TYPE=pvc -e DISK_SIZE=20Gi -e MEMORY=48Gi -e POWER_ON=true -e STORAGE_CLASS=sc-jnds-yw -e 'TCP_PORTS=22 5432 18198 18196 18089' --cmd=./opt/uninstall.sh -f
2024-08-14T09:23:51 info sync new version copy pki config: /var/lib/sealos/data/default/pki /root/.sealos/default/pki
2024-08-14T09:23:51 info sync new version copy etc config: /var/lib/sealos/data/default/etc /root/.sealos/default/etc
2024-08-14T09:23:51 info start to install app in this cluster
2024-08-14T09:23:51 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2024-08-14T09:23:51 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2024-08-14T09:23:51 info Executing PreProcess Pipeline in InstallProcessor
Error: failed to mount: mounting ""default-qbfp1r97"" container ""default-qbfp1r97"": saving updated state for build container ""0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b"": saving builder state to ""/var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json"": rename /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/.tmp-buildah.json1423736918 /var/lib/containers/storage/overlay-containers/0062981e675161ae37750216353d78f98db90a7be794d07751749bb050e7d02b/userdata/buildah.json: no such file or directory

### Operating environment

```markdown
- Sealos version: v4.3.5
- Docker version: 24.0.9
- Kubernetes version: 1.26
- Operating system:  openeuler 22.03
- Runtime environment:
- Cluster size:  standalone k3s cluster with only one node maintained
- Additional information:
```


### Additional information

_No response_",3258,true,2024-12-18 05:27:34,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,58,cuisongliu,etc 支持自定义目录,"```
etcd:
  local:
    dataDir: {{ default ""/var/lib/etcd"" .KUBEADM_ETCD_DATA }}

```",85,true,2025-09-29 02:05:03,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4966,zlwzlwzlw,"centos7 with kernel 5.6.14  install and  failed to pull image ""k8s.gcr.io/pause:3.2""","### Sealos Version

5.0.0

### How to reproduce the bug?

1. centos 7 with kernel 5.6.14
![8d813db3-2262-4c41-a36b-f9d213ee5764](https://github.com/user-attachments/assets/a54aabde-dea0-4d5b-bbf1-bcb24f5f4844)
2. curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.0/scripts/cloud/install.sh -o /tmp/install.sh && bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com
![5e5797fa-cf0d-4967-af8b-257530a32b7f](https://github.com/user-attachments/assets/254f1aa3-c34d-445c-82ea-5f2cb9abc8c7)


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",959,true,2024-08-16 08:31:52,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,60,cuisongliu,深信服内核检测不通过,"",0,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4977,ttsdmmn,Mirror pull failed during offline deployment,"centos7 sealos4.2.0 4.3.0都试过，看起来是kubeadm config images pull 从本地仓库拉取kube-proxy的时候卡住很长时间，最后失败，但是其他的组件kube-apiserver，kube-controller-manager等可以正常下载
![1723733335960](https://github.com/user-attachments/assets/85163d56-d6d5-40a5-9960-2bce4e0ab333)

",247,true,2024-08-15 15:03:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,61,cuisongliu,支持 IPV6,"```
{
	name:                 ""ipv4 cidr too big"",
	cidr:                 *getIPnetFromCIDR(""10.92.0.0/8""),
	maxCIDRBits:          20,
	cidrFlag:             ""--service-cluster-ip-range"",
	expectedErrorMessage: ""specified --service-cluster-ip-range is too large; for 32-bit addresses, the mask must be >= 12"",
	expectErrors:         true,
},
{
	name:                 ""ipv6 cidr too big"",
	cidr:                 *getIPnetFromCIDR(""3000::/64""),
	maxCIDRBits:          20,
	cidrFlag:             ""--service-cluster-ip-range"",
	expectedErrorMessage: ""specified --service-cluster-ip-range is too large; for 128-bit addresses, the mask must be >= 108"",
	expectErrors:         true,
},
```",681,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4978,ttsdmmn,Failed to download image during offline deployment,"centos7 sealos4.2.0 4.3.0都试过，部署单节点k8s，看起来是kubeadm config images pull 从本地仓库拉取kube-proxy的时候卡住很长时间，最后失败，但是其他的组件kube-apiserver，kube-controller-manager等可以正常下载。
![1723733625084](https://github.com/user-attachments/assets/263f3c46-409d-4db3-8c74-f925a82003f0)
docker pull手动拉取kube-proxy也是在划线的地方卡了很久，但最后能下载成功。是否能设置超时时间，或者我手动拉取kube-proxy之后，还需要执行哪些脚本，才能部署完k8s？
![1723734061116](https://github.com/user-attachments/assets/5dbc7c82-980b-4b4d-b33d-ac05311e0170)
",452,true,2024-12-18 05:27:34,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,62,cuisongliu,模版仓库bitnami问题,Error: buildx failed with: ERROR: failed to build: failed to solve: bitnami/git: failed to resolve source metadata for docker.io/bitnami/git:latest: docker.io/bitnami/git:latest: not found,188,true,2025-10-01 04:23:59,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4985,TingqiaoXu,BUG: brief description of the bug,"### Sealos Version

4.1.3

### How to reproduce the bug?

_No response_

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

账户登录问题。如何找回我的账户？",407,true,2024-12-23 21:37:33,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos-pro,63,cuisongliu,插件化devbox sql,"",0,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,4994,JACKCHEN99,support install k8s cluster  with mixed architectures,"### What is the problem this feature will solve?

Is it possible to support k8s cluster deployments with mixed architectures (mixed x86 and arm).  I tried it and now it doesn't work. If it's not feasible, or if there's not much demand for it, forget it

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",371,true,2025-01-14 03:58:23,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4995,malone231214,BUG: brief description of the bug,"### Sealos Version

v5.0.0

### How to reproduce the bug?

root@malone:~# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single
Flag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node
2024-08-23T11:47:20 info Start to create a new cluster: master [192.168.163.23], worker [], registry 192.168.163.23
2024-08-23T11:47:20 info Executing pipeline Check in CreateProcessor.
2024-08-23T11:47:20 info checker:hostname [192.168.163.23:22]
2024-08-23T11:47:20 info checker:timeSync [192.168.163.23:22]
2024-08-23T11:47:20 info checker:containerd [192.168.163.23:22]
2024-08-23T11:47:20 info Executing pipeline PreProcess in CreateProcessor.
2024-08-23T11:47:20 info Executing pipeline RunConfig in CreateProcessor.
2024-08-23T11:47:20 info Executing pipeline MountRootfs in CreateProcessor.
2024-08-23T11:47:26 info Executing pipeline MirrorRegistry in CreateProcessor.
2024-08-23T11:47:26 info trying default http mode to sync images to hosts [192.168.163.23:22]
2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:22829/coredns/coredns:v1.10.1: trying to reuse blob sha256:25b7032c281a433b92d09930f3a03c0f7382c27eb69ae7f35addf2e3853dbba7 at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway
2024-08-23T11:48:18 warn failed to copy image 127.0.0.1:11009/cilium/certgen:v0.1.8: copying image 1/2 from manifest list: trying to reuse blob sha256:0342af6c6dfc37396624a2296e5e6a737e1a61a8ca5d48ba9259a86270d4bc7e at destination: pinging container registry 192.168.163.23:5050: received unexpected HTTP status: 502 Bad Gateway



### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",2124,true,2024-08-23 12:37:59,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,4997,lllicg,Support modifying kubelet rootdir when installing k8s cluster,"### What is the problem this feature will solve?

Is it possible to support modifying kubelet root-dir during the installation of k8s cluster. In the file /usr/lib/systemd/system/kubelet.service. d/10-kubeadm. conf, add KUBELET_EXTRA_ARGS=""-- root dir=/data/dock/kubelet""

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",390,true,2024-09-26 01:27:33,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5007,zijiren233,BUG: sealos reset nil pointer,"### Sealos Version

v5.0.0

### How to reproduce the bug?

sealos reset

### What is the expected behavior?

_No response_

### What do you see instead?

![img_v3_02e7_f295c4c1-12ee-405a-aecf-79ce047fe78g](https://github.com/user-attachments/assets/a8292b3a-7bbc-4081-accd-b783b657d627)


### Operating environment

```markdown
- Sealos version: v5.0.0
- Kubernetes version: v1.27.11
```


### Additional information

_No response_",433,true,2024-12-22 17:19:02,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5029,snakeliwei,BUG: There is a fake ip:10.103.97.2 in the Clusterfile with sealos v5.0.0 gen,"### Sealos Version

v5.0.0

### How to reproduce the bug?

sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.12            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1            registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8            --masters 10.10.16.5,10.10.16.7,10.10.16.8            --nodes 10.10.16.11,10.10.16.12,10.10.16.14            -i '/root/.ssh/id_rsa' --output ~/Clusterfile

### What is the expected behavior?
```
apiServer:
  certSANs:
  - 127.0.0.1
  - apiserver.cluster.local
  - 10.10.16.5
  - 10.10.16.7
  - 10.10.16.8
 ```

### What do you see instead?

```
apiServer:
  certSANs:
  - 127.0.0.1
  - apiserver.cluster.local
  - 10.103.97.2
  - 10.10.16.5
  - 10.10.16.7
  - 10.10.16.8
 ```

### Operating environment

```markdown
- Sealos version: v5.0.0
- Docker version: v20.10.7
- Kubernetes version: v1.28.12
- Operating system: Rocky 9.4
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",1065,true,2024-09-11 06:01:59,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5033,biao-lvwan,BUG: deploy nginx Error: can't apply application type images only since RootFS type image is not applied yet,"### Sealos Version

5.0.0-beta5

### How to reproduce the bug?

![image](https://github.com/user-attachments/assets/b2616a1e-a7d8-48bf-ac17-9f011cf3f322)
Follow the helm installation steps on the official website: https://sealos.run/docs/self-hosting/lifecycle-management/operations/build-image/build-image-helm_charts, an error occurred during run

### What is the expected behavior?

Hope nginx can be pulled up

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",696,true,2024-09-11 06:00:21,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5054,gitnehc,BUG: brief description of the bug panic: runtime error: invalid memory address or nil pointer dereference,"### Sealos Version

v5.0.0

### How to reproduce the bug?

1. sealos reset

### What is the expected behavior?

[root@master0 deployment]# sealos reset --force --masters=xx.xx.xx.xx
2024-09-09T14:08:17 info start to delete Cluster: master [10.105.5.233], node []
2024-09-09T14:08:17 info start to reset nodes: []
2024-09-09T14:08:17 info start to reset masters: [10.105.5.233:22]
2024-09-09T14:08:17 info start to reset node: 10.105.5.233:22
/usr/bin/kubeadm
[preflight] Running pre-flight checks
W0909 14:08:17.401140   12652 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in ""/var/lib/kubelet""
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the ""iptables"" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
2024-09-09T14:08:17 info Executing pipeline Bootstrap in DeleteProcessor
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x359c0c6]

goroutine 1 [running]:
github.com/labring/sealos/pkg/bootstrap.NewContextFrom(0xc000614240)
	github.com/labring/sealos/pkg/bootstrap/context.go:71 +0x2c6
github.com/labring/sealos/pkg/bootstrap.New(0x0?)
	github.com/labring/sealos/pkg/bootstrap/bootstrap.go:49 +0x1d
github.com/labring/sealos/pkg/apply/processor.(*DeleteProcessor).UndoBootstrap(0xc00059f3e0, 0xc000614240)
	github.com/labring/sealos/pkg/apply/processor/delete.go:83 +0x28c
github.com/labring/sealos/pkg/apply/processor.DeleteProcessor.Execute({{0x44c6e10?, 0xc000b47350?}, {0x44bae80?, 0xc000627ba0?}}, 0xc000627ba0?)
	github.com/labring/sealos/pkg/apply/processor/delete.go:50 +0x35c
github.com/labring/sealos/pkg/apply/applydrivers.(*Applier).deleteCluster(0xc000496820)
	github.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:265 +0x53
github.com/labring/sealos/pkg/apply/applydrivers.(*Applier).Delete(0xc000496820)
	github.com/labring/sealos/pkg/apply/applydrivers/apply_drivers_default.go:256 +0xe7
github.com/labring/sealos/cmd/sealos/cmd.newResetCmd.func1(0xc000522c00?, {0xc00059e300?, 0x2?, 0x2?})
	github.com/labring/sealos/cmd/sealos/cmd/reset.go:54 +0xa3
github.com/spf13/cobra.(*Command).execute(0xc000522c00, {0xc00059e2e0, 0x2, 0x2})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x862
github.com/spf13/cobra.(*Command).ExecuteC(0x5c6d560)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3bd
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/labring/sealos/cmd/sealos/cmd.Execute()
	github.com/labring/sealos/cmd/sealos/cmd/root.go:49 +0x25
main.main()
	github.com/labring/sealos/cmd/sealos/main.go:27 +0x29


### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:v5.0.0
- Docker version:
- Kubernetes version:1.27.7
- Operating system:openeuler 22.03
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",3856,true,2025-01-09 18:58:19,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5065,Qiuqi777,etc/sealos/.env: No such file or directory,"### Sealos Version

v5.0.0

### How to reproduce the bug?

1. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com
2. ip change
3. sealos reset --force
4. rm -rf /usr/bin/sealos
5. rm -rf /root/.sealos
6. rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd /root/.kube
7. sudo apt-get remove --purge docker-ce docker-ce-cli containerd.io 
8. sudo rm -rf /var/lib/docker /var/lib/containerd
9. rm find / -name ""*sealos*"" 2>/dev/null
10. sudo reboot
11. bash /tmp/install.sh   --cloud-version=v5.0.0   --image-registry=registry.cn-shanghai.aliyuncs.com --zh   --proxy-prefix=https://mirror.ghproxy.com
12. ERROR: No such file or directoryscripts/init.sh:line 18:etc/sealos/.env 

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

I thought etc/sealos/.env was created by the script, but during execution it showed that there was no such file.",1216,true,2024-09-18 08:54:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5083,weironz,"BUG:  warn image tar config is not exists,skip","### Sealos Version

v5.0.0

### How to reproduce the bug?

```
sealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0
```

### What is the expected behavior?

no warn message

### What do you see instead?

get warn message

```
2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip
```
```
root@ubuntu:/data/cluster-image/kubernetes-nmstate/latest# sealos build -t registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0
2024-09-14T17:55:15 warn image tar config /data/cluster-image/kubernetes-nmstate/latest/images/skopeo/tar.txt is not exists,skip
Getting image source signatures
Copying blob 52b6e7f862c8 done  
Copying blob 68f4756818ef done  
Copying config 6a28855840 done  
Writing manifest to image destination
Storing signatures
Getting image source signatures
Copying blob 5a8fd2560a4c done  
Copying blob eac1b95df832 done  
Copying blob e5f565e86761 done  
Copying blob d382f544b01f done  
Copying blob 47aa3ed2034c done  
Copying blob 7c282516a5e0 done  
Copying config 19fe8540f8 done  
Writing manifest to image destination
Storing signatures
Getting image source signatures
Copying blob e500b42a2bb5 skipped: already exists  
Copying blob 2895d6faeea8 skipped: already exists  
Copying config 4268cf1312 done  
Writing manifest to image destination
Storing signatures
2024-09-14T17:56:03 info saving images quay.io/nmstate/kubernetes-nmstate-handler:v0.82.0, quay.io/openshift/origin-kube-rbac-proxy:4.10.0, quay.io/nmstate/kubernetes-nmstate-operator:v0.82.0
STEP 1/3: FROM scratch
STEP 2/3: COPY . .
STEP 3/3: CMD [""bash entrypoint.sh""]
COMMIT registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0
Getting image source signatures
Copying blob 46100663e36e done  
Copying config b90302488f done  
Writing manifest to image destination
Storing signatures
--> b90302488f6a
Successfully tagged registry.cn-shenzhen.aliyuncs.com/cnmirror/kubernetes-nmstate:v0.82.0
b90302488f6abf1e33910c9b8638e56260e3418a1c166ebe1a8f85324022b232
```

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",2389,true,2025-01-14 03:58:24,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5106,sharkat,sealos run Error: signal: killed,"### Sealos Version

5.0.0

### How to reproduce the bug?

![QQ_1727251467658](https://github.com/user-attachments/assets/2812456b-11fd-4af8-b4d8-8cc4a7694723)
![QQ_1727251490677](https://github.com/user-attachments/assets/82097b4b-280f-4ce5-a903-670af50825fd)
![QQ_1727251530883](https://github.com/user-attachments/assets/55699506-b733-4760-993b-7c38a632e726)
![QQ_1727251563595](https://github.com/user-attachments/assets/b1921db2-9517-4dbe-b8fa-b28185ae4a09)
![QQ_1727251595720](https://github.com/user-attachments/assets/e69c02af-7b8a-430d-8832-8bbcd2efb3af)


### What is the expected behavior?

sealos run images:v0.0.1 执行相关脚本通过helm install中间件 调用检查脚本 等待相关pod就绪，手动执行脚本没问题，通过run执行到中途Error: signal: killed 就报错了。

### What do you see instead?

Error: signal: killed 

### Operating environment

```markdown
- Sealos version:5.0.0
- Docker version:
- Kubernetes version:1.27.7
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",1021,true,2024-09-26 09:52:41,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1072,weironz,Feature: support selecting an architecture type when building an image,"Longhorn v1.7.1 build failed.

```
Error: failed to save images: save image longhornio/openshift-origin-oauth-proxy:4.15: choosing an image from manifest list docker://longhornio/openshift-origin-oauth-proxy:4.15: no image found in manifest list for architecture arm64, variant """", OS linux
```

![image](https://github.com/user-attachments/assets/0229b976-3edd-49c0-a4ca-87a5df3c16fb)

There have no arm image
![image](https://github.com/user-attachments/assets/ef52bd9e-2396-4716-937b-d6e132331d76)

should support 
```
/imagebuild_apps longhorn v1.7.1 amd64
```
",580,true,2024-12-15 07:20:31,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5107,sharkat,sealos add node Error: master not allow empty,"### Sealos Version

5.0.0

### How to reproduce the bug?

rootfs：
![QQ_1727251867263](https://github.com/user-attachments/assets/3cf27fbd-30d1-4e72-bdeb-3a4847168e08)


### What is the expected behavior?

sealos run 自己打包的镜像，将cni ingress-nginx 通过helm的方式进行install安装，sealos run 镜像:v1.0.1 --masters 192.10.100.1 能够正常运行整个集群，网络组件 ingress 正常就绪，但是我通过sealos run 镜像:v1.0.1 --masters  192.10.100.1  --nodes 192.10.100.2 就会报错 Error: master not allow empty


### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",729,true,2024-09-27 05:14:44,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5152,clstech,How to disable verification for adding custom domain names,"### What is the problem this feature will solve?

Disable the verification of domain name resolution when adding custom domains

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",246,true,2025-02-26 03:33:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5155,alltobebetter,BUG: Problem adding custom domain name to sealos.io,"### Sealos Version

None

### How to reproduce the bug?

I tried to add a custom domain name to my project today, but I found that it could not be added successfully, even though I had parsed it 6 hours later, as shown in the picture:

![pic](https://github.com/user-attachments/assets/f32289f8-6272-49b6-9342-684d166e22a2)

I have also seen many users reporting this problem. Is it possible to try without verifying the CNAME? My parsing is on CloudFlare.

![image](https://github.com/user-attachments/assets/25e9ebee-f426-4d6c-a64c-9efcf6e8e98d)

### What is the expected behavior?

i hope to improve

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",890,true,2024-10-18 13:08:46,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1075,cuisongliu,【Auto-build】metrics-server,"```
Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
",186,true,2024-09-21 15:57:39,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5176,VocabVictor,Does the official intend to develop third-party or custom application template deployment?,"### What is the problem this feature will solve?

当前应用商店模板过少，很多地方不能够自定义，部分在实际使用时还会有bug，并且应用管理当中创建应用的功能太不灵活了。

### If you have solution，please describe it

读取网络文件，或者以yaml编辑器的形式传入写好的文件，和正常应用商店里面的模板一样被部署，如果有安全考虑的话，可以不支持用户看到其他用户的自定义模板，不然的话，我觉得在用户本人允许的前提下，可以开放让其他用户获取到第三方模板。这样子对很多sealos的潜在用户而言，可以解决很多痛点问题，同时对于很多有定制化需求的小团队也可以更灵活的使用。

### What alternatives have you considered?

_No response_",385,true,2025-02-27 12:45:57,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5179,biao-lvwan,registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17  install error,"### Sealos Version

5.0.0

### How to reproduce the bug?

sealos run  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.22.17   ----

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory ""/etc/kubernetes/manifests"". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

        Unfortunately, an error has occurred:
                timed out waiting for the condition

        This error is likely caused by:
                - The kubelet is not running
                - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
                - 'systemctl status kubelet'
                - 'journalctl -xeu kubelet'

        Additionally, a control plane component may have crashed or exited when started by the container runtime.
        To troubleshoot, list all containers using your preferred container runtimes CLI.

        Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl:
                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
                Once you have found the failing container, you can inspect its logs with:
                - 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'

error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
2024-10-29T12:21:13 error Applied to cluster error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall
Error: failed to init masters: init master0 failed, error: exit status 1. Please clean and reinstall
```


### Additional information

_No response_",2090,true,2025-02-27 12:45:56,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5180,cuisongliu,BUG: can't get local addresses in single node,"### Sealos Version

5.0.0

### How to reproduce the bug?

2024-10-29T22:13:45 debug sync workdir: /root/.sealos/default
2024-10-29T22:13:45 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 10.4.0.95:22 locally
Error: could not get local addresses: route ip+net: no such network interface

```
func New(inner ssh.Interface) (Interface, error) {
	addr, err := net.InterfaceAddrs()
	if err != nil {
		return nil, fmt.Errorf(""could not get local addresses: %v"", err)
	}
	return &wrap{
		inner:          inner,
		localAddresses: sets.Set[string](netutil.AddressSet(isValid, addr)),
	}, nil
}
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",964,true,2025-02-27 12:45:56,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5183,fu7100,I want to independently deploy etcd to a host outside the cluster. Does sealos support offline deployment of etcd?,"### What is the problem this feature will solve?

_No response_

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",182,true,2025-03-01 01:51:34,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5184,dxygit1,install Cilium version 1.16.0  error,"### Sealos Version

5.0.1

### How to reproduce the bug?

sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1     

cilium  1.16.0 or 1.16.1  

```
ℹ️  Using Cilium version 1.16.0
 Auto-detected cluster name: kubernetes
 Auto-detected kube-proxy has been installed

Error: Unable to install Cilium: execution error at (cilium/templates/cilium-configmap.yaml:71:5): kubeProxyReplacement must be explicitly set to a valid value (true or false) to continue.

```


### What is the expected behavior?

Where do I need to specify kubeProxyReplacement

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",970,true,2024-12-03 05:08:48,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1081,zijiren233,【Auto-build】curl-kubectl,"```
Usage:
   /imagebuild_dockerimages appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version
   buildArgs:  build args, split by ','

Example:
   /imagebuild_dockerimages helm v3.8.2 Key1=Value1,Key2=Value2
```
",267,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5185,AutuSnow,BUG: brief description of the bug,"### Sealos Version

5.0

### How to reproduce the bug?

Problems encountered during make build

### What is the expected behavior?

===========> Building binary lvscare v5.0.1 for linux_amd64
===========> Building binary sealctl v5.0.1 for linux_amd64
# github.com/labring/sealos/cmd/sealctl
/usr/local/go/pkg/tool/linux_amd64/link: running x86_64-linux-gnu-gcc failed: exit status 1
/usr/bin/x86_64-linux-gnu-gcc -m64 -s -Wl,-z,now -Wl,-z,nocopyreloc -o $WORK/b001/exe/a.out -rdynamic /tmp/go-link-3363692486/go.o /tmp/go-link-3363692486/000000.o /tmp/go-link-3363692486/000001.o /tmp/go-link-3363692486/000002.o /tmp/go-link-3363692486/000003.o /tmp/go-link-3363692486/000004.o /tmp/go-link-3363692486/000005.o /tmp/go-link-3363692486/000006.o /tmp/go-link-3363692486/000007.o /tmp/go-link-3363692486/000008.o /tmp/go-link-3363692486/000009.o /tmp/go-link-3363692486/000010.o /tmp/go-link-3363692486/000011.o /tmp/go-link-3363692486/000012.o /tmp/go-link-3363692486/000013.o /tmp/go-link-3363692486/000014.o /tmp/go-link-3363692486/000015.o /tmp/go-link-3363692486/000016.o /tmp/go-link-3363692486/000017.o /tmp/go-link-3363692486/000018.o /tmp/go-link-3363692486/000019.o /tmp/go-link-3363692486/000020.o /tmp/go-link-3363692486/000021.o /tmp/go-link-3363692486/000022.o /tmp/go-link-3363692486/000023.o /tmp/go-link-3363692486/000024.o /tmp/go-link-3363692486/000025.o /tmp/go-link-3363692486/000026.o /tmp/go-link-3363692486/000027.o -O2 -g -O2 -g -lpthread -O2 -g -ldl -O2 -g -O2 -g -O2 -g -ldl
/usr/bin/x86_64-linux-gnu-ld: cannot find crt1.o: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find crti.o: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find -lpthread: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find -ldl: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find -lc: No such file or directory
/usr/bin/x86_64-linux-gnu-ld: cannot find crtn.o: No such file or directory
collect2: error: ld returned 1 exit status

make[1]: *** [scripts/make-rules/golang.mk:60: go.build.linux_amd64.sealctl] Error 1
make: *** [Makefile:58: build] Error 2

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",2493,true,2025-03-01 03:51:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5200,imococo,"Singapore cloud development ID: pxyyxos06z cannot be started, cannot be stopped, and cannot be deleted.","### Sealos Version

当前版本

### How to reproduce the bug?

新加坡区云开发id：pxyyxos06z
无法启动，无法停止，导致无法删除。

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",429,true,2025-03-10 04:46:02,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5201,wenbo-quan,vue application cannot connect to cursor,"### Sealos Version

网页版的

### How to reproduce the bug?

_No response_

### What is the expected behavior?

vue应用连接不上cursor


### What do you see instead?

一直显示 ‘’设置 SSH 主机 gzg.sealos.run-ns-d1l8go8n-devboxreact-scxvqiz699: 正在下载 VS Code 服务器‘’  一直转圈圈连不上

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

node.js是可以连接上",490,true,2025-03-10 04:46:03,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5211,cfy3133,About the local existing code associated with the remotely created Devbox project,"### Documentation Section

怎么把本地已存在的代码项目关联到远程刚创建的Devboox项目下面，这部分的流程文档说明

### What is the current documentation state?

去到文档链接部分未看到这部分的文档输出

### What is your suggestion?

补充一下怎么把本地已存在的代码项目关联到远程刚创建的Devboox项目下面，这部分的流程文档说明

### Why is this change beneficial?

_No response_

### Additional information

_No response_",312,true,2025-03-16 16:10:07,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5213,liseri,"Sealos v5.0.1 self-hosted installation failed, stuck at the “run launchpad monitoring” step.","### Sealos Version

v5.0.1

### How to reproduce the bug?

1. install sealos tool v5.0.1
2. install k8s use:
```
sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.14.1 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \
     --masters 172.17.132.44 \
     --nodes 172.17.132.43 -p xxxxxxx

```
3. install sealos use:
```
$ curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \
  --cloud-version=v5.0.1 \
  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \
  --proxy-prefix=https://mirror.ghproxy.com
```
4. installation failed, stuck at the “run launchpad monitoring” step. log is:
[root@hkserver-44~.sealoslogs.txt](https://github.com/user-attachments/files/17797950/root%40hkserver-44.sealoslogs.txt)


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: 5.0.1
- Docker version: no
- Kubernetes version:1.28.11
- Operating system: centos8
- Runtime environment: glibc 2.28; kernel 5.4
- Cluster size: 2(Time synched)
- Additional information:

[root@hkserver-44 logs]# kubectl get pods -A
NAMESPACE                   NAME                                                              READY   STATUS              RESTARTS      AGE
account-system              account-controller-manager-6fd48875b8-627mg                       2/2     Running             1 (14m ago)   15m
account-system              account-service-5f94b5f779-7ww8r                                  1/1     Running             0             15m
account-system              init-job-5kqgb                                                    0/1     Completed           1             15m
account-system              license-controller-manager-54fdd5d6f7-ckg9j                       2/2     Running             0             15m
app-system                  app-controller-manager-fb7b86fb8-b7cbr                            2/2     Running             0             16m
applaunchpad-frontend       applaunchpad-frontend-55b94f448f-rn6kf                            1/1     Running             0             14m
cert-manager                cert-manager-656b9496d4-qjm7b                                     1/1     Running             0             24m
cert-manager                cert-manager-cainjector-766bf6f44c-6vbf2                          1/1     Running             0             24m
cert-manager                cert-manager-webhook-66b5cd7597-4rjwf                             1/1     Running             0             24m
cockroach-operator-system   cockroach-operator-manager-77cdd79458-lxshg                       1/1     Running             0             23m
costcenter-frontend         costcenter-frontend-6fd7c894f-cwdn4                               1/1     Running             0             13m
cronjob-frontend            cronjob-frontend-d9cbb67b4-stpjc                                  1/1     Running             0             12m
dbprovider-frontend         dbprovider-frontend-b4fd55fb7-f68cg                               1/1     Running             0             13m
higress-system              higress-controller-6766c9486f-kj47d                               0/2     Pending             0             22m
higress-system              higress-gateway-8cr6w                                             0/1     ContainerCreating   0             22m
higress-system              higress-gateway-lznr7                                             0/1     ContainerCreating   0             22m
kb-system                   csi-attacher-s3-0                                                 1/1     Running             0             19m
kb-system                   csi-provisioner-s3-0                                              2/2     Running             0             19m
kb-system                   csi-s3-jm7sg                                                      2/2     Running             0             19m
kb-system                   csi-s3-r7r2p                                                      2/2     Running             0             19m
kb-system                   kb-addon-migration-dt-platform-548c895976-xltfd                   0/1     ImagePullBackOff    0             19m
kb-system                   kb-addon-snapshot-controller-7bc7cf9dbf-npgvv                     1/1     Running             0             19m
kb-system                   kubeblocks-85ddddddd4-rblm9                                       1/1     Running             0             21m
kb-system                   kubeblocks-dataprotection-7f9c76fb8f-79nxp                        1/1     Running             0             21m
kube-system                 cilium-dtfkj                                                      1/1     Running             0             142m
kube-system                 cilium-operator-5448c894cc-xcwp9                                  1/1     Running             0             142m
kube-system                 cilium-zrrtj                                                      1/1     Running             0             142m
kube-system                 coredns-5dd5756b68-p5lh2                                          1/1     Running             0             143m
kube-system                 coredns-5dd5756b68-rtpnp                                          1/1     Running             0             143m
kube-system                 etcd-hkserver-44                                                  1/1     Running             4             143m
kube-system                 kube-apiserver-hkserver-44                                        1/1     Running             4             143m
kube-system                 kube-controller-manager-hkserver-44                               1/1     Running             11            143m
kube-system                 kube-proxy-8mxl7                                                  1/1     Running             0             143m
kube-system                 kube-proxy-lw465                                                  1/1     Running             0             142m
kube-system                 kube-scheduler-hkserver-44                                        1/1     Running             12            143m
kube-system                 kube-sealos-lvscare-hkserver-43                                   1/1     Running             3             142m
kube-system                 metrics-server-68f967f7dc-j76nc                                   1/1     Running             0             23m
kuboard                     kuboard-v3-hkserver-44                                            1/1     Running             0             43m
license-frontend            license-frontend-597c57bfc9-nzxb5                                 1/1     Running             0             13m
openebs                     openebs-localpv-provisioner-56d6489bbc-zdgpb                      1/1     Running             0             23m
resources-system            resources-controller-manager-77fdd6fdd4-r4vn4                     2/2     Running             0             15m
sealos                      database-monitor-deployment-6b964bd695-wm42h                      1/1     Running             0             12m
sealos                      desktop-frontend-59958dd9df-gktp4                                 1/1     Running             0             16m
sealos                      launchpad-monitor-deployment-84fdd6884f-q9mpr                     1/1     Running             0             12m
sealos                      sealos-cockroachdb-0                                              1/1     Running             0             19m
sealos                      sealos-cockroachdb-1                                              1/1     Running             0             19m
sealos                      sealos-cockroachdb-2                                              1/1     Running             0             19m
sealos                      sealos-mongodb-mongodb-0                                          3/3     Running             0             19m
template-frontend           template-frontend-df96f944d-97llk                                 1/1     Running             0             13m
terminal-frontend           terminal-frontend-7969b49c99-5drs5                                1/1     Running             0             14m
terminal-system             terminal-controller-manager-79fc658d89-qzm4k                      2/2     Running             0             16m
user-system                 user-controller-manager-658d4445bb-dqxcn                          2/2     Running             0             16m
vm                          victoria-metrics-k8s-stack-grafana-76cfcccd7f-7tsgg               3/3     Running             0             22m
vm                          victoria-metrics-k8s-stack-kube-state-metrics-6bcdff8ff9-tbbmj    1/1     Running             0             22m
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-p6794         1/1     Running             0             22m
vm                          victoria-metrics-k8s-stack-prometheus-node-exporter-rzd6l         1/1     Running             0             22m
vm                          victoria-metrics-k8s-stack-victoria-metrics-operator-7f87bch94f   1/1     Running             0             22m
vm                          vmagent-victoria-metrics-k8s-stack-69857ff7c7-458hw               2/2     Running             0             19m
vm                          vmalert-victoria-metrics-k8s-stack-5474df58f7-s6sx7               2/2     Running             0             22m
vm                          vmalertmanager-victoria-metrics-k8s-stack-0                       2/2     Running             0             22m
vm                          vmsingle-victoria-metrics-k8s-stack-6d79dc698-dxrls               1/1     Running             0             22m
```
```


### Additional information

how to debug/log/resolve it;",10160,true,2025-01-07 10:41:48,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5231,qikaih,BUG: brief description of the bug,"### Sealos Version

v5.0.1

### How to reproduce the bug?

sealos run labring/kubernetes:v1.31.0 labring/helm:v3.12.0 labring/calico:v3.24.6 --masters xxx, return error 
2024-11-23T17:33:21 info start to init master0...
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",853,true,2025-06-26 18:31:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5250,camilamacedo86,:warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy,"
## Description

:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**
**You must move as soon as possible, sometime from early 2025, the GCR will go away.**

> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._

- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**
  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**

- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.

## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?

[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).

This feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.

### What To Do?

**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**
- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):
  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._
- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.

**For further information, suggestions, and guidance:**
- 📖 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)
- 💬 Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).

> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._

We sincerely apologize for any inconvenience this may cause.

**Thank you for your cooperation and understanding!** :pray:
",3246,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5283,Ironeie,"BUG: error when upgrade kuberenetes from v1.29.7 to v1.30.5, can not mix '--config' with arguments [certificate-renewal]","### Sealos Version

5.0.0

### How to reproduce the bug?

Error when upgrade kuberenetes from v1.29.7 to v1.30.5
can not mix '--config' with arguments [certificate-renewal]

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: 5.0.0
- Containerd version: 1.6.28
- Kubernetes version: v1.29.7
- Operating system: ubuntu 22.04
- Runtime environment: containerd 1.6.28
- Cluster size: 3 master
- Additional information: none
```


### Additional information

_No response_",576,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5303,villain6666,BUG: sealos reset Command Resets Entire Cluster Instead of Specified Node,"### Sealos Version

Sealos Version: 5.0.1

### How to reproduce the bug?

1. In this environment:
   - Sealos 5.0.1 version installed.
   - Cluster consists of 5 nodes:
     - Master nodes: 192.168.37.10, 192.168.37.11, 192.168.37.12
     - Worker nodes: 192.168.37.13, 192.168.37.14
   - Using default Sealos configurations for cluster setup.

2. With this config:
   - Default configuration with Kubernetes setup and basic network settings.
   - No custom modifications made to the Sealos configuration.

3. Run:
   Run `sealos reset --nodes 192.168.37.13` to reset only the node `192.168.37.13`.

4. See error:
   Instead of resetting just the node `192.168.37.13`, the command resets the entire cluster, including master and other worker nodes. This behavior is not expected, as I only intended to reset the specified node.

### What is the expected behavior?

I expect that when running `sealos reset --nodes 192.168.37.13`, only the node `192.168.37.13` should be reset, leaving the rest of the cluster (master and other worker nodes) intact.

### What do you see instead?

Instead of resetting just the specified node `192.168.37.13`, the entire cluster gets reset, including the master and other worker nodes, which is not the intended behavior.

### Operating environment

```markdown
- Sealos version: 5.0.1
- Docker version:
- Kubernetes version: 1.28.10
- Operating system: Ubuntu 22.04
- Runtime environment: Physical machine (16G memory, 8 core CPU, 200GB storage)
- Cluster size: 3 master, 2 node
- Additional information: No additional services like Istio or Dashboard enabled.
```


### Additional information
- The issue occurred when running the `sealos reset --nodes` command, where the reset command affected the entire cluster instead of the specified node.
- Cluster is using default Sealos configuration with Kubernetes setup.
- No custom configurations were applied to Sealos or Kubernetes.",1962,true,2025-04-23 16:06:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1090,cuisongliu,【Auto-build】automq-operator,"```
Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
",186,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5332,liseri,BUG: frontend devbox ingress host is not correct,"### Sealos Version

main

### How to reproduce the bug?

_No response_

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```


### Additional information

_No response_",403,true,2025-03-10 03:17:01,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5355,allran,sealos add can not visit network,"### Sealos Version

v5.0.1

### How to reproduce the bug?

1. 先通过sealos run 安装的master1及node1, 正常运行。
```
sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.30.5 \
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 \
registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8 \
--masters 10.100.0.1 \
--nodes 10.100.0.11 \
--user root \
--passwd abcd1234
```
2. 在node1节点上一个pod里面也可正常访问网络。
```
root@node1:~# kubectl exec -it 678a3ac94db887506c4d9fa0  -n xd -- /bin/sh
Defaulted container ""app"" out of: app, lifecycle-sidecar
/app # ping www.baidu.com
PING www.baidu.com (110.242.68.3): 56 data bytes
64 bytes from 110.242.68.3: seq=0 ttl=43 time=13.281 ms
64 bytes from 110.242.68.3: seq=1 ttl=43 time=12.461 ms
64 bytes from 110.242.68.3: seq=2 ttl=43 time=12.353 ms
64 bytes from 110.242.68.3: seq=3 ttl=43 time=12.365 ms
^Z[1]+  Stopped                    ping www.baidu.com
/app # exit
You have stopped jobs.
/app # exit
root@node1:~# ping www.baidu.com
PING www.a.shifen.com (110.242.69.21) 56(84) bytes of data.
64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=1 ttl=45 time=13.0 ms
64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=2 ttl=45 time=12.7 ms
64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=3 ttl=45 time=12.5 ms
64 bytes from 110.242.69.21 (110.242.69.21): icmp_seq=4 ttl=45 time=12.5 ms
^Z
[1]+  Stopped                 ping www.baidu.com
root@node1:~# 
```

问题：然后通过`sealos add --nodes 10.100.0.13` 添加新的节点，发现里面创建的pod不能访问网络, 但节点本身可访问网络。
```
root@node3:~# kubectl exec -it 6788b47bd5b570fe47ef8b0e  -n xd -- /bin/sh
Defaulted container ""app"" out of: app, lifecycle-sidecar
/app # ping www.baidu.com
^Z[1]+  Stopped                    ping www.baidu.com
/app # exit
You have stopped jobs.
/app # exit
root@node3:~# 
root@node3:~# ping www.baidu.com
PING www.a.shifen.com (180.101.50.242) 56(84) bytes of data.
64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=1 ttl=47 time=11.3 ms
64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=2 ttl=47 time=11.3 ms
64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=3 ttl=47 time=11.3 ms
64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=4 ttl=47 time=11.3 ms
64 bytes from 180.101.50.242 (180.101.50.242): icmp_seq=5 ttl=47 time=11.3 ms
^Z
[5]+  Stopped                 ping www.baidu.com
root@node3:~# 
```

以下是pod信息：
```
root@master1:~# kubectl get pods -o wide -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS          AGE     IP            NODE      NOMINATED NODE   READINESS GATES
kube-system   cilium-2s2h4                       1/1     Running   4 (5d10h ago)     5d10h   10.100.0.1    master1   <none>           <none>
kube-system   cilium-6g7lm                       1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>
kube-system   cilium-djjgr                       1/1     Running   0                 16h     10.100.0.12   node2     <none>           <none>
kube-system   cilium-dtcmn                       1/1     Running   0                 16h     10.100.0.13   node3     <none>           <none>
kube-system   cilium-operator-5dccd84bff-f7jhc   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>
kube-system   coredns-55cb58b774-4x4xz           1/1     Running   0                 5d10h   10.0.0.215    node1     <none>           <none>
kube-system   coredns-55cb58b774-f558l           1/1     Running   0                 5d10h   10.0.0.106    node1     <none>           <none>
kube-system   etcd-master1                       1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>
kube-system   kube-apiserver-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>
kube-system   kube-controller-manager-master1    1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>
kube-system   kube-proxy-284zd                   1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>
kube-system   kube-proxy-gmtkj                   1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>
kube-system   kube-proxy-n688p                   1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>
kube-system   kube-proxy-zkcl7                   1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>
kube-system   kube-scheduler-master1             1/1     Running   0                 5d10h   10.100.0.1    master1   <none>           <none>
kube-system   kube-sealos-lvscare-node1          1/1     Running   0                 5d10h   10.100.0.11   node1     <none>           <none>
kube-system   kube-sealos-lvscare-node2          1/1     Running   0                 23h     10.100.0.12   node2     <none>           <none>
kube-system   kube-sealos-lvscare-node3          1/1     Running   0                 22h     10.100.0.13   node3     <none>           <none>
xd          6788a6ead5b570fe47ef8b0d           2/2     Running   10 (2m35s ago)    18h     10.0.0.11     node1     <none>           <none>
xd          6788b47bd5b570fe47ef8b0e           2/2     Running   2 (8h ago)        16h     10.0.3.78     node3     <none>           <none>
root@master1:~# 
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",5688,true,2025-05-21 21:00:21,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5358,lin-calvin,the ` tailscale-options` envoy filter dosn't pass headscale's `Upgrade: tailscale-control-protocol` to backend,"",0,true,2025-05-24 20:04:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5372,jemerci,"Docs: devbox plugin file , how  to download it?","### Documentation Section

plugin download

### What is the current documentation state?

https://hzh.sealos.run/?bd_vid=&k=&s=
I can't open the online project from this location using curser, it tells me. 
Cursor's Devbox is often not the latest. If there are any issues, please manually install the plugin referenced this URI.

Microsoft's Plugin Store does not have a download option for devbox. 
Where can I download the plugin devbox?

<img width=""1136"" alt=""Image"" src=""https://github.com/user-attachments/assets/8f2bbda7-aad7-4cbf-8e42-1f216c182769"" />

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",698,true,2025-02-26 00:19:42,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5375,docker-master,= Is it OK to deploy sealos for a video version? Now these look a little dizzy😵‍💫,"### Documentation Section

脚本不太好用，能做到1panel面板那样的安装就好了

### What is the current documentation state?

sealos 集群部署 这一块看不懂

### What is your suggestion?

加入哔哩哔哩的安装视频

### Why is this change beneficial?

_No response_

### Additional information

_No response_",256,true,2025-08-30 10:30:06,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5376,steinvenic,BUG: Error: client rate limiter Wait returned an error: context deadline exceeded,"### Sealos Version

5.0.1

### How to reproduce the bug?

# 概述
- 感觉主要是安装参数` --master-ips=172.19.117.70,172.19.117.71,172.19.117.72 `导致的，使用单节点方式，没有该问题，可以正常手动添加
- 在ubuntu 24.04上应该是必现问题，在阿里云和本地都能复现


# 安装命令
```
bash install.sh  --zh   --cloud-version=v5.0.1  --image-registry=registry.cn-shanghai.aliyuncs.com  --proxy-prefix=https://mirror.ghproxy.com  --pod-cidr=100.64.0.0/10  --service-cidr=10.96.0.0/22  --cloud-port=443  --ssh-private-key=/root/.ssh/id_rsa  --kubernetes-version=1.28.11  --master-ips=172.19.117.70,172.19.117.71,172.19.117.72  --cloud-domain=sealos.mydomain.com
```

# 操作系统
root@iZuf6cb62qby5xh37bhzv6Z:~/sealos_5.0.1# cat /etc/os-release 
PRETTY_NAME=""Ubuntu 22.04.5 LTS""
NAME=""Ubuntu""
VERSION_ID=""22.04""
VERSION=""22.04.5 LTS (Jammy Jellyfish)""
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
UBUNTU_CODENAME=jammy



![Image](https://github.com/user-attachments/assets/2c23128d-f072-4e89-b39a-9d1c6bb59735)

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1464,true,2025-02-16 04:07:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5377,wscjxky,BUG: 禁止匿名登录，anonymous-auth=false，节点notready,"### Sealos Version

5.0.1

### How to reproduce the bug?

1. k8s==1.28.12
2. 当我禁止anonymous-auth时，master与node节点notready。   查看node上的lvscare服务，显示连不上api-server
vim etc/kubernetes/manifests/kube-apiserver.yaml
 - --anonymous-auth=false

3. curl api-server.local:6443
返回401 unauthorized

### What is the expected behavior?

希望能够正常禁止匿名登录

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: 5.0.1
- Docker version: 26.1.4
- Kubernetes version: 1.28.12
- Operating system: ubuntu 24.04
- Runtime environment: ubuntu 24.04
- Cluster size: 3 master 3 node
```

### Additional information

_No response_",642,true,2025-06-16 20:50:58,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5384,zhuyoulong,The system provides a non-root-run version of sealos,"### What is the problem this feature will solve?

用github action执行ci-patch-image.yml的工作流生成的sealos-amd64文件会报错：
`sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.38' not found (required by sealos2)
sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.33' not found (required by sealos2)
sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.34' not found (required by sealos2)
sealos2: /usr/lib64/libc.so.6: version `GLIBC_2.32' not found (required by sealos2)`

### If you have solution，please describe it

我先用你们提供的方式试试
BXY4543:
Makefile里边的 make

BXY4543:
Options:
  DEBUG            Whether or not to generate debug symbols. Default is 0.

  BINS             Binaries to build. Default is all binaries under cmd.
                   This option is available when using: make {build}(.multiarch)
                   Example: make build BINS=""sealos sealctl""

  PLATFORMS        Platform to build for. Default is linux_arm64 and linux_amd64.
                   This option is available when using: make {build}.multiarch
                   Example: make build.multiarch PLATFORMS=""linux_arm64 linux_amd64""

  V                Set to 1 enable verbose build. Default is 0.

BXY4543:
make build BINS=""sealos"" PLATFORMS=""linux_arm64 linux_amd64""


### What alternatives have you considered?

_No response_",1285,true,2025-06-19 16:30:49,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5388,houbaionee,"sealos 5.0.1 An error was reported when installing k8s 1.21 and below, and the installation of 1.27.7 was successful.","### Sealos Version

v5.0.1

### How to reproduce the bug?

1、sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.21.0 \
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.12.0 \
registry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1 \
registry.cn-shanghai.aliyuncs.com/labring/cert-manager:v1.8.0 \
 --masters 192.168.73.201  --nodes 192.168.73.202 -p centos -o  Clusterfile

2、sealos apply -f Clusterfile

### What is the expected behavior?

k8s安装成功

### What do you see instead?

k8s安装失败，以下为使用sealos v5.0.1安装k8s1.21.0的报错日志，但是使用同样的命令安装k8s1.27.7就可以成功安装k8s
报错日志如下：
W0219 23:19:27.942539   24132 strict.go:47] unknown configuration schema.GroupVersionKind{Group:""kubeadm.k8s.io"", Version:""v1beta3"", Kind:""InitConfiguration""} for scheme definitions in ""k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31"" and ""k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28""
no kind ""InitConfiguration"" is registered for version ""kubeadm.k8s.io/v1beta3"" in scheme ""k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31""
To see the stack trace of this error execute with --v=5 or higher


### Operating environment

```markdown
- Sealos version: v5.0.1
- Docker version:
- Kubernetes version:v1.21.0
- Operating system: centos7
- Runtime environment: vm
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1383,true,2025-02-24 01:51:39,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5396,fu7100,BUG: Offline mode deployment encounters problem of still pulling the network image,"### Sealos Version

5.0.0、5.0.1

### How to reproduce the bug?

1、我将离线包load到本地后，然后生成一个配置文件
[root@k8s-1 ~]# sealos images
REPOSITORY                         TAG        IMAGE ID       CREATED         SIZE
localhost/labring/kubernetes       v1.28.14   4ab085e9b7cb   4 months ago    630 MB
localhost/labring/metrics-server   v0.6.4     79507e56a004   17 months ago   30.1 MB
localhost/labring/calico           v3.24.6    3d5490e2bcb4   21 months ago   355 MB
localhost/labring/flannel          v0.20.2    af12a008a763   2 years ago     90 MB

#生成配置文件
sealos gen localhost/labring/kubernetes:v1.28.14 localhost/labring/calico:v3.24.6 --masters 192.168.80.141 -o clusterfile.yaml

2、使用生成的配置文件安装k8s，遇到如下解析image name错误
[root@k8s-1 ~]# sealos run -f clusterfile.yaml 
2025-02-21T01:04:16 info Start to create a new cluster: master [192.168.80.141], worker [], registry 192.168.80.141
2025-02-21T01:04:16 info Executing pipeline Check in CreateProcessor.
2025-02-21T01:04:16 info checker:hostname [192.168.80.141:22]
2025-02-21T01:04:16 info checker:timeSync [192.168.80.141:22]
2025-02-21T01:04:16 info checker:containerd [192.168.80.141:22]
2025-02-21T01:04:16 info Executing pipeline PreProcess in CreateProcessor.
Error: error parsing image name ""//clusterfile.yaml"": pinging container registry registry-1.docker.io: Get ""http://registry-1.docker.io/v2/"": dial tcp 31.13.96.194:80: connect: connection refused

### What is the expected behavior?

正常情况下应该是不用走外网拉镜像了，这里不知道为什么会这样

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1746,true,2025-02-21 11:17:48,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5397,zhaoyangpp,BUG: Error: payload does not match any of the supported image formats,"### Sealos Version

 5.0.1

### How to reproduce the bug?

sealos load -i xxx.tar

### What is the expected behavior?

_No response_

### What do you see instead?

[root@localhost ~]# sealos load -i 11111.tar
Getting image source signatures
Copying blob 5879b58524ff done  
Copying blob 6e76332bdae4 done  
Copying blob ce8d20826ef0 done  
Copying blob 8aa9e05fa73d done  
Getting image source signatures
Copying blob 8aa9e05fa73d done  
Copying blob 6e76332bdae4 done  
Copying blob 5879b58524ff done  
Copying blob ce8d20826ef0 done  
Error: payload does not match any of the supported image formats:
 * oci: initializing source oci:11111.tar:: open 11111.tar/index.json: not a directory
 * oci-archive: writing blob: adding layer with blob ""sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552"": processing tar fil
e(open /bin/kubeadm: permission denied): exit status 1
 * docker-archive: writing blob: adding layer with blob ""sha256:5879b58524ffc2152e79af8f80dafc5db7b539d8151959c4f01168626f986552"": processing tar 
file(open /bin/kubeadm: permission denied): exit status 1
 * dir: open 11111.tar/manifest.json: not a directory

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1380,true,2025-06-22 01:17:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5407,maybe-why-not,BUG: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first,"### Sealos Version

v5.0.1

### How to reproduce the bug?

root@ubuntu:/home/ubuntu# sealos version
SealosVersion:
  buildDate: ""2024-10-09T02:18:27Z""
  compiler: gc
  gitCommit: 2b74a1281
  gitVersion: 5.0.1
  goVersion: go1.20.14
  platform: linux/amd64

root@ubuntu:/home/ubuntu# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single
Flag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node
2025-02-25T00:38:42 info Start to create a new cluster: master [192.168.110.132], worker [], registry 192.168.110.132
2025-02-25T00:38:42 info Executing pipeline Check in CreateProcessor.
2025-02-25T00:38:42 info checker:hostname [192.168.110.132:22]
2025-02-25T00:38:42 info checker:timeSync [192.168.110.132:22]
2025-02-25T00:38:42 info checker:containerd [192.168.110.132:22]
Error: failed to run checker: containerd is installed on 192.168.110.132:22 please uninstall it first

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1396,true,2025-02-25 08:51:01,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5420,zhuyoulong,"Supports sealos gen to generate ipv6 Clusterfile, and can be successfully deployed","### What is the problem this feature will solve?

[root@k8s-host218 cluster-file]# sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.24.2 \
>   --masters [10:1:12::122] --nodes [10:1:12::236] \
>   --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123 \
>   --pk /root/.ssh/id_rsa --user=root --cluster ipv6test  > /root/cluster-file/Clusterfile-ipv6
Error: invalid ip: [10:1:12::122]

无法用ipv6地址生成 Clusterfile文件

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",614,true,2025-07-15 17:30:10,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5442,zuoFeng59556,update readme,"### Documentation Section

1

### What is the current documentation state?

https://github.com/user-attachments/assets/b0a7937f-ca81-4021-a743-9d5d605fa61d

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",294,true,2025-03-06 12:47:24,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5455,hubyao,BUG: Error: run chmod to rootfs failed: exit status 1,"### Sealos Version

v5.0.1-beta2-22d9a138f

### How to reproduce the bug?

```
curl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \
  --cloud-version=v5.0.1 \
  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \
  --proxy-prefix=$PROXY_PREFIX
```
![Image](https://github.com/user-attachments/assets/60b154ff-0ca5-4c2d-a910-5f21a2ab9b1a)

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system: Ubuntu 24.04.2 LTS
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",810,true,2025-07-08 13:20:31,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5469,BussanQ,Is it supported run app image on existing k8s clusters?,是否支持在已有k8s集群上 run app镜像,23,true,2025-03-30 05:14:59,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5488,lhsaq2009,"After installation, there is no master?","### Sealos Version

5.0.1

### How to reproduce the bug?

```
wget https://****-public.oss-cn-beijing.aliyuncs.com/sealos_5.0.1_linux_amd64.rpm \
    && rpm -ivh sealos_5.0.1_linux_amd64.rpm                                           \
    && sudo yum install -y chrony iproute socat iproute-tc jq

MASTER_NODE_1=$(hostname -I | awk '{print $1}')
WORKER_NODE_1=""${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 1 ))""
WORKER_NODE_2=""${MASTER_NODE_1%.*}.$(( ${MASTER_NODE_1##*.} + 2 ))""

# 仅需第一个 Master 节点上运行 sealos run 命令即可
# 使用内外地址大约 4 分钟 集群构建完成

# https://hub.docker.com/r/labring/kubernetes/tags
# https://hub.docker.com/r/labring/helm/tags

sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.8 \
           registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4       \
           registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.5     \
     --masters $MASTER_NODE_1                                           \
     --nodes   $WORKER_NODE_1,$WORKER_NODE_2                            \
     -p Haisen123

# 安装失败重试前，先执行清理：sealos reset

kubectl get nodes
    
    NAME      STATUS   ROLES           AGE   VERSION
    haisen1   Ready    control-plane   67s   v1.28.8
    haisen2   Ready    <none>          43s   v1.28.8
    haisen3   Ready    <none>          42s   v1.28.8
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1624,true,2025-03-17 12:30:32,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5498,lhsaq2009,How to automatically default and avoid interactive input Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue,"### What is the problem this feature will solve?

```
[root@haisen1 ~]# sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14            registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4                   registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8           --masters $MASTER_NODE_1                                                 -p Hxxxxxxen123
2025-03-23T12:46:49 info start to install app in this cluster
2025-03-23T12:46:49 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2025-03-23T12:46:49 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2025-03-23T12:46:49 info are you sure to override these following apps?
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.14
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.15.4
registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8
✗ Do you want to continue on 'haisen1' cluster? Input 'haisen1' to continue: █
```

我想在脚本里执行，但是它会阻塞住

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",1079,true,2025-03-26 03:51:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5516,dinoallo,Tracking issue for Github Actions workflow files reworking,"According to [this post](https://securitylab.github.com/resources/github-actions-preventing-pwn-requests/), the workflow files are susceptible to leaking sensitive data like secrets of repositories.
As a result, we should make the following changes to the workflow files:
1. Use [GITHUB_TOKEN](https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication) everywhere.
2. The actions should be triggered by `pull_request` or similarly less permissive event type.

This is the issue for tracking the above change. All pull requests should reference this issue.",607,true,2025-08-25 07:48:45,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5531,Zoey-Zhang922,正式应用无法拉取镜像,"### Sealos Version

网页版最新

### How to reproduce the bug?

请问我把DevBox发布成应用之后，pod一直拉镜像失败是因为啥？

<img width=""2289"" alt=""Image"" src=""https://github.com/user-attachments/assets/5a1061c6-8bd1-4927-97c8-b5a3729e8b7b"" />

<img width=""2558"" alt=""Image"" src=""https://github.com/user-attachments/assets/31fdd36e-3e7c-463a-a30a-46f9e8e65e7a"" />

<img width=""2558"" alt=""Image"" src=""https://github.com/user-attachments/assets/43ec5d0d-c8d5-4c28-a83b-ac98ea5be3fd"" />

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",777,true,2025-04-17 06:35:04,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5558,cx2c,Feature: install cloud miss devbox runtime,"### Sealos Version

v5.0.1

### How to reproduce the bug?

安装完没有devbox，应用商店也没有
![Image](https://github.com/user-attachments/assets/269830d7-0faf-4777-a45d-6d2bf9edf326)

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",494,true,2025-08-30 10:29:40,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5570,lingdie,"BUG: jaevor/go-nanoid deleted, replace to github.com/matoous/go-nanoid/v2 v2.1.0","### Sealos Version

main

### How to reproduce the bug?

_No response_

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",396,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5571,shadowlaser,BUG: install success but doesn't work,"### Sealos Version

v5.0.1

### How to reproduce the bug?

1. ubuntu 24.04  server 3台机器，使用的如下命令在一台机器上安装：
（域名+内网ip的方式安装的）
$ curl -sfL $PROXY_PREFIX/https://raw.githubusercontent.com/labring/sealos/v5.0.1/scripts/cloud/install.sh -o /tmp/install.sh && SEALOS_VERSION=v5.0.1 && bash /tmp/install.sh \
  --cloud-version=v5.0.1 \
  --image-registry=registry.cn-shanghai.aliyuncs.com --zh \
  --proxy-prefix=$PROXY_PREFIX \
  --cloud-domain=<your_domain>

2. 安装过程出现如下错误，但不影响安装执行。
Error from server (NotFound): configmaps ""desktop-frontend-config"" not found
![Image](https://github.com/user-attachments/assets/5cb2025e-e428-4335-b934-e2a9b118e770)

4. 安装完成之后，k8s查看pod日志如下：

![Image](https://github.com/user-attachments/assets/faa81804-729f-484a-b46e-62cf9590ab65)


1、安装过程出现问题：Error from server (NotFound): configmaps ""desktop-frontend-config"" not found。
是否需要解决？

2、使用域名+内网ip的访问不到，这个怎么解决？

![Image](https://github.com/user-attachments/assets/3d4db61e-75bb-4270-8cc2-f0125e8b50e4)



### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1300,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5577,monshunter,BUG: image pull failed,"### Sealos Version

5.0.1

### How to reproduce the bug?

1. In this env:
```
Linux k8s-master-0 6.8.0-58-generic #60-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:09:50 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
SealosVersion:
  buildDate: ""2024-10-09T02:18:27Z""
  compiler: gc
  gitCommit: 2b74a1281
  gitVersion: 5.0.1
  goVersion: go1.20.14
  platform: linux/arm64
```
2. With this config (cluster.yaml):
```
apiVersion: apps.sealos.io/v1beta1
kind: Cluster
metadata:
  name: default
spec:
  hosts:
    - ips:
      - ""192.168.64.15""
      roles:
        - master
        - arm64
    - ips:
      - ""192.168.64.21""
      - ""192.168.64.22""
      - ""192.168.64.23""
      roles:
        - node
        - arm64 
  image:
    - registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0
    - registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1
    - registry.cn-shanghai.aliyuncs.com/labring/cilium:1.16.3
  ssh:
    passwd: xxxx
    pk: /root/.ssh/id_rsa
    port: 22
    user: root
```
3. Run:
```
sealos apply -f cluster.yaml
```
4. See Error:
```
2025-05-05T00:06:40 info start to copy kubeconfig files to masters
2025-05-05T00:06:40 info start to copy static files to masters
2025-05-05T00:06:40 info start to init master0...
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher
2025-05-05T00:06:40 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
Error: failed to init masters: master pull image failed, error: exit status 1
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",2059,true,2025-07-24 06:47:27,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5596,dwtyxugy,"When installing, I get an error saying that port 6433 is already in use.","I'm entering the system through wsl -d Ubuntu-22.04, and this system is newly installed. I want to install a single-node version using Sealos on it. However, during the installation, it keeps saying that port 6433 is occupied.

But when I check with sudo lsof -i :6433, it shows nothing is using it. Why is that?

My installation command is:
bash ./install.sh --cloud-version=v5.0.0 --zh --single

![Image](https://github.com/user-attachments/assets/966da201-9a7b-403a-bb06-1b079670316c)",487,true,2025-08-30 10:28:23,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5604,maolchen,BUG: sealos自定义配置更新不生效,"### Sealos Version

4.3.7

### How to reproduce the bug?

对于使用sealos部署的已经运行的集群去修改配置，是不生效的。

官方文档中有关于自定义配置更新的描述“集群运行成功后会把 Clusterfile 保存到 .sealos/default/Clusterfile 文件中，可以修改其中字段来重新 apply 对集群进行变更。” 不知道这个是我理解错误，还是bug没有生效。
https://sealos.run/docs/k8s/operations/run-cluster/gen-apply-cluster 

我集群部署运行起来后，发现calico网络有些异常，需要指定一下interface，然后我到Clusterfile中添加了相关的calico配置，重新sealos apply -f Clusterfile,发现没有生效。我指定配置的方法也是参考官方文档
---
apiVersion: apps.sealos.io/v1beta1
kind: Config
metadata:
  name: calico
spec:
  path: charts/calico/values.yaml
  strategy: merge
  data: |
    installation:
      enabled: true
      kubernetesProvider: """"
      calicoNetwork:
        ipPools:
        - blockSize: 26
          cidr: 10.160.0.0/12
          encapsulation: IPIP
          natOutgoing: Enabled
          nodeSelector: all()
        nodeAddressAutodetectionV4:
          interface: ""eth0""

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1202,true,2025-05-23 06:42:43,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5608,gitfxx,podIP段默认使用了100地址，导致某些场景下网段冲突不通,"### Documentation Section

install

### What is the current documentation state?

pod内访问一个外部服务地址，该地址解析的ip也为100.64开头，结果无法正常访问，修改podIP段后才正常


### What is your suggestion?

建议默认podIP端设置为内网常见地址段，如192.168、10、172等。

### Why is this change beneficial?

_No response_

### Additional information

_No response_",302,true,2025-05-23 06:43:05,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5614,A2ureStone,BUG: data race between control plane components and host modification,"### Sealos Version

v5.0.1

### How to reproduce the bug?

I attempted to test how Sealos recovers after a failure of the master0 node. During this, I discovered an issue: on control plane nodes other than master0, the host file on the host machine and the host file inside the pods are inconsistent. For example, in the controller-manager pod, when master0 becomes unavailable, controller-manager fails. The same behavior is observed in other control plane components as well.
### Reproduce
The cluster is configured with three control plane nodes: master1, master2, and master3, and one worker node node1. The cluster was started with the following command:
```sh
root@master1:~# sealos gen registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 \
     --masters 192.168.64.15,192.168.64.16,192.168.64.17 \
     --nodes 192.168.64.18 \
     -u root --pk='/root/.ssh/multipass_key' \
     --output Clusterfile
sealos apply -f Clusterfile
```
Then shutdown the master1. Below is the output from controller-manager on master2 after master1 (master0) went offline. As you can see, the DNS resolution of apiserver.cluster.local points to master1 (master0).
```sh
root@master2:~# kubectl logs -n kube-system kube-controller-manager-master2 | tail -n10
E0526 13:58:46.446458       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:58:52.590254       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:58:58.736356       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:01.811818       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:04.879391       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:11.023681       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:14.104059       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:17.166370       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:20.240788       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
E0526 13:59:26.395983       1 leaderelection.go:332] error retrieving resource lock kube-system/kube-controller-manager: Get ""https://apiserver.cluster.local:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"": dial tcp 192.168.64.15:6443: connect: no route to host
```
By directly inspecting the hosts file under /var/lib/kubelet/pods, I confirmed this was indeed the case.
```sh
root@master2:~# grep -i ""apiserver.cluster.local"" -r /var/lib/kubelet/pods/
/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/etc-hosts:192.168.64.15 apiserver.cluster.local
/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/etc-hosts:192.168.64.15 apiserver.cluster.local
/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/etc-hosts:192.168.64.15 apiserver.cluster.local
/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/etc-hosts:192.168.64.15 apiserver.cluster.local
/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/volumes/kubernetes.io~configmap/kube-proxy/..2025_05_26_02_51_19.923862820/kubeconfig.conf:    server: https://apiserver.cluster.local:6443
/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/etc-hosts:192.168.64.15 apiserver.cluster.local
/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/etc-hosts:192.168.64.16 apiserver.cluster.local
root@master2:~# ls /var/lib/kubelet/pods/*/containers
/var/lib/kubelet/pods/14812d81b7c93b918d4faed7ae4a6dcf/containers:
kube-scheduler

/var/lib/kubelet/pods/362d9138-58c5-4961-b8b9-39d9aa57f8d7/containers:
coredns

/var/lib/kubelet/pods/56a1a6061487d03b440de1b2e6d4cba5/containers:
kube-controller-manager

/var/lib/kubelet/pods/a27a8174-6ac0-4f5d-81fe-17a05a525c43/containers:
kube-proxy

/var/lib/kubelet/pods/a9d78507a0d74897c9c340c682a3413f/containers:
etcd

/var/lib/kubelet/pods/cfb54c6d-8bd4-4533-b2e1-8b9b74d47e43/containers:
apply-sysctl-overwrites  cilium-agent  clean-cilium-state  config  install-cni-binaries  mount-bpf-fs  mount-cgroup

/var/lib/kubelet/pods/e669d082174b1b8e93a3b80fa1a4a2b9/containers:
kube-apiserver

/var/lib/kubelet/pods/e9afef92-76ae-461a-9fde-f105f5521e07/containers:
coredns
```
We can see that pod 56a1a6061487d03b440de1b2e6d4cba5 is kube-controller-manager with host record
192.168.64.15 apiserver.cluster.local.
However, the host machine’s /etc/hosts file looks like this:
```sh
root@master2:~# cat /etc/hosts
# Your system has configured 'manage_etc_hosts' as True.
# As a result, if you wish for changes to this file to persist
# then you will need to either
# a.) make changes to the master file in /etc/cloud/templates/hosts.debian.tmpl
# b.) change or remove the value of 'manage_etc_hosts' in
#     /etc/cloud/cloud.cfg or cloud-config from user-data
#
127.0.1.1 master2 master2
127.0.0.1 localhost
# The following lines are desirable for IPv6 capable hosts
::1 localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.64.15 sealos.hub
192.168.64.16 apiserver.cluster.local
```
And not all control plane component pods have their apiserver.cluster.local entry pointing to master1 (master0) — some point to the host machine instead.
After reading parts of Sealos’ source code, I suspect this inconsistency is caused by a data race. During the initialization of control plane nodes, Sealos modifies the host machine’s /etc/hosts file twice: the first time during the init phase, it checks whether the node is a master, and if so, it points to master0. This makes sense because during join master, the node locates master0 through apiserver.cluster.local. The second modification happens after the kubeadm join command completes, at which point it only waits for the API server to become ready, but not all control plane components.
The first host modification in init stage
```go
defaultInitializers = append(defaultInitializers, &registryHostApplier{}, &registryApplier{}, &defaultCRIInitializer{}, &apiServerHostApplier{}, &lvscareHostApplier{}, &defaultInitializer{})

func (a *apiServerHostApplier) Apply(ctx Context, host string) error {
    if slices.Contains(ctx.GetCluster().GetMasterIPAndPortList(), host) {
    	if err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetMaster0IP(), constants.DefaultAPIServerDomain); err != nil {
    		return fmt.Errorf(""failed to add hosts: %v"", err)
    	}
    	return nil
    }
    if err := ctx.GetRemoter().HostsAdd(host, ctx.GetCluster().GetVIP(), constants.DefaultAPIServerDomain); err != nil {
    	return fmt.Errorf(""failed to add hosts: %v"", err)
    }
    
    return nil
}
```
The second host modification after kubeadm join
```go
func (k *KubeadmRuntime) joinMasters(masters []string) error {
    // ...
    err = k.sshCmdAsync(master, joinCmd)
    if err != nil {
    	return fmt.Errorf(""exec kubeadm join in %s failed %v"", master, err)
    }
    
    err = k.execHostsAppend(master, master, k.getAPIServerDomain())
    if err != nil {
    	return fmt.Errorf(""add master0 apiserver domain hosts in %s failed %v"", master, err)
    }
    // ...
}
```
The possible cases
```
case1 apiserver.cluster.local points to master0: first host modification --> pod start --> second host modification
case2 apiserver.cluster.local points to host machine: first host modification --> second host modification --> pod start 
```
Here is the doc about behavior of [kubeadm join](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#feature-gates)
```
Without the feature gate enabled, kubeadm will only wait for the kube-apiserver on a control plane node to become ready. 
The wait process starts right after the kubelet on the host is started by kubeadm. 
You are advised to enable this feature gate in case you wish to observe a ready state from all control plane components 
during the kubeadm init or kubeadm join command execution.
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",10321,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5616,notionev,BUG:sealos 重启机器后 集群无法自动重启,"### Sealos Version

4.3.7

### How to reproduce the bug?

reboot集群后 ，k8s无法自动起来，并且找到不到 k8s ,reset 之后重新 run 报错 warn failed to copy image 127.0.0.1:40945/coredns:1.7.0: copying system image from manifest list: trying to reuse blob sha256:c6568d217a0023041ef9f729e8836b19f863bcdb612bb3a329ebc165539f5a80 at destination: pinging container registry 192.31.16.161:5050: received unexpected HTTP status: 503 Service Unavailable


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",746,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5634,maqun02,One-click to start or stop billing,"### What is the problem this feature will solve?

Press the button to stop all projects at once to ensure no financial expenses.

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",247,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5637,yifeng-x,离线部署sealos，镜像拉取失败导致部署失败,"按照官方文档进行离线部署，发现sealos run执行中，仍然会从远端拉取kube相关的镜像，请问该如何处理

![Image](https://github.com/user-attachments/assets/1a8ceffb-530c-4042-9299-0056b457d06f)",145,true,2025-07-24 06:47:27,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5638,yifeng-x,The latest version ( v5.0.1 ) of the sealos deployment k8s will fail due to a public image pull,"### Sealos Version

v5.0.1

### How to reproduce the bug?

1.sealos version
v5.0.1

![Image](https://github.com/user-attachments/assets/f5f5358c-6158-476a-b9df-ed182d165e16)
2.linux Operating system kernel version:

![Image](https://github.com/user-attachments/assets/6f6c93c5-955d-4bc9-a4bc-c7e8a7f8aff8)

3.sealos run --debug registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.11 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.15.8

4.error

![Image](https://github.com/user-attachments/assets/0c859efa-f0c9-43d3-b603-04e411040f14)

5.两周前是可以部署的，最近一直报上面的错误，我理解这个应该是个bug，也不知道如何配置能够让sealos 拉取本地镜像，使用sealos pull 可以正常拉取kube相关镜像

![Image](https://github.com/user-attachments/assets/0f03d4c8-d09b-49f1-b36e-586f18cde874)


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1110,true,2025-07-30 06:12:02,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5640,JocelynFloresz,Feature: 什么时候支持Flutter,"### What is the problem this feature will solve?

none

### If you have solution，please describe it

none

### What alternatives have you considered?

none",155,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5642,zhuyoulong,BUG: sealos自定义安装，k8s版本v1.33.1部署集群失败,"### Sealos Version

5.0.1

### How to reproduce the bug?

liunx环境
生成Clusterfile文件命令：
`sealos gen  harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \
			harbor.paas.nl:10081/paas_public/kube-system/pause:3.10 \
			harbor.paas.nl:10081/paas_public/kube-system/kube-apiserver:v1.33.1 \
			harbor.paas.nl:10081/paas_public/kube-system/kube-controller-manager:v1.33.1 \
			harbor.paas.nl:10081/paas_public/kube-system/kube-scheduler:v1.33.1 \
			harbor.paas.nl:10081/paas_public/kube-system/kube-proxy:v1.33.1 \
			harbor.paas.nl:10081/paas_public/kube-system/coredns:v1.12.0 \
            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \
            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \
            --masters 10.1.21.175 --nodes 10.1.21.176 \
            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \
            --pk /root/.ssh/id_rsa --user=root --cluster test3  > /root/cluster-file/test3/Clusterfile`

其中kubernetes-sealos:v1.33.1-5.0.1是拉取了docker pull labring/kubernetes:v1.33.1-5.0.1 这个镜像传到我自己的harbor仓库中的地址

执行后报错：

<img width=""1499"" alt=""Image"" src=""https://github.com/user-attachments/assets/172d263c-c36d-462c-a25c-5efc815c843f"" />

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:5.0.1
- Docker version:无
- Kubernetes version:1.33.1
- Operating system:centos7
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1647,true,2025-07-30 06:08:46,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5644,zhuyoulong,sealos部署k8s v1.32.5集群错误,"### Sealos Version

v5.0.1

### How to reproduce the bug?

拉取的版本是：docker pull labring/kubernetes:v1.33.1-5.0.1 到自己的harbar仓库中

执行
`sealos gen harbor.paas.nl:10081/paas_public/kube-system/kubernetes-sealos:v1.33.1-5.0.1 \
            harbor.paas.nl:10081/paas_public/kube-system/helm:v3.10.3 \
            harbor.paas.nl:10081/paas_public/kube-system/calico:v3.25.0 \
            --masters 10.1.21.175 --nodes 10.1.21.176 \
            --env criData=/paas/containerd,registryDomain=harbor.paas.nl,registryUsername=admin,registryPassword=Nlpaas123,registryPort1:80,registryPort1:10081 \
            --pk /root/.ssh/id_rsa --user=root --cluster test131  > /root/cluster-file/test131/Clusterfile`

然后执行：
sealos apply -f Clusterfile --debug

最后报错：
`Error: failed to init masters: master pull image failed, error: run command `kubeadm config images pull --cri-socket unix:///var/run/image-cri-shim.sock  --kubernetes-version v1.32.5  -v 6` on 10.1.21.175:22, output: I0610 18:39:09.550114  588091 interface.go:432] Looking for default routes with IPv4 addresses
I0610 18:39:09.550175  588091 interface.go:437] Default route transits interface ""ens192""
I0610 18:39:09.550494  588091 interface.go:209] Interface ens192 is up
I0610 18:39:09.550554  588091 interface.go:257] Interface ""ens192"" has 3 addresses :[10.1.21.175/24 10:1:21::175/64 fe80::250:56ff:fe9f:2775/64].
I0610 18:39:09.550570  588091 interface.go:224] Checking addr  10.1.21.175/24.
I0610 18:39:09.550579  588091 interface.go:231] IP found 10.1.21.175
I0610 18:39:09.550602  588091 interface.go:263] Found valid IPv4 address 10.1.21.175 for interface ""ens192"".
I0610 18:39:09.550611  588091 interface.go:443] Found active IP 10.1.21.175
I0610 18:39:09.550630  588091 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to ""systemd""
validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
failed to create new CRI runtime service
k8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect
        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:387
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
k8s.io/kubernetes/cmd/kubeadm/app.Run
        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47
main.main
        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
, error: Process exited with status 1,
`

我用1.29.9不会报错，1.32.5和1.33.1都会报同样的错误
containerd版本是：v1.6.15

错误如何解决呢

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",3284,true,2025-07-24 06:47:27,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5652,cnfatal,Feature: Allow save multi-arch images on `sealos build`,"### What is the problem this feature will solve?

The [build-multi-arch-image](https://sealos.run/docs/k8s/operations/build-image/build-multi-arch-image) is designed to build images targeting multi-architecture clusters. However, the images currently loaded into the cluster are single-architecture only. As a result, in mixed-architecture clusters, nodes may pull images that do not match their architecture, leading to unexpected behavior.

### If you have solution，please describe it

To address this, I propose introducing a flag such as `--multi-arch` or `--save-multi-arch` to allow saving multi-architecture images at build time, it call into sreg and use `copy.CopyAllImages`  to save images.

I’ve done some preliminary work in this area:

- https://github.com/labring/sealos/compare/main...cnfatal:sealos:main

- https://github.com/labring/sreg/compare/main...cnfatal:sreg:main

### What alternatives have you considered?

_No response_",946,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5657,912988434,BUG: sealos v5.0.1 从v1.30.x升级到1.31.x失败，can not mix '--config' with arguments [certificate-renewal],"### Sealos Version

v5.0.1

### How to reproduce the bug?

从v1.30.13 升级到v1.31.0出现can not mix '--config' with arguments [certificate-renewal]
root@k8s-1:~/.sealos/default# sealos  run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0
2025-06-19T07:37:14 info start to install app in this cluster
2025-06-19T07:37:14 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2025-06-19T07:37:14 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2025-06-19T07:37:14 info are you sure to override these following apps? 
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.0
Do you want to continue on 'k8s-1' cluster? Input 'k8s-1' to continue: k8s-1
2025-06-19T07:37:17 info Executing PreProcess Pipeline in InstallProcessor
2025-06-19T07:37:17 info Executing pipeline MountRootfs in InstallProcessor.
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
2025-06-19T07:37:20 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
192.168.2.102:22es to 192025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
192.168.2.102:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
192.168.2.103:22        2025-06-19T07:37:47 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
2025-06-19T07:37:47 info Executing pipeline MirrorRegistry in InstallProcessor.
2025-06-19T07:37:47 info trying default http mode to sync images to hosts [192.168.2.101:22]
2025-06-19T07:37:49 info Executing UpgradeIfNeed Pipeline in InstallProcessor
2025-06-19T07:37:49 info Change ClusterConfiguration up to newVersion if need.
2025-06-19T07:37:49 info start to upgrade master0
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.31.0
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.31.0
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.31.0
[config/images] Pulled registry.k8s.io/kube-proxy:v1.31.0
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.11.3
[config/images] Pulled registry.k8s.io/pause:3.9
[config/images] Pulled registry.k8s.io/etcd:3.5.15-0
can not mix '--config' with arguments [certificate-renewal]
To see the stack trace of this error execute with --v=5 or higher
2025-06-19T07:37:54 error upgrade cluster failed
Error: exit status 1

换成1.31.10 也是一样的
root@k8s-1:~/.sealos/default# sealos  run labring/kubernetes:v1.31.10
2025-06-19T07:41:54 info start to install app in this cluster
2025-06-19T07:41:54 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2025-06-19T07:41:54 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2025-06-19T07:41:54 info Executing PreProcess Pipeline in InstallProcessor
Trying to pull docker.lishuai.fun/labring/kubernetes:v1.31.10...
Getting image source signatures
Copying blob 98e5ac018847 done  
Copying blob 2587c72eb500 done  
Copying blob d9dc5de7feeb done  
Copying blob f4b501a28252 done  
Copying config 792e378009 done  
Writing manifest to image destination
Storing signatures
2025-06-19T07:42:59 info Executing pipeline MountRootfs in InstallProcessor.
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
2025-06-19T07:43:00 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
192.168.2.103:22es to 192025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
192.168.2.103:22        2025-06-19T07:43:30 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
192.168.2.102:22es to 192025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/config.toml from /var/lib/sealos/data/default/rootfs/etc/config.toml.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/containerd.service from /var/lib/sealos/data/default/rootfs/etc/containerd.service.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/hosts.toml from /var/lib/sealos/data/default/rootfs/etc/hosts.toml.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.service.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml from /var/lib/sealos/data/default/rootfs/etc/image-cri-shim.yaml.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/kubelet.service from /var/lib/sealos/data/default/rootfs/etc/kubelet.service.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.service from /var/lib/sealos/data/default/rootfs/etc/registry.service.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry.yml from /var/lib/sealos/data/default/rootfs/etc/registry.yml.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/registry_config.yml from /var/lib/sealos/data/default/rootfs/etc/registry_config.yml.tmpl completed
192.168.2.102:22        2025-06-19T07:43:31 info render /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf from /var/lib/sealos/data/default/rootfs/etc/systemd/system/kubelet.service.d/10-kubeadm.conf.tmpl completed
2025-06-19T07:43:31 info Executing pipeline MirrorRegistry in InstallProcessor.
2025-06-19T07:43:31 info trying default http mode to sync images to hosts [192.168.2.101:22]
2025-06-19T07:43:32 info Executing UpgradeIfNeed Pipeline in InstallProcessor
2025-06-19T07:43:32 info Change ClusterConfiguration up to newVersion if need.
2025-06-19T07:43:32 info start to upgrade master0
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher
2025-06-19T07:43:33 warn image pull pre-upgrade failed: master pull image failed, error: exit status 1
can not mix '--config' with arguments [certificate-renewal]
To see the stack trace of this error execute with --v=5 or higher
2025-06-19T07:43:33 error upgrade cluster failed
Error: exit status 1
感觉v5.0.1 这个版本好久没更新了

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: v5.0.1
- Docker version:
- Kubernetes version: 1.30.13
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",15234,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5661,yanweiyi11,注销账号后，再次登录一直显示验证码错误，怎么解决？,不能重新注册账号，也不能恢复账号了吗？没有在页面上看到有任何提示呢,33,true,2025-08-06 02:16:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5663,fengjing1009,BUG: Sealos集群镜像同步失败,"### Sealos Version

5.0.1

### How to reproduce the bug?

[root@gpustack-server01 ~]# sealos version
SealosVersion:
  buildDate: ""2024-10-09T02:18:27Z""
  compiler: gc
  gitCommit: 2b74a1281
  gitVersion: 5.0.1
  goVersion: go1.20.14
  platform: linux/amd64

### What is the expected behavior?

参考文档进行操作 https://sealos.run/docs/k8s/operations/registry/sealos_sync_backup_solution
sealos pull labring/kubernetes:v1.24.0 
sealos create labring/kubernetes:v1.24.0
sealos registry serve filesystem -p 9090 registry
sealos login sealos.hub:5000
sealos registry sync 127.0.0.1:9090 sealos.hub:5000

### What do you see instead?

同步操作失败
[root@gpustack-server01 ~]# sealos --debug  registry sync 127.0.0.1:9090 http://sealos.hub:5000
2025-06-25T16:25:34 debug checking if endpoint http://sealos.hub:5000/v2/ is alive
2025-06-25T16:25:34 debug checking if endpoint http://127.0.0.1:9090/v2/ is alive
2025-06-25T16:25:34 debug http endpoint http://127.0.0.1:9090/v2/ is alive
Error: registry http status code not 200

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1235,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1131,camilamacedo86,:warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy,"
## Description

:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**
**You must move as soon as possible, sometime from early 2025, the GCR will go away.**

> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._

- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**
  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**

- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.

## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?

[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).

This feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.

### What To Do?

**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**
- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):
  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._
- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.

**For further information, suggestions, and guidance:**
- 📖 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)
- 💬 Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).

> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._

We sincerely apologize for any inconvenience this may cause.

**Thank you for your cooperation and understanding!** :pray:
",3246,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5667,shunzi115,sealos 如何构建离线kubesphere 部署镜像,"### Documentation Section

sealos 如何构建离线kubesphere 部署镜像

### What is the current documentation state?

sealos 如何构建离线kubesphere 部署镜像

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",270,true,2025-08-05 07:48:39,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5668,allenonline,Can Sealos set proxy address to connect docker.io?,"### What is the problem this feature will solve?

As u know,  u can not access docker.io directly in China. 
so I got an error when I use sealos build

> Storing signatures
> WARN[0123] Failed, retrying in 1s ... (1/3). Error: initializing source docker://alpine:latest: pinging container registry registry-1.docker.io: Get ""http://registry-1.docker.io/v2/"": dial tcp 75.126.164.178:80: i/o timeout 

Can I set a proxy address to connect docker.io in Sealos? How can I do that?

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",596,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5676,qq547475331,BUG: 没有手机号登陆了,"### Sealos Version

v4.1.3

### How to reproduce the bug?

<img width=""1496"" alt=""Image"" src=""https://github.com/user-attachments/assets/474da4fb-a54c-457f-97ab-32b69d8a862f"" />

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",503,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5677,Axm11,BUG: failed to init masters: master pull image failed,"### Sealos Version

v5.0.1

### How to reproduce the bug?

Base env:
    a. OS version: Centos 7
    b. Kernel version: 5.4.226-1.el7.elrepo.x86_64
    c. master * 1

CriVersionInfo:
  RuntimeApiVersion: v1
  RuntimeName: containerd
  RuntimeVersion: v1.7.27
  Version: 0.1.0

SealosVersion:
  buildDate: ""2024-10-09T02:18:27Z""
  compiler: gc
  gitCommit: 2b74a1281
  gitVersion: 5.0.1
  goVersion: go1.20.14
  platform: linux/amd64

How to reproduct(offline install):
    a. sealos pull labring/kubernetes:v1.31.0
    b. sealos save ... && copy by manual
    c. sealos load
    d. sealos run labring/kubernetes:v1.31.0 -e cirData=/data/containerd --debug=true --single

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown

```

### Additional information

SOME LOGS
```
2025-07-03T22:23:00 info start to copy static files to masters
2025-07-03T22:23:00 info start to init master0...
I0703 22:23:00.622532    6445 interface.go:432] Looking for default routes with IPv4 addresses
I0703 22:23:00.622588    6445 interface.go:437] Default route transits interface ""ens160""
I0703 22:23:00.623008    6445 interface.go:209] Interface ens160 is up
I0703 22:23:00.623063    6445 interface.go:257] Interface ""ens160"" has 4 addresses :[172.30.0.135/24 fe80::a1ed:122:73a0:f8ed/64 fe80::9d3b:4bf8:7733:7810/64 fe80::f6d0:5eef:587f:b799/64].
I0703 22:23:00.623081    6445 interface.go:224] Checking addr  172.30.0.135/24.
I0703 22:23:00.623100    6445 interface.go:231] IP found 172.30.0.135
I0703 22:23:00.623117    6445 interface.go:263] Found valid IPv4 address 172.30.0.135 for interface ""ens160"".
I0703 22:23:00.623125    6445 interface.go:443] Found active IP 172.30.0.135 
I0703 22:23:00.623148    6445 kubelet.go:195] the value of KubeletConfiguration.cgroupDriver is empty; setting it to ""systemd""
validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
failed to create new CRI runtime service
k8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).Connect
        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:83
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:392
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
k8s.io/kubernetes/cmd/kubeadm/app.Run
        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47
main.main
        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
        runtime/proc.go:271
runtime.goexit
        runtime/asm_amd64.s:1695
2025-07-03T22:23:00 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
2025-07-03T22:23:00 debug save objects into local: /root/.sealos/default/Clusterfile, objects: apiVersion: apps.sealos.io/v1beta1
kind: Cluster
metadata:
  creationTimestamp: ""2025-07-03T14:23:00Z""
  name: default
spec:
  hosts:
  - ips:
    - 172.30.0.135:22
    roles:
    - master
    - amd64
  image:
  - labring/kubernetes:v1.31.0
  ssh: {}
status:
  conditions:
  - lastHeartbeatTime: ""2025-07-03T14:23:00Z""
    message: 'failed to init masters: master pull image failed, error: exit status
      1'
    reason: Apply Cluster
    status: ""False""
    type: ApplyClusterError
  mounts:
  - env:
      SEALOS_SYS_CRI_ENDPOINT: /var/run/containerd/containerd.sock
      SEALOS_SYS_IMAGE_ENDPOINT: /var/run/image-cri-shim.sock
      cirData: /data/containerd
      criData: /var/lib/containerd
      defaultVIP: 10.103.97.2
      disableApparmor: ""false""
      registryConfig: /etc/registry
      registryData: /var/lib/registry
      registryDomain: sealos.hub
      registryPassword: passw0rd
      registryPort: ""5000""
      registryUsername: admin
      sandboxImage: pause:3.10
    imageName: labring/kubernetes:v1.31.0
    labels:
      check: check.sh $registryData
      clean: clean.sh && bash clean-cri.sh $criData
      clean-registry: clean-registry.sh $registryData $registryConfig
      image: ghcr.io/labring/lvscare:v5.0.1
      init: init-cri.sh $registryDomain $registryPort && bash init.sh
      init-registry: init-registry.sh $registryData $registryConfig
      io.buildah.version: 1.30.0
      org.opencontainers.image.description: kubernetes-v1.31.0 container image
      org.opencontainers.image.licenses: MIT
      org.opencontainers.image.source: https://github.com/labring-actions/cache
      sealos.io.type: rootfs
      sealos.io.version: v1beta1
      version: v1.31.0
      vip: $defaultVIP
    mountPoint: /var/lib/containers/storage/overlay/e72dc732ee4dd7e9eea09ec5f3dea9a6d8c1435968e05317c1962bdb8c0748f7/merged
    name: default-7oh654x7
    type: rootfs
  phase: ClusterFailed

2025-07-03T22:23:00 debug sync workdir: /root/.sealos/default
2025-07-03T22:23:00 debug copy files src /root/.sealos/default to dst /root/.sealos/default on 172.30.0.135:22 locally
Error: failed to init masters: master pull image failed, error: exit status 1
```",5306,true,2025-07-24 06:47:27,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5682,xiao1wen2,coredns解析异常,"### Sealos Version

v4.3.7

### How to reproduce the bug?

1. sealos使用的4.3.7版本；kubernetes安装的是1.20.15版本，coredns安装的是1.8.4版本；系统版本：ubuntu20.04；系统内核版本：5.4.0-128-generic
2. 1个master节点
3. 出现的问题：在pod中解析svc时  超时
root@master-01:~# kubectl exec -it -n rel-apollo kjtsyxzx-msa-apollo-rv-portal-5f94b47d4b-rhw5v -- sh
/opt/msa # nslookup hd-apollo.hd-apollo.svc.cluster.local
;; connection timed out; no servers could be reached


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",742,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5687,xcbeyond,BUG: MacOS arm环境sealos二进制文件执行失败,"### Sealos Version

v5.0.1

### How to reproduce the bug?

macOS arm环境

从 https://github.com/labring/sealos/releases/download/v5.0.1/sealos_5.0.1_linux_arm64.tar.gz 下载的包

执行sealos命令失败：

```
./sealos 
zsh: exec format error: ./sealos
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",562,true,2025-07-22 15:16:20,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5688,bestgopher,BUG: deploy k8s cluster failed,"### Sealos Version

v5.0.0

### How to reproduce the bug?

<img width=""1902"" height=""78"" alt=""Image"" src=""https://github.com/user-attachments/assets/b6ce5ee7-3c61-4840-ae71-497d9deb0b76"" />

```shell
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher
2025-07-09T10:40:38 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
Error: failed to init masters: master pull image failed, error: exit status 1
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1028,true,2025-07-30 03:20:06,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5689,neo515,又见崩溃,"",0,true,2025-08-07 06:55:39,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5694,zinda2000,【紧急求助】Sealos 安装的 K8s v1.27.7 集群，kube-apiserver 启动参数被强制固定，无法修改,"1.  **核心问题**：
    *   `kube-apiserver` 告警 `KubeAPIServerLatencyHigh`。
    *   排查发现，根本原因是 `enable-admission-plugins` 参数被错误地设置为 `NodeRestriction`，导致 `validatingadmissionwebhookconfigurations` 等资源类型不存在。
2.  **已尝试的、无效的修复手段**：
    *   **证据一**：直接修改 `/etc/kubernetes/manifests/kube-apiserver.yaml` 文件为正确配置后，强制删除 Pod (`kubectl delete pod ...`)，重建的容器通过 `crictl inspect` 查看，其启动参数**依然是错误的**。
    *   **证据二**：修改 `kube-system/kubeadm-config` `ConfigMap` 为正确配置后，运行 `kubeadm upgrade apply ...` 命令，虽然命令本身宣告成功，但问题**依旧存在**。
    *   **证据三**：修改 `containerd` 的配置以解决私有仓库访问问题后，`kubeadm` 依然无法解决根本问题。
    *   **证据四**：检查了 `/var/lib/sealos/data/default/rootfs/scripts/` 目录下的脚本，没有发现直接覆盖配置的行为。重启 `kubelet` 也**无效**。
    *   **证据五**：尝试通过 `kubectl edit clusters.apps.sealos.io` 来修改 Sealos 的 CR，但发现该 CRD **不存在**。
 
## 请问，Sealos 是通过什么机制来保证 `kube-apiserver` 的配置的？这个机制的配置文件在哪里？我应该如何正确地、永久性地修改 `kube-apiserver` 的启动参数？",880,true,2025-07-18 23:59:04,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5697,chenbsun,sealos怎么将它所在的节点delete,"源master 1  sealos  192.168.0.10
源master 2 192.168.0.11
源master 3 192.168.0.12
想通过新增节点剔除旧节点的方式完成节点更换，问题是源master 1怎么剔除，sealos就在这个节点上，最终结果希望是：
新master 1 sealos  192.168.0.13
新master 2 192.168.0.14
新master 3 192.168.0.15",216,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5711,Xiaomajohn,BUG: 域名配置,"### Sealos Version

v5.0.1

### How to reproduce the bug?

1、fedora环境，安装5.0.1版本，安装之后，使用自签名证书：IP.nip.ip

正常访问该IP.nip.io,但是使用用户名和密码登录后，点击应用管理，跳转到了新的域名中
提示：
网址为 https://applaunchpad.IP.nip.io/ 的网页可能暂时无法连接，或者它已永久性地移动到了新网址。

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",544,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5721,cuisongliu,BUG: ci error sealos container build,"### Sealos Version

main

### How to reproduce the bug?

https://github.com/labring/sealos/actions/runs/16489814949/job/46622113359

ci error

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",467,true,2025-08-02 09:46:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1143,cuisongliu,【Auto-build】oceanbase-operator,"```
Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
",186,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5722,cuisongliu,Feature: change sealos LICENSE task,"### What is the problem this feature will solve?

https://github.com/labring/sealos/pull/5719

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",212,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5747,Conmi-WhiteJoker,想问问未来会不会像kubesphere这样删库？,刚经历ks删库，不知道敢不敢入手这个架构,20,true,2025-08-05 08:54:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5755,cuisongliu,Feature: bump golangci-lint to v2.3.0,"## PR: Bump golangci-lint to v2.3.1

need close #5755

### 📋 Task Assignment & Completion Status

#### Services Group
| Status | Module | Assignee |
|--------|--------|----------|
| ✅ | golangci-lint (./service/devbox) | @zijiren233 |
| ✅ | golangci-lint (./service/vlogs) | @zijiren233 |
| ✅ | golangci-lint (./service/pay) | @zijiren233 |
| ✅ | golangci-lint (./service/exceptionmonitor) | @zijiren233 |
| ✅ | golangci-lint (./service) | @zijiren233 |
| ⬜ | golangci-lint (./service/account) | @bxy4543 |
| ✅ | golangci-lint (./service/database) | @zijiren233 |
| ✅ | golangci-lint (./service/launchpad) | @zijiren233 |

#### Controllers Group
| Status | Module | Assignee |
|--------|--------|----------|
| ✅ | golangci-lint (./controllers/db/adminer) | @zijiren233 |
| ✅ | golangci-lint (./controllers/devbox) | @zijiren233 |
| ✅ | golangci-lint (./controllers/pkg) | @zijiren233 |
| ✅ | golangci-lint (./controllers/resources) | @zijiren233 |
| ✅ | golangci-lint (./controllers/user) | @zijiren233 |
| ✅ | golangci-lint (./controllers/terminal) | @lingdie |
| ✅ | golangci-lint (./controllers/job/init) | @zijiren233 |
| ✅ | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |
| ✅ | golangci-lint (./controllers/app) | @zijiren233 |
| ✅ | golangci-lint (./controllers/license) | @zijiren233 |
| ✅ | golangci-lint (./controllers/objectstorage) | @zijiren233 |
| ✅ | golangci-lint (./controllers/db) | @zijiren233 |
| ✅ | golangci-lint (./controllers/account) | @zijiren233 |
| ✅ | golangci-lint (./controllers/node) | @zijiren233 |

#### Kubernetes Group
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./lifecycle) | @cuisongliu  |

*Note: Replace ⬜ with ✅ when completed*

### 🛠️ Local Testing & Quick Fix Guide

To test and fix lint issues locally:

```bash
# Navigate to your module directory
cd controllers/account

# Run linter to check for issues
golangci-lint-v2 run --color=always --config=../../.golangci.yml

# Auto-fix issues (only for linters that support --fix)
golangci-lint-v2 run --color=always --config=../../.golangci.yml --fix
```

**Note:** The `--fix` flag only works for certain linters. Manual fixes may still be required for some issues.",2209,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5756,cuisongliu,Feature: bump golangci-lint to v2.3.0 service/devbox,"## PR: Bump golangci-lint to v2.3.0

### 📋 Task Assignment & Completion Status

#### Services Group
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/devbox) | @bxy4543 |
",214,true,2025-08-20 09:49:39,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5758,zijiren233,Feature: bump golangci-lint to v2.3.0 service/vlogs,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/vlogs) | @zijiren233 |",152,true,2025-08-30 10:32:05,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5759,zijiren233,Feature: bump golangci-lint to v2.3.0 service/pay,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/pay) | @zijiren233 |",150,true,2025-08-20 09:49:13,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5760,zijiren233,Feature: bump golangci-lint to v2.3.0 service/exceptionmonitor,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/exceptionmonitor) | @zijiren233 |",163,true,2025-08-20 09:49:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5761,zijiren233,Feature: bump golangci-lint to v2.3.0 service,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service) | @zijiren233 |",146,true,2025-08-30 10:31:57,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5762,zijiren233,Feature: bump golangci-lint to v2.3.0 service/account,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/account) | @bxy4543 |",151,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5763,zijiren233,Feature: bump golangci-lint to v2.3.0 service/database,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/database) | @zijiren233 |",155,true,2025-08-20 09:49:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5764,zijiren233,Feature: bump golangci-lint to v2.3.0 service/launchpad,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./service/launchpad) | @zijiren233 |",156,true,2025-08-20 09:49:14,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5765,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/db/adminer,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/db/adminer) | @zijiren233 |",161,true,2025-08-20 09:49:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5766,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/devbox,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/devbox) | @zijiren233 |",157,true,2025-08-20 09:49:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5767,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/pkg,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/pkg) | @zijiren233 |",154,true,2025-08-20 09:49:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5768,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/resources,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/resources) | @zijiren233 |",160,true,2025-08-20 09:49:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5769,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/user,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/user) | @zijiren233 |",155,true,2025-08-20 09:49:16,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5770,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/terminal,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/terminal) | @lingdie |",156,true,2025-08-20 09:49:16,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5771,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/job/init,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/job/init) | @zijiren233 |",159,true,2025-08-20 09:49:16,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5772,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/job/heartbeat,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/job/heartbeat) | @zijiren233 |",164,true,2025-08-20 09:49:17,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1162,relleo,【Auto-build】kubesphere,"```
Usage:
   /imagebuild_apps kubesphere v3.3.2
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
",185,true,2025-01-08 10:37:50,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5773,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/app,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/app) | @zijiren233 |",154,true,2025-08-20 09:49:17,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5774,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/license,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/license) | @zijiren233 |",158,true,2025-08-20 09:49:17,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5775,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/objectstorage,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/objectstorage) | @zijiren233 |",164,true,2025-08-20 09:49:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5776,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/db,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/db) | @zijiren233 |",153,true,2025-08-20 09:49:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5777,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/account,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/account) | @zijiren233 |",158,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5778,zijiren233,Feature: bump golangci-lint to v2.3.0 controllers/node,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./controllers/node) | @zijiren233 |",155,true,2025-08-20 09:49:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5779,zijiren233,Feature: bump golangci-lint to v2.3.0 lifecycle,"**PR: Bump golangci-lint to v2.3.0**
| Status | Module | Assignee |
|--------|--------|----------|
| ⬜ | golangci-lint (./lifecycle) | @cuisongliu |",148,true,2025-08-19 08:43:37,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5787,cuisongliu,Feature: auto assign action,"### What is the problem this feature will solve?

_No response_

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",182,true,2025-08-25 07:52:58,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5789,cuisongliu,Feature: set Kubelet new default version ,"### What is the problem this feature will solve?

fix default values for Kubelet

```
eventBurst: 100
eventRecordQPS: 50
...
kubeAPIBurst: 100
kubeAPIQPS: 50
```

### If you have solution，please describe it

https://github.com/kubernetes/kubernetes/pull/116121/files

### What alternatives have you considered?

_No response_",325,true,2025-08-28 09:02:57,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5797,benz9527,BUG: sealos install kubernetes 1.31.10 with cilium 1.16.1 with kubeproxyreplacement error,"### Sealos Version

v5.0.1

### How to reproduce the bug?

# Sealos Version
由于 sealos v5.0.1 release 安装 kubernetes 1.31.10 会出现卡在 master0 的 cri 版本错误上，参照 https://github.com/labring/sealos/issues/5644 的修复，直接改用 master 的源码进行编译的

#  安装流程
```
 sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.10 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.17.1-amd64 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.16.1-amd64 --masters x.x.x.x --nodes x.x.x.x
```

# 错误截图
<img width=""975"" height=""588"" alt=""Image"" src=""https://github.com/user-attachments/assets/60411bdc-ad79-454a-ba6e-b8d61440c6ae"" />
提示 cilium 的配置中出现参数遗漏问题，导致安装失败

### What is the expected behavior?

kubernetes 可以正常安装

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version: v5.0.1(self build from latest master)
- Docker version: None
- Kubernetes version: 1.31.10
- Operating system: OpenEuler
- Runtime environment: 
- Cluster size: 1 master + 1 worker
- Additional information:
```

### Additional information

_No response_",1052,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5804,dinoallo,BUG: duplicate `pods` enforcements in `enforceNodeAllocatable` in `/var/lib/kubelet/config.yaml`,"### Sealos Version

main

### How to reproduce the bug?

Install sealos using the [cloud installation script](https://github.com/labring/sealos/blob/main/scripts/cloud/install.sh), after changing the used Kubernetes version to v1.31.10

### What is the expected behavior?

Successful installation of sealos

### What do you see instead?

Installing sealos with Kubernetes v1.13.10 fails:
```
Aug 08 14:27:22 cloud-m1 kubelet[241103]: E0808 14:27:22.751318  241103 run.go:72] ""command failed"" err=""failed to validate kubelet configuration, error: invalid configuration: duplicated enforcements \""pods\"" in enforceNodeAllocatable (--enforce-node-allocatable), path: &TypeMeta{Kind:,APIVersion:,}""
Aug 08 14:27:22 cloud-m1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
```
Check `/var/lib/kubelet/config.yaml`:
```
# grep enforceNodeAllocatable -A 2 /var/lib/kubelet/config.yaml
enforceNodeAllocatable:
- pods
- pods
```
Removing duplicate `pods` results in successful installation.

### Operating environment

```markdown
- Sealos version: main
- Docker version:
- Kubernetes version: v1.31.10
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1257,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5827,43669522,sealos演示视频更新,"### Documentation Section

进行视频更新替换

### What is the current documentation state?

[https://objectstorageapi.hzh.sealos.run/ecv14ujz-appstore/GitHub-2.mp4](url)

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",299,true,2025-08-22 03:06:40,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5844,cuisongliu,add CODE_OF_CONDUCT for sealos,"Objective: Draft a professional and inclusive Code of Conduct for an open-source software project. The document should be ready to use but will be customized by the project maintainers.

Please structure the output as follows:

    Header: Title: ""Code of Conduct""

    Introduction: A brief, welcoming paragraph stating the project's commitment to providing a welcoming and harassment-free experience for everyone, regardless of background or identity. Mention that it applies to all project spaces (e.g., GitHub repos, issue trackers, Discord/Slack, mailing lists, in-person events).

    Core Principles / Our Pledge: Frame this positively. As members and maintainers, we pledge to make participation in our community a harassment-free experience for everyone. We will act and interact in ways that contribute to an open, friendly, diverse, inclusive, and healthy community.

    Expected Behavior: Provide a bulleted list of positive behaviors to encourage. Include examples such as:

        Using welcoming and inclusive language.

        Being respectful of differing viewpoints and experiences.

        Gracefully accepting constructive criticism.

        Focusing on what is best for the community.

        Showing empathy towards other community members.

    Unacceptable Behavior: Provide a bulleted list of behaviors that are considered harassment and are unacceptable. Include examples such as:

        The use of sexualized language or imagery, and unwelcome sexual attention or advances.

        Trolling, insulting/derogatory comments, and personal or political attacks.

        Public or private harassment.

        Publishing others' private information, such as a physical or email address, without their explicit permission.

        Other conduct which could reasonably be considered inappropriate in a professional setting.

    Scope: Clearly state that this Code of Conduct applies within all project spaces and also applies when an individual is officially representing the project in public spaces (e.g., using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an event).

    Enforcement Responsibilities & Guidelines: State that project maintainers are responsible for clarifying and enforcing standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior they deem inappropriate.

        Mention that maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.

    Reporting and Enforcement Procedures:

        Reporting: Instruct community members to contact the project team via a private, designated channel (e.g., a dedicated email address like conduct@project.example.com or a direct message to a specific team member on a platform). Assure them that all reports will be reviewed and investigated promptly and fairly. State that the project team is obligated to maintain confidentiality with regard to the reporter of an incident.

        Enforcement: Outline a general process. For example:

            Correction: A private, written warning from the maintainers, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.

            Warning: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period.

            Temporary Ban: A temporary ban from any sort of public interaction or communication with the community.

            Permanent Ban: A permanent ban from the community.

    Attribution: Include an attribution line at the bottom. Use the following text:

        ""This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/), version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.""

    Final Instructions:

        Use clear, professional, and accessible language.

        Use gender-neutral language (e.g., ""they/them"").

        Use contact information: contact@sealos.io.

        Do not invent a specific project name; keep it generic.",4369,true,2025-08-28 06:57:22,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5846,cuisongliu,add SECURITY.md for sealos,"Instruction:

Objective: Draft a clear and actionable SECURITY.md file for Sealos. This document is intended for security researchers and users who need to report a vulnerability.

Please structure the output as follows:

    Header: Title: ""Security Policy""

    Introduction: A brief statement thanking security researchers and users for helping to keep the project and its community safe.

    Supported Versions: Create a table or section clearly stating which versions of the project are currently supported with security updates. Use placeholders for version numbers. For example:

        [ Project Name ] | Version | Supported

        ------------------ | ------- | -------------------------

        [Latest Major Version] (e.g., 2.x) | >= 2.0.0 | ✅ Yes

        [Previous Major Version] (e.g., 1.x) | >= 1.5.0, < 2.0.0 | ⚠️ Best-effort

        [Older Version] | < 1.5.0 | ❌ No

    Reporting a Vulnerability: This is the most critical section. Provide a clear, step-by-step guide on how to report a vulnerability.

        Preferred Method: Instruct reporters to privately disclose vulnerabilities by emailing a dedicated security address (e.g., security@sealos.io) or by using the provider's private feature (e.g., GitHub's ""Privately report a security vulnerability"" feature on the repository's ""Security"" tab). Emphasize the importance of NOT creating a public GitHub Issue for security bugs.

        What to Include: Request that the report includes a clear description, the steps to reproduce the issue, the affected versions, and the potential impact. Mention that providing a fix or suggestion is appreciated, but not required.

    What to Expect After Reporting: Set clear expectations for the reporter.

        Response Time: State that the team will acknowledge the report within a specific timeframe (e.g., ""within 48 hours"").

        Process Outline: Briefly explain the process: the team will triage the report, work on a fix, and keep the reporter updated throughout. Define the goal for a fix timeline (e.g., ""We aim to address critical vulnerabilities within 14 days"").

        Post-Fix Disclosure: Explain the project's disclosure policy. The standard is to give reporters credit and issue a public disclosure (e.g., a GitHub Security Advisory) after a fix is released and users have had time to patch.

    Vulnerability Management Philosophy: Briefly describe the project's approach to security (e.g., ""We treat security vulnerabilities as our highest priority,"" ""We believe in responsible disclosure"").

    Security Updates: Explain how users will be notified of security updates. Typically, this is through:

        GitHub Releases (tagged with a [security] prefix or a security fix version).

        GitHub Security Advisories (GHSA).

        The project's official blog or mailing list.

    Final Instructions for the AI:

        Use clear, concise, and professional language.

        Do not invent specific contact details; Use contact information: security@sealos.io

        The tone should be grateful and collaborative, encouraging security research.

- Additional Reference: https://github.com/electron/electron/blob/main/SECURITY.md",3186,true,2025-09-01 05:08:21,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5848,cuisongliu,add CODEOWNERS for sealos,"https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners
TODO:

- [x] add CODEOWNERS
- [x] enable 'Require review from CODEOWNERS' in this repository",226,true,2025-08-21 04:48:07,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5850,cuisongliu,remove sync images task,"```
  - name: Renew issue and Sync Images
    uses: labring/gh-rebot@v0.0.6
    if: ${{ github.repository_owner == env.DEFAULT_OWNER }}
    with:
      version: v0.0.8-rc1
    env:
      GH_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
      SEALOS_TYPE: ""issue_renew""
      SEALOS_ISSUE_TITLE: ""[DaylyReport] Auto build for sealos""
      SEALOS_ISSUE_BODYFILE: ""scripts/ISSUE_RENEW.md""
      SEALOS_ISSUE_LABEL: ""dayly-report""
      SEALOS_ISSUE_TYPE: ""day""
      SEALOS_ISSUE_REPO: ""labring-actions/cluster-image""
      SEALOS_COMMENT_BODY: ""/imagesync ghcr.io/${{ github.repository_owner }}/sealos-patch:latest""

```

- [x]  delete sealos ci sync images
- [x]  add auto sync image ci in cluster-image ",695,true,2025-08-22 08:17:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5852,junlintianxiazhifulinzhongguo,BUG: sealos 添加节点，发现ssh端口被固定成22，无法使用自定义端口,"### Sealos Version

5.0.1

### How to reproduce the bug?

```
[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 -h
Add nodes into cluster

Examples:

add to nodes :
        sealos add --nodes x.x.x.x

add to default cluster:
        sealos add --masters x.x.x.x --nodes x.x.x.x
        sealos add --masters x.x.x.x-x.x.x.y --nodes x.x.x.x-x.x.x.y

add with different ssh setting:
        sealos add --masters x.x.x.x --nodes x.x.x.x --passwd your_diff_passwd
Please note that the masters and nodes added in one command should have the save password.

Options:
    --cluster='default':
        name of cluster to applied join action

    --masters='':
        masters to be joined

    --nodes='':
        nodes to be joined

    -p, --passwd='':
        use given password to authenticate with

    -i, --pk='/root/.ssh/id_rsa':
        selects a file from which the identity (private key) for public key authentication is read

    --pk-passwd='':
        passphrase for decrypting a PEM encoded private key

    --port=22:
        port to connect to on the remote host

    -u, --user='':
        username to authenticate as

Usage:
  sealos add [flags] [options]

Use ""sealos options"" for a list of global command-line options (applies to all commands).
[root@192-168-100-62 ~]# 

[root@192-168-100-62 ~]# sealos add --masters 192.168.100.63 --port=65535
2025-08-21T15:19:05 warn failed to get host arch: failed to create ssh session for 192.168.100.63:22: dial tcp 192.168.100.63:22: connect: connection refused, defaults to amd64
2025-08-21T15:19:05 info start to scale this cluster
2025-08-21T15:19:05 info Executing pipeline JoinCheck in ScaleProcessor.
2025-08-21T15:19:05 info Warning: Using an even number of master nodes is a risky operation and can lead to reduced high availability and potential resource wastage. It is strongly recommended to use an odd number of master nodes for optimal cluster stability. Are you sure you want to proceed?
Do you want to continue on '192-168-100-62' cluster? Input '192-168-100-62' to continue: 192-168-100-62
2025-08-21T15:19:15 info checker:containerd [192.168.100.63:22]
2025-08-21T15:19:15 info Executing pipeline PreProcess in ScaleProcessor.
2025-08-21T15:19:16 info Executing pipeline PreProcessImage in ScaleProcessor.
2025-08-21T15:19:16 info Executing pipeline RunConfig in ScaleProcessor.
2025-08-21T15:19:16 info Executing pipeline MountRootfs in ScaleProcessor.
2025-08-21T15:19:16 error error occur while sending mount image default-xlop4cm2: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused
2025-08-21T15:19:16 error Applied to cluster error: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused
2025-08-21T15:19:16 error failed to sync workdir: /root/.sealos/default error, failed to connect: dial tcp 192.168.100.63:22: connect: connection refused
Error: failed to copy entry /var/lib/containers/storage/overlay/f85a99b3daf775d156d059509dfa51181193f59bcbe5d340a451ab16441ad177/merged/Kubefile -> /var/lib/sealos/data/default/rootfs/Kubefile to 192.168.100.63:22: failed to connect: dial tcp 192.168.100.63:22: connect: connection refused
[root@192-168-100-62 ~]#
[root@192-168-100-62 ~]#


```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",3961,true,2025-08-21 09:12:38,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5853,cuisongliu,BUG:  issues-translator is not work,"### Sealos Version

main

### How to reproduce the bug?

https://github.com/labring/sealos/actions/runs/17122518546/job/48566545145

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",457,true,2025-08-29 02:30:09,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5878,dzy888,mount-cgroup,"### Sealos Version

5.0.1

### How to reproduce the bug?

月 27 16:26:46 slave1 kubelet[18965]: I0827 16:26:46.352575 18965 reconciler_common.go:258] ""operationExecutor.VerifyControllerAttachedVolume started for volume ""lib-modules"" (UniqueName: ""kubernetes.io/host 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321152 18965 scope.go:117] ""RemoveContainer"" containerID=""301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a"" 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.321797 18965 scope.go:117] ""RemoveContainer"" containerID=""301810c5d0dfb721c971b806d9824bfc57a06b9a8c4f308d35881e9d329a9f4a"" 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.323939 18965 remote_runtime.go:385] ""RemoveContainer from runtime service failed"" err=""rpc error: code = Unknown desc = failed to set removing state for cont 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324006 18965 kuberuntime_container.go:858] failed to remove pod init container ""mount-cgroup"": rpc error: code = Unknown desc = failed to set removing state 8月 27 16:26:50 slave1 kubelet[18965]: E0827 16:26:50.324401 18965 pod_workers.go:1298] ""Error syncing pod, skipping"" err=""failed to ""StartContainer"" for ""mount-cgroup"" with CrashLoopBackOff: ""back-off 10s 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377405 18965 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/cilium-operator-6946ccbcc5-kfplj"" podStartSLOduration=3.187 8月 27 16:26:50 slave1 kubelet[18965]: I0827 16:26:50.377726 18965 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/kube-sealos-lvscare-slave1"" podStartSLOduration=4.377699505 8月 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.357959 18965 scope.go:117] ""RemoveContainer"" containerID=""a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f"" 8月 27 16:27:04 slave1 kubelet[18965]: I0827 16:27:04.358911 18965 scope.go:117] ""RemoveContainer"" containerID=""a5a7c6ef619d909a5f8cf0b19c709284ae2c5862bc9d91538fecaec74f31c77f"" 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361781 18965 remote_runtime.go:385] ""RemoveContainer from runtime service failed"" err=""rpc error: code = Unknown desc = failed to set removing state for cont 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.361857 18965 kuberuntime_container.go:858] failed to remove pod init container ""mount-cgroup"": rpc error: code = Unknown desc = failed to set removing state 8月 27 16:27:04 slave1 kubelet[18965]: E0827 16:27:04.362621 18965 pod_workers.go:1298] ""Error syncing pod, skipping"" err=""failed to ""StartContainer"" for ""mount-cgroup"" with CrashLoopBackOff: ""back-off 20s 8月 27 16:27:17 slave1 kubelet[18965]: E0827 16:27:17.212276 18965 pod_workers.go:1298] ""Error syncing pod, skipping"" err=""failed to ""StartContainer"" for ""mount-cgroup"" with CrashLoopBackOff: ""back-off 20s

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",3177,true,2025-09-11 10:34:05,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5895,cuisongliu,✨ Set up Copilot instructions,"Configure instructions for this repository as documented in [Best practices for Copilot coding agent in your repository](https://gh.io/copilot-coding-agent-tips).

<Onboard this repo>",183,true,2025-08-29 03:12:13,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5898,cuisongliu,测试的问题,主要是测一下翻译插件,10,true,2025-08-29 02:31:47,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5899,cuisongliu,GitHub Translator version error,"```
- name: GitHub Translator

  uses: lizheming/github-translate-action@1.1.2
```",82,true,2025-08-29 03:04:08,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5901,cuisongliu,测试翻译插件 || Test translation plugin,"主要测试翻译转换
<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->
---
Main test translation conversion
",180,true,2025-08-29 03:11:50,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5902,cuisongliu,Docs: LICENSE change declaration,"### Documentation Section

LICENSE

### What is the current documentation state?

CONTRIBUTING.md
README.md
README_zh.md

It is necessary to explain the LICENSE change declaration. According to the contenthttps://github.com/labring/sealos/blob/main/LICENSE.md and  https://github.com/labring/sealos/blob/main/CONTRIBUTOR_LICENSE_AGREEMENT.md

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_",480,true,2025-08-29 09:17:13,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5906,cuisongliu,BUG: release cloud error,"### Sealos Version

v5.1.0

### How to reproduce the bug?

https://github.com/labring/sealos/actions/runs/17283064704/job/49055050941 This action did not upload the mirror image normally, because there is no write permission of the package.

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",566,true,2025-08-30 06:37:44,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5924,cuisongliu,BUG: delete admin and devbox deploy,"### Sealos Version

5.1.0

### How to reproduce the bug?

_No response_

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",397,true,2025-09-03 04:59:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5945,renziyou,什么时候支持微软的MSSQL || When will Microsoft's MSSQL be supported,"### What is the problem this feature will solve?

什么时候支持微软的MSSQL

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_
<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->
---
### What is the problem this feature will solve?

When will Microsoft MSSQL be supported

### If you have solution, please describe it

_No response_

### What alternatives have you considered?

_No response_
",531,true,2025-09-08 08:58:43,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5952,kevinydsk,BUG: desktop-frontend-xxx can't be running,"### Sealos Version

5.0.1

### How to reproduce the bug?

1. Use https://github.com/labring/sealos/blob/main/scripts/cloud/build-offline-tar.sh build offline package.
2. Use the install.sh in build path.
3. desktop-frontend can't be runing, it always in Init:CrashLoopBackOff
4. The log in desktop-frontend-xxx init-database container is:
```
Prisma schema loaded from desktop/prisma/global/schema.prisma
Datasource ""db"": CockroachDB database ""global"", schema ""public"" at ""sealos-cockroachdb-public.sealos.svc.cluster.local:26257""

Error: P1001: Can't reach database server at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`

Please make sure your database server is running at `sealos-cockroachdb-public.sealos.svc.cluster.local`:`26257`.
Prisma schema loaded from desktop/prisma/region/schema.prisma
Datasource ""db"": CockroachDB database, schema ""public"" at ""sealos-cockroachdb-public.sealos.svc.cluster.local:26257""
```

I change the db url from:
```
globalCockroachdbURI: ""postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global""
regionalCockroachdbURI: ""postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257
```
to
```
globalCockroachdbURI: ""postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257/global?sslmode=require""
 regionalCockroachdbURI: ""postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require""
```

It always have the same error.

### What is the expected behavior?

desktop-frontend-xxx be running

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:5.0.1
- Containerd version:v1.7.27
- Kubernetes version:v1.28.11
- Operating system:Ubuntu22.04 Linux k8s-master-01 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
- Runtime environment:
- Cluster size:1master 4node
```

### Additional information

The cockroachdb can be connected use psql in k8s by postgresql://sealos:z4yq343vqg9ferp7n6c0ykn02urjh7e9km69jqofpdxsb8h42b0zgypht7ojkuzj@sealos-cockroachdb-public.sealos.svc.cluster.local:26257?sslmode=require.",2404,true,2025-09-11 10:33:35,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5972,cuisongliu,pr week report,"## Submit PR according to my requirements

1. Generate weekly community submission record PR and issue

2. Count the number of PRs submitted by each person and the number of consolidations

3. Or what you think is a good open source community weekly newspaper

4. It's better to write a workflow timed trigger. If you can't do it, you can raise the issue regularly and submit my request.",387,true,2025-09-11 11:01:15,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5975,cuisongliu,Feature: sealos v2 registry design,"### What is the problem this feature will solve?

It is mainly to solve the problem of mirror offline caching. At present, all of them need to be packaged into tar before they can be offline. So is it more difficult for us to cache all cluster mirror images into registry format? We only need sealos run k8s-all to run all the clusters.


### If you have solution，please describe it

Sealos starts the temporary registry by default, and then sealos pulls the cluster mirror image directly from this temporary registry, and sealos runs the sealos according to the configuration file.

```
FROM scratch
MAINTAINER sealos
LABEL init=""...
      ""apps.sealos.io/type""=registry \  
      ""apps.sealos.io/version""=v1beta1  \  
     ""apps.sealos.io/config""=config.yaml 
```
#### apps.sealos.io/type=registry 
Define the type of cluster mirror as registry type.

#### apps.sealos.io/config

Defining the configuration of registry mainly solves the startup process problem. It will carry out sealos run according to the configuration of config. Of course, there will be no dependency problem here.

```yaml
apiVersion: apps.sealos.io/v1beta1
kind: Action
spec:
   check:
      role: master,node
      runCmd:
      -  xxx
      -  xxx
   runApp:
   - kubernetes:v1.28.15
   - xxx
   - xxx
    

```

### What alternatives have you considered?

_No response_",1347,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5978,cuisongliu,Feature: image shim online regsitry auto load,"### What is the problem this feature will solve?

```
# Copyright © 2022 sealos.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

shim: /var/run/image-cri-shim.sock
cri: /run/containerd/containerd.sock
address: http://sealos.hub:5000
force: true
debug: true
image: /var/lib/image-cri-shim
timeout: 15m
auth: admin:passw0rd

registries:
- address: http://192.168.64.1:5000
  auth: admin:passw0rd

```

### If you have solution，please describe it

It needs to be restarted, and dynamic loading is better.


### What alternatives have you considered?

```
# Copyright © 2022 sealos.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

shim: /var/run/image-cri-shim.sock
cri: /run/containerd/containerd.sock
address: http://sealos.hub:5000
force: true
debug: true
image: /var/lib/image-cri-shim
timeout: 15m
auth: admin:passw0rd

registryConfig: /etc/image-cri-shim.registry.yaml
```

```
registries:
- address: http://192.168.64.1:5000
  auth: admin:passw0rd
```",1952,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,5979,cuisongliu,BUG: images shim not sport local  registry,"### Sealos Version

main

### How to reproduce the bug?

<img width=""2040"" height=""204"" alt=""Image"" src=""https://github.com/user-attachments/assets/3d34da88-fbce-435f-a177-4499adc3ee34"" />

```
shim: /var/run/image-cri-shim.sock
cri: /var/run/containerd/containerd.sock
address: http://sealos.hub:5000
force: true
debug: false
timeout: 15m
auth: admin:passw0rd

registries:
- address: https://hub.192.168.64.4.nip.io
  auth: admin:3c0e9478196ec93f2e5291d119408ddc
```

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",793,true,2025-09-14 11:57:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5987,aiqianbao1314,"BUG: sealos run 安装1.31及以上版本的k8s时，全部会安装失败！ || BUG: sealos run When installing k8s version 1.31 and above, all installations will fail!","### Sealos Version

v5.0.0

### How to reproduce the bug?

系统： rockylinux8.5
sealos: v5.0.0 v5.0.1
异常的k8s版本：v1.31及以上，docker版本container版本都是
安装命令:
images='
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4
registry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1
'
sealos run $images
1.31 1.32 1.33都进行了测试
主要报错片段：
vm.max_map_count = 2147483642 # sealos
 INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10 
Image is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
 INFO [2025-09-15 23:10:33] >> init kubelet success 
 INFO [2025-09-15 23:10:33] >> init rootfs success 
2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.
2025-09-15T23:10:34 info Copying kubeadm config to master0
2025-09-15T23:10:34 info start to generate cert and kubeConfig...
2025-09-15T23:10:34 info start to generate and copy certs to masters...
2025-09-15T23:10:34 info apiserver altNames : {map[apiserver.cluster.local:apiserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}
2025-09-15T23:10:34 info Etcd altnames : {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128 ::1:::1]}, commonName : k8s-01
2025-09-15T23:10:35 info start to copy etc pki files to masters
2025-09-15T23:10:35 info start to create kubeconfig...
2025-09-15T23:10:36 info start to copy kubeconfig files to masters
2025-09-15T23:10:36 info start to copy static files to masters
2025-09-15T23:10:36 info start to init master0...
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher
2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
Error: failed to init masters: master pull image failed, error: exit status 1

<img width=""1920"" height=""671"" alt=""Image"" src=""https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d"" />

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_
<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->
---
### Sealos Version

v5.0.0

### How to reproduce the bug?

System: rockylinux8.5
sealos: v5.0.0 v5.0.1
Exception k8s version: v1.31 and above, docker version container version are all
Installation command:
images='
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4
registry.cn-shanghai.aliyuncs.com/labring/calico:v3.24.1
'
sealos run $images
1.31 1.32 1.33 are all tested
Main error report segments:
vm.max_map_count = 2147483642 # sealos
 INFO [2025-09-15 23:10:33] >> pull pause image sealos.hub:5000/pause:3.10
Image is up to date for sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
 INFO [2025-09-15 23:10:33] >> init kubelet success
 INFO [2025-09-15 23:10:33] >> init rootfs success
2025-09-15T23:10:33 info Executing pipeline Init in CreateProcessor.
2025-09-15T23:10:34 info Copying kubeadm config to master0
2025-09-15T23:10:34 info start to generate cert and kubeConfig...
2025-09-15T23:10:34 info start to generate and copy certs to masters...
2025-09-15T23:10:34 info apiserver altNames: {map[apserver.cluster.local:apserver.cluster.local k8s-01:k8s-01 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.110.128:192.168.110.128]}
2025-09-15T23:10:34 info Etcd altnames: {map[k8s-01:k8s-01 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.110.128::1:::1]}, commonName: k8s-01
2025-09-15T23:10:35 info start to copy etc pki files to masters
2025-09-15T23:10:35 info start to create kubeconfig...
2025-09-15T23:10:36 info start to copy kubeconfig files to masters
2025-09-15T23:10:36 info start to copy static files to masters
2025-09-15T23:10:36 info start to init master0...
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint ""unix:///var/run/image-cri-shim.sock"": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
To see the stack trace of this error execute with --v=5 or higher
2025-09-15T23:10:36 error Applied to cluster error: failed to init masters: master pull image failed, error: exit status 1
Error: failed to init masters: master pull image failed, error: exit status 1

<img width=""1920"" height=""671"" alt=""Image"" src=""https://github.com/user-attachments/assets/eb3f7999-7e59-4a9c-b276-bf1e9e463c5d"" />

### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_
",6010,true,2025-09-16 14:03:24,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1212,kjagsdq,【Auto-build】helm,/imagebuild_dockerimages abc,28,true,2025-04-19 18:04:24,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,5992,ATCThunder,fail to install,"### Sealos Version

v5.0.1

### How to reproduce the bug?

$ sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.29.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single

执行这个安装命令的时候报：
E0917 10:50:50.919408 39371 remote_image.go:180] ""PullImage from image service failed"" err=""rpc error: code = Unknown desc = failed to pull and unpack image \""sealos.hub:5000/pause:3.9\"": failed to resolve reference \""sealos.hub:5000/pause:3.9\"": failed to do request: Head \""https://sealos.hub:5000/v2/pause/manifests/3.9\"": EOF"" image=""sealos.hub:5000/pause:3.9"" FATA[0001] pulling image: failed to pull and unpack image ""sealos.hub:5000/pause:3.9"": failed to resolve reference ""sealos.hub:5000/pause:3.9"": failed to do request: Head ""https://sealos.hub:5000/v2/pause/manifests/3.9"": EOF Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service. Error: failed to init masters: master pull image failed, error: exit status 1

改配置，让其通过http访问，但依然还是通过https访问

### What is the expected behavior?

能够正常安装成功

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",1402,true,2025-09-18 02:24:34,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1214,kjagsdq,【Auto-build】helm,1,1,true,2025-04-19 18:04:18,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,6007,cuisongliu,Feature: admission webhook release tag,"### What is the problem this feature will solve?

_No response_

### If you have solution，please describe it

_No response_

### What alternatives have you considered?

_No response_",182,true,2025-09-20 08:42:53,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1216,lingdie,【Auto-build】elasticsearch,"```
Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
/imagebuild_apps elasticsearch v8.16.2",214,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1221,dinoallo,【Auto-build】helm,"/imagebuild_apps helm v3.8.2
",29,true,2025-05-29 08:47:44,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1224,dinoallo,【Auto-build】helm,/imagebuild_apps helm v3.8.2,28,true,2025-05-28 07:59:00,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1225,macaty,【Auto-build】longhorn,"Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2",167,true,2025-06-09 01:30:36,CLOSED,false,2025-10-06 00:06:29.060857+08
labring/sealos,6036,menchao1992,Docs: brief description of your suggestion || Docs: brief description of your suggestions,"### Documentation Section

安装文档

### What is the current documentation state?

sealos 集群信息，我在master1节点上创建的集群，如果master1 挂了，就GG。

### What is your suggestion?

_No response_

### Why is this change beneficial?

_No response_

### Additional information

_No response_
<!--This is a translation content dividing line, the content below is generated by machine, please do not modify the content below-->
---
### Documentation Section

Installation documentation

### What is the current documentation state?

sealos cluster information, the cluster I created on the master1 node, if master1 is hung, it will GG.

### What is your suggestion?

_No response_

### Why is this change benefit?

_No response_

### Additional information

_No response_
",744,true,2025-09-24 15:06:54,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1226,macaty,【Auto-build】longhorn,"Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps longhorn v1.8.0",171,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,6056,ATCThunder,使用官方脚本，安装失败,"### Sealos Version

5.1.0-beta4

### How to reproduce the bug?

==========================================================================================================
You are installing the open source version of Sealos Cloud. Some features are missing. 
For full functionality, please purchase the commercial edition: https://sealos.io/contact
==========================================================================================================
 INFO [2025-09-28 17:37:02] >> Using GitHub proxy for downloads. 
 INFO [2025-09-28 17:37:02] >> [Step 1] Environment and dependency checks... 
 INFO [2025-09-28 17:37:02] >> Environment and dependency checks passed. 
 INFO [2025-09-28 17:37:02] >> Sealos CLI is already installed. 
 INFO [2025-09-28 17:37:02] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubernetes:v1.28.15 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cilium:v1.17.1 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cert-manager:v1.14.6 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/helm:v3.16.2 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/openebs:v3.10.0 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/higress:v2.1.3 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/kubeblocks:v0.8.2 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/cockroach:v2.12.0 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/metrics-server:v0.6.4 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos/victoria-metrics-k8s-stack:v1.124.0 
 INFO [2025-09-28 17:37:03] >> Pulling image: ghcr.nju.edu.cn/labring/sealos-cloud:latest 
Error: initializing image from source docker://ghcr.nju.edu.cn/labring/sealos-cloud:latest: invalid character '<' looking for beginning of value


### What is the expected behavior?

_No response_

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",2275,true,2025-09-29 08:33:22,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1228,LinkMaq,【Auto-build】nerdctl,"```
Usage:
   /imagebuild_apps nerdctl v2.1.2
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps nerdctl v2.1.2
```
",175,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,6057,xiachenrui,BUG: install fail,"### Sealos Version

5.1.0-rc1

### How to reproduce the bug?

SEALOS_V2_CLOUD_DOMAIN=10.128.250.128.nip.io SEALOS_V2_MASTERS=""10.128.250.128:22"" ./install-v2.sh

### What is the expected behavior?

Install

### What do you see instead?

```sh
2025-09-28T12:45:45 info start to install app in this cluster
2025-09-28T12:45:45 info Executing SyncStatusAndCheck Pipeline in InstallProcessor
2025-09-28T12:45:45 info Executing ConfirmOverrideApps Pipeline in InstallProcessor
2025-09-28T12:45:45 info Executing PreProcess Pipeline in InstallProcessor
2025-09-28T12:45:45 info Executing pipeline MountRootfs in InstallProcessor.
2025-09-28T12:45:46 info Executing pipeline MirrorRegistry in InstallProcessor.
2025-09-28T12:45:46 info Executing UpgradeIfNeed Pipeline in InstallProcessor
Error from server (NotFound): configmaps ""sealos-config"" not found
Error from server (NotFound): configmaps ""sealos-config"" not found
namespace/sealos-system configured
namespace/sealos configured
Release ""cert-config"" does not exist. Installing it now.
Error: Unable to continue with install: Certificate ""sealos-cloud"" in namespace ""sealos-system"" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key ""app.kubernetes.io/managed-by"": must be set to ""Helm""; annotation validation error: missing key ""meta.helm.sh/release-name"": must be set to ""cert-config""; annotation validation error: missing key ""meta.helm.sh/release-namespace"": must be set to ""sealos-system""
2025-09-28T12:45:47 info using v1beta3 kubeadm config
Error: exit status 1
```

### Operating environment

```markdown
- Sealos version:5.1.0-rc1
- Docker version:
- Kubernetes version:
- Operating system: Ubuntu 24
- Runtime environment:physical machine 200G mem
- Cluster size:1 node
- Additional information:
```

### Additional information

_No response_",1871,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1229,kaneleven,【Auto-build】cilium v1.17.4,"```
/imagebuild_apps cilium  v1.17.4
```
",41,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1230,ilyndon,【Auto-build】rainbond,/imagebuild_apps rainbond v6.3.1,32,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring/sealos,6062,ancienter,BUG: VIP Conflicts with Service Subnet in Sealos,"### Sealos Version

v5.0.1

### How to reproduce the bug?

After running sealos gen -e defaultVIP=198.19.0.2, I manually changed networking.serviceSubnet to 198.19.0.0/16 in the Clusterfile. However, executing sealos apply -f Clusterfile still results in the error:

""Applied to cluster error: failed to init masters: generate init config error: ensure IP 198.19.0.2 is not in serviceSubnet range"".

### What is the expected behavior?

The VIP has been correctly set to 198.19.0.2 without any errors.

### What do you see instead?

_No response_

### Operating environment

```markdown
- Sealos version:
- Docker version:
- Kubernetes version:
- Operating system:
- Runtime environment:
- Cluster size:
- Additional information:
```

### Additional information

_No response_",775,true,2025-09-30 13:06:43,CLOSED,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1231,cuisongliu,【Auto-build】minio,"```
Usage:
   /imagebuild_apps appName appVersion
Available Args:
   appName:  current app dir
   appVersion: current app version

Example:
   /imagebuild_apps helm v3.8.2
```
",176,false,,OPEN,false,2025-10-06 00:06:29.060857+08
labring-actions/cluster-image,1232,macaty,请继续维护这个仓库的accepted的标签，谢谢,"请问能帮忙维护编译应用仓库吗？目前仅有几个允许accepted标签和编译，帮忙把常用的longhorn,openebs，metallb、calico、higress、apisix、kubeblocks打一下accepted标签，谢谢。",117,false,,OPEN,false,2025-10-06 00:06:29.060857+08
